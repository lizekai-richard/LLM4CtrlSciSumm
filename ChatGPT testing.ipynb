{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9524a06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/yanxia/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.20 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from openai) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /home/yanxia/anaconda3/lib/python3.9/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yanxia/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c6fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031e4ad",
   "metadata": {},
   "source": [
    "# 0. Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b61383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas because of the COVID-19 pandemic.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7af4593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2020 World Series was played at Globe Life Field in Arlington, Texas because of the COVID-19 pandemic.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_output = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "str_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ce672",
   "metadata": {},
   "source": [
    "# 1. CtrlSciSumm testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db264072",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example from BERT paper\n",
    "article = \"\"\"Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.\"\"\"\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04117102",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "Please summarize this paper: \n",
      "\n",
      "**Output**: The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a pre-training language model that uses a \"masked language model\" (MLM) pre-training objective to alleviate the unidirectionality constraint of standard language models. BERT is able to fuse the left and right context, allowing for deep bidirectional representations. The paper shows that pre-trained representations reduce the need for heavily-engineered task-specific architectures and achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks. The code and pre-trained models are available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please summarize this paper: \"\n",
    "\n",
    "model_input = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful research assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":prompt+article}\n",
    "]\n",
    "\n",
    "output = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=model_input,\n",
    "  max_tokens=200,\n",
    "  temperature=0.2\n",
    ")\n",
    "\n",
    "str_output = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {str_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f751ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "Provide a TLDR summary for the text: \n",
      "\n",
      "**Output**: The paper introduces BERT (Bidirectional Encoder Representations from Transformers), a pre-training language model that uses a \"masked language model\" (MLM) pre-training objective to enable deep bidirectional representations. Unlike previous unidirectional language models, BERT can incorporate context from both directions, making it more effective for sentence-level and token-level tasks. BERT outperforms many task-specific architectures and advances the state of the art for eleven NLP tasks. The code and pre-trained models are available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Provide a TLDR summary for the text: \"\n",
    "\n",
    "model_input = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful research assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":prompt+article}\n",
    "]\n",
    "\n",
    "output = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=model_input,\n",
    "  max_tokens=200,\n",
    "  temperature=0.2\n",
    ")\n",
    "\n",
    "str_output = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {str_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "756ed37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "Provide a TLDR summary for the scientific introduction text, focus on masked language model: \n",
      "\n",
      "**Output**: The introduction discusses the effectiveness of language model pre-training for natural language processing tasks. Two existing strategies for applying pre-trained language representations to downstream tasks are feature-based and fine-tuning. However, the authors argue that current techniques restrict the power of pre-trained representations, especially for fine-tuning approaches, due to the unidirectional nature of standard language models. To address this, the authors propose BERT, a bidirectional encoder representation from transformers, which uses a masked language model pre-training objective to enable pre-trained deep bidirectional representations. BERT achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Provide a TLDR summary for the scientific introduction text, focus on masked language model: \"\n",
    "\n",
    "model_input = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful research assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":prompt+article}\n",
    "]\n",
    "\n",
    "output = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=model_input,\n",
    "  max_tokens=200,\n",
    "  temperature=0.2\n",
    ")\n",
    "\n",
    "str_output = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {str_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45501f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "Please polish the text extracted from PDF, which talks about natural language processing, try to split concatenated words: \n",
      "\n",
      "**Output**: Introduction:\n",
      "\n",
      "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).\n",
      "\n",
      "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
      "\n",
      "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.\n",
      "\n",
      "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pre-trains text-pair representations.\n",
      "\n",
      "The contributions of our paper are as follows:\n",
      "\n",
      "• We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pre-trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n",
      "\n",
      "• We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n",
      "\n",
      "• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please polish the text extracted from PDF, which talks about natural language processing, try to split concatenated words: \"\n",
    "\n",
    "model_input = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful research assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":prompt+article}\n",
    "]\n",
    "\n",
    "output = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=model_input,\n",
    "  max_tokens=1000,\n",
    "  temperature=0.2\n",
    ")\n",
    "\n",
    "str_output = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {str_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7071e",
   "metadata": {},
   "source": [
    "Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15359a",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "ChatGPT can do well in (1) full text polishing, like split concatenated words, (2) understand natural language instructions, (3) better keyword controlled summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272162f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
