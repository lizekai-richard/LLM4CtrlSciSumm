{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9524a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install galai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c6fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import galai as gal\n",
    "import time\n",
    "from galai.notebook_utils import *\n",
    "\n",
    "#model = gal.load_model(\"standard\") # 6.7B\n",
    "model = gal.load_model(\"base\") # 1.3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031e4ad",
   "metadata": {},
   "source": [
    "# 0. Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b61383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>:\n",
       "Scaled dot product attention:</p>\n",
       "<p>$$</p>\n",
       "<p><strong>Output</strong>: Scaled dot product attention:</p>\n",
       "<p>$$ \\displaystyle\\text{Scaled dot product attention}(Q,K,V)=\\text{softmax}(\\frac{QK^{%\n",
       "T}}{\\sqrt{d_{k}}})V $$ (1)</p>\n",
       "<p>where $Q\\in\\mathbb{R}^{n\\times d_{q}}$, $K\\in\\mathbb{R}^{n</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Scaled dot product attention:\\n\\n\\\\[\"\n",
    "#print(\"Generation start time:\", time.asctime())\n",
    "output = model.generate(prompt, max_new_tokens=100)\n",
    "# Scaled dot product attention:\\n\\n\\\\[ \\\\displaystyle\\\\text{Attention}(Q,K,V)=\\\\text{softmax}(\\\\frac{QK^{T}}{\\\\sqrt{d_{k}}}%\\n)V \\\\]\n",
    "display_markdown(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {output}\")\n",
    "#print(\"Generation end time:\", time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319194ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>: The paper that presented a novel computing block given by the formula:\n",
       "$$\n",
       "f(A, K, V) = \\textrm{softmax}\\left(\\frac{AK^T}{\\sqrt{d_k}}\\right)V\n",
       "$$</p>\n",
       "<p><strong>Reference</strong>: A Simple Framework for Contrastive Learning of Visual Representations, Chen</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The paper that presented a novel computing block given by the formula:\n",
    "\\\\[\n",
    "f(A, K, V) = \\\\textrm{softmax}\\\\left(\\\\frac{AK^T}{\\\\sqrt{d_k}}\\\\right)V\n",
    "\\\\]\n",
    "\n",
    "\"\"\"\n",
    "#print(\"Generation start time:\", time.asctime())\n",
    "reference = model.generate_reference(prompt)\n",
    "display_markdown(f\"**Prompt**: {prompt}\\n\\n**Reference**: {reference}\")\n",
    "#print(\"Generation end time:\", time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58d93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>: The paper that presented a novel computing block given by the formula:\n",
       "$$\n",
       "f(Q, K, V) = \\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
       "$$</p>\n",
       "<p><strong>Reference</strong>: Attention is All you Need, Vaswani</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The paper that presented a novel computing block given by the formula:\n",
    "\\\\[\n",
    "f(Q, K, V) = \\\\textrm{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\n",
    "\\\\]\n",
    "\n",
    "\"\"\"\n",
    "#print(\"Generation start time:\", time.asctime())\n",
    "reference = model.generate_reference(prompt)\n",
    "display_markdown(f\"**Prompt**: {prompt}\\n\\n**Reference**: {reference}\")\n",
    "#print(\"Generation end time:\", time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ce672",
   "metadata": {},
   "source": [
    "# 1. CtrlSciSumm testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db264072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"\"\"Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.\"\"\"\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4216596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>:\n",
       "Please summarize this paper:</p>\n",
       "<p><strong>Output</strong>: Please summarize this paper:\n",
       "Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce fine-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures thatinclude the pre-trained representations as addi-tional features. The fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-specific parameters, and is trained on thedownstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the fine-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying fine-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the fine-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-specific architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.</p>\n",
       "<h1>2 Related Work</h1>\n",
       "<p>Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 20</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Please summarize this paper:\n",
    "\"\"\"\n",
    "\n",
    "output = model.generate(prompt+article)\n",
    "display_markdown(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f751ffed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>:\n",
       "TLDR:</p>\n",
       "<p><strong>Output</strong>: Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce fine-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures thatinclude the pre-trained representations as addi-tional features. The fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-specific parameters, and is trained on thedownstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the fine-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying fine-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the fine-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-specific architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.</p>\n",
       "<p>TLDR: We propose BERT: BidirectionalEncoder Representationsfrom Transformers, which is the first fine-tuning based representation model thatachieves state-of-the-art performance on a large suite of sentence-level and token-level tasks.</p>\n",
       "<h1>2 Related Work</h1>\n",
       "<p>Language model pre-training has</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"TLDR:\"\"\"\n",
    "\n",
    "output = model.generate(article+\"\\n\\n\"+prompt)\n",
    "display_markdown(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45501f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Prompt</strong>:\n",
       "Please proofread this paper:</p>\n",
       "<p><strong>Output</strong>: Please proofread this paper:\n",
       "Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce fine-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures thatinclude the pre-trained representations as addi-tional features. The fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-specific parameters, and is trained on thedownstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the fine-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying fine-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the fine-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-specific architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.</p>\n",
       "<h1>2 Related Work</h1>\n",
       "<p>Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 20</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Please proofread this paper:\n",
    "\"\"\"\n",
    "\n",
    "output = model.generate(prompt+article)\n",
    "display_markdown(f\"**Prompt**:\\n{prompt}\\n\\n**Output**: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518e445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
