{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9524a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install protobuf==3.20.0 transformers==4.26.1 icetk cpm_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c6fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031e4ad",
   "metadata": {},
   "source": [
    "# 0. Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b61383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b752ca478e0b495fac9ce5958e774568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n",
      "晚上睡不着可能会让人感到焦虑和不安。以下是一些可能有用的技巧，可以帮助你入睡：\n",
      "\n",
      "1. 创建一个舒适的睡眠环境：确保房间安静、黑暗、凉爽和舒适。还可以使用耳塞、眼罩和柔软的床单、枕头等。\n",
      "\n",
      "2. 规律锻炼：规律的运动可以减轻压力和焦虑，并有助于入睡。但请注意，不要在晚上进行过度剧烈的运动。\n",
      "\n",
      "3. 放松技巧：使用深呼吸、渐进性肌肉松弛、冥想或瑜伽等放松技巧，可以帮助你放松身体和心灵。\n",
      "\n",
      "4. 避免饮用咖啡因和酒精：咖啡因和酒精可以刺激大脑，使你更难入睡。尽量避免在睡前饮用这些物质。\n",
      "\n",
      "5. 建立睡前例行程序：建立一个睡前的例行程序，如洗澡、读书、冥想或听轻柔的音乐，可以帮助你放松并准备入睡。\n",
      "\n",
      "如果使用了这些方法，但仍然无法入睡，建议咨询医生或睡眠专家。他们可以提供更进一步的建议和治疗。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ce672",
   "metadata": {},
   "source": [
    "# 1. CtrlSciSumm testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db264072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example from BERT paper\n",
    "article = \"\"\"Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.\"\"\"\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a60cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a new approach to language representation pre-training, called BERT: BidirectionalEncoder Representationsfrom Transformers (BERT). BERT uses Masked Language Models (MLM) to enable pre-trained deep bidirectional representations, which allows it to improve the performance of sentence-level and token-level tasks on a wide range of NLP tasks. Unlike other pre-training approaches that useUnidirectional Language Models (GPT), BERT uses Masked Language Models to achieve better performance on both left-to-right and right-to-left tasks. The paper also demonstrates that pre-trained representations reduce the need for many heavily-engineered task-specific architectures, and advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please summarize this paper: \"\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt+article, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f751ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a new approach to pre-training language representations, called BERT (BidirectionalEncoder Representationsfrom Transformers). BERT uses Masked Language Models (MLMs) to enable pre-trained deep bidirectional representations, which are essential for improving many natural language processing tasks, including sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), as well as token-level tasks such as named entity recognition (Tjong Kim Sang and De Meulder, 2003) and question answering (Rajpurkar et al., 2016). BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. The paper also proposes a new feature-based approach to apply pre-trained language representations to downstream tasks, which is currently limited by the unidirectional nature of standard language models.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Provide a TLDR summary for the text: \"\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt+article, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756ed37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a recent paper, Dai and Le (2015) proposed aMasked Language Model (MLM) pre-training objective for language models, which enabled them to learn general language representations that were better suited for various natural language processing tasks. This pre-training approach was also used by Peters et al. (2018a) and Radford et al. (2018), but with different architectures and task-specific settings.\n",
      "\n",
      "Howard and Ruder (2018) proposed a feature-based approach to apply pre-trained language representations to下游 tasks, using task-specific architectures and additional parameters. However, this approach had limitations, particularly for token-level tasks such as named entity recognition and question answering, where it was crucial to incorporate context from both directions.\n",
      "\n",
      "In this paper, we propose BERT, a bidirectionalEncoder Representationsfrom Transformers (BERT) model, which alleviates the limitations of the previous approaches by using aMasked Language Model (MLM) pre-training objective. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and right context, which allows us to pre-train a deep bidirectional Transformer. We also use a “next sentence prediction” task that jointly pre-trains text-pair representations, which improve the performance of BERT on下游 tasks.\n",
      "\n",
      "Our contributions are as follows:\n",
      "\n",
      "1.  We demonstrate the importance of bidirectional pre-training for language representations. Unlike previous approaches, we use aMasked Language Model (MLM) pre-training objective to enable pre-trained deep bidirectional representations.\n",
      "\n",
      "2.  We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n",
      "\n",
      "3.  BERT advances the state of the art for elevenNLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n",
      "\n",
      "The main advantage of using theMasked Language Model (MLM) pre-training objective is that it enables the representation to learn general language representations that are better suited for various natural language processing tasks. This pre-training approach has been shown to be effective for improving many natural language processing tasks, including sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), as well as token-level tasks such as named entity recognition and question answering (Tjong Kim Sang and De Meulder, 2003;Rajpurkar et al., 2016).\n",
      "\n",
      "The MLM objective enables the representation to learn general context from both directions, which allows us to pre-train a deep bidirectional Transformer. Unlike previous approaches, we use aMasked Language Model (MLM) pre-training objective to enable pre-trained deep bidirectional representations. We also use a “next sentence prediction” task that jointly pre-trains text-pair representations, which improve the performance of BERT on下游 tasks. Our contributions are as follows:\n",
      "\n",
      "1.  We demonstrate the importance of bidirectional pre-training for language representations. Unlike previous approaches, we use aMasked Language Model (MLM) pre-training objective to enable pre-trained deep bidirectional representations.\n",
      "\n",
      "2.  We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n",
      "\n",
      "3.  BERT advances the state of the art for elevenNLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n",
      "\n",
      "The main advantage of using theMasked Language Model (MLM) pre-training objective is that it enables the representation to learn general language representations that are better suited for various natural language processing tasks. This pre-training approach has been shown to be effective for improving many natural language processing tasks, including sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), as well as token-level tasks such as named entity recognition and question answering (Tjong Kim Sang and De Meulder, 2003;Rajpurkar et al., 2016).\n",
      "\n",
      "The MLM objective enables the representation to learn general context from both directions, which allows us to pre-train a deep bidirectional Transformer. Unlike previous approaches, we use aMasked Language Model (MLM) pre-training objective to enable pre-trained deep bidirectional representations. We also use a “next sentence prediction” task that jointly pre-trains text-pair representations, which improve the performance of BERT on下游 tasks. Our contributions are as follows:\n",
      "\n",
      "1.  We demonstrate the importance of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Provide a TLDR summary for the scientific introduction text, focus on masked language model: \"\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt+article, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45501f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), as well as token-level tasks such as named entity recognition and question answering (Tjong Kim Sang and De Meulder, 2003;Rajpurkar et al., 2016). There are two existing strategies for applying pre-trained language representations to down-stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peterset al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The main limitations of the current approaches are: • Theunidirectional nature of standard language models, which limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only affect previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). such limitations are sub-optimal for sentence-level tasks, and could be very harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we propose BERT: BidirectionalEncoder Representationsfrom Transformers, which alleviates the previously mentioned unidirectional constraints by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to improving the fine-tuning based approach, we show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. BERT advances the state of the art for elevenNLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert. We also provide a comparison of BERT with other pre-trained language models and other fine-tuning based approaches.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please polish the text extracted from PDF, which talks about natural language processing, try to split concatenated words: \"\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt+article, history=[])\n",
    "print(response)\n",
    "\n",
    "polished_article = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7071e",
   "metadata": {},
   "source": [
    "Introduction Language model pre-training has been shown tobe effective for improving many natural languageprocessing tasks (Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018). These include sentence-level tasks such asnatural language inference (Bowman et al., 2015;Williams et al., 2018) and paraphrasing (Dolanand Brockett, 2005), which aim to predict the re-lationships between sentences by analyzing themholistically, as well as token-level tasks such asnamed entity recognition and question answering,where models are required to produce ﬁne-grainedoutput at the token level (Tjong Kim Sang andDe Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and ﬁne-tuning. Thefeature-based approach, such as ELMo (Peterset al., 2018a), uses task-speciﬁc architectures thatinclude the pre-trained representations as addi-tional features. The ﬁne-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018), introduces minimaltask-speciﬁc parameters, and is trained on thedownstream tasks by simply ﬁne-tuning all pre-trained parameters. The two approaches share thesame objective function during pre-training, wherethey use unidirectional language models to learngeneral language representations. We argue that current techniques restrict thepower of the pre-trained representations, espe-cially for the ﬁne-tuning approaches. The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training. Forexample, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only at-tend to previous tokens in the self-attention layersof the Transformer (Vaswani et al., 2017). Such re-strictions are sub-optimal for sentence-level tasks,and could be very harmful when applying ﬁne-tuning based approaches to token-level tasks suchas question answering, where it is crucial to incor-porate context from both directions. In this paper, we improve the ﬁne-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representationsfrom Transformers.BERT alleviates the previously mentioned unidi-rectionality constraint by using a “masked lan-guage model” (MLM) pre-training objective, in-spired by the Cloze task (Taylor, 1953). Themasked language model randomly masks some ofthe tokens from the input, and the objective is topredict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM ob-jective enables the representation to fuse the leftand the right context, which allows us to pre-In addi-train a deep bidirectional Transformer.tion to the masked language model, we also usea “next sentence prediction” task that jointly pre-trains text-pair representations. The contributionsof our paper are as follows: • We demonstrate the importance of bidirectionalpre-training for language representations. Un-like Radford et al. (2018), which uses unidirec-tional language models for pre-training, BERTuses masked language models to enable pre-trained deep bidirectional representations. Thisis also in contrast to Peters et al. (2018a), whichuses a shallow concatenation of independentlytrained left-to-right and right-to-left LMs. • We show that pre-trained representations reducethe need for many heavily-engineered task-speciﬁc architectures. BERT is the ﬁrst ﬁne-tuning based representation model that achievesstate-of-the-art performance on a large suiteof sentence-level and token-level tasks, outper-forming many task-speciﬁc architectures. • BERT advances the state of the art for elevenNLP tasks. The code and pre-trained mod-els are available at https://github.com/google-research/bert.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417cf658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we propose BERT: BidirectionalEncoder Representationsfrom Transformers (BERT), a fine-tuning based representation model that alleviates the limitations of current pre-trained language models. BERT is an advance on the state of the art for elevenNLP tasks by achieving state-of-the-art performance using aMasked Language Model (MLM) objective. The MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. BERT is the first fine-tuning based representation model that achieves this level of performance, outperforming many task-specific architectures. We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures, making BERT an essential tool for a wide range of natural language processing tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Provide a TLDR summary for the scientific introduction text, focus on masked language model: \"\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt+polished_article, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15359a",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "ChatGPT can do well in (1) full text polishing, like split concatenated words, (2) understand natural language instructions, (3) better keyword controlled summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272162f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bd44a059ff45be975d09ccf8fa1736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/yanxia/.cache/huggingface/datasets/allenai___csv/allenai--mup-c30ba3347ec8183d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90800eccbe47400aa128c1ce08042910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Validation 3604\n",
      " ** running 5 Thu Mar 30 14:24:53 2023\n",
      " ** running 10 Thu Mar 30 14:25:16 2023\n",
      " ** running 15 Thu Mar 30 14:25:44 2023\n",
      " ** running 20 Thu Mar 30 14:26:08 2023\n",
      " ** running 25 Thu Mar 30 14:26:32 2023\n",
      " ** running 30 Thu Mar 30 14:26:51 2023\n",
      " ** running 35 Thu Mar 30 14:27:17 2023\n",
      " ** running 40 Thu Mar 30 14:27:39 2023\n",
      " ** running 45 Thu Mar 30 14:28:04 2023\n",
      " ** running 50 Thu Mar 30 14:28:29 2023\n",
      " ** running 55 Thu Mar 30 14:28:55 2023\n",
      " ** running 60 Thu Mar 30 14:29:29 2023\n",
      " ** running 65 Thu Mar 30 14:29:59 2023\n",
      " ** running 70 Thu Mar 30 14:30:25 2023\n",
      " ** running 75 Thu Mar 30 14:30:48 2023\n",
      " ** running 80 Thu Mar 30 14:31:26 2023\n",
      " ** running 85 Thu Mar 30 14:31:52 2023\n",
      " ** running 90 Thu Mar 30 14:32:16 2023\n",
      " ** running 95 Thu Mar 30 14:32:38 2023\n",
      " ** running 100 Thu Mar 30 14:33:16 2023\n",
      " ** running 105 Thu Mar 30 14:33:38 2023\n",
      " ** running 110 Thu Mar 30 14:34:00 2023\n",
      " ** running 115 Thu Mar 30 14:34:30 2023\n",
      " ** running 120 Thu Mar 30 14:34:54 2023\n",
      " ** running 125 Thu Mar 30 14:35:20 2023\n",
      " ** running 130 Thu Mar 30 14:35:56 2023\n",
      " ** running 135 Thu Mar 30 14:36:24 2023\n",
      " ** running 140 Thu Mar 30 14:36:51 2023\n",
      " ** running 145 Thu Mar 30 14:37:17 2023\n",
      " ** running 150 Thu Mar 30 14:37:38 2023\n",
      " ** running 155 Thu Mar 30 14:38:04 2023\n",
      " ** running 160 Thu Mar 30 14:38:26 2023\n",
      " ** running 165 Thu Mar 30 14:38:49 2023\n",
      " ** running 170 Thu Mar 30 14:39:12 2023\n",
      " ** running 175 Thu Mar 30 14:39:34 2023\n",
      " ** running 180 Thu Mar 30 14:39:58 2023\n",
      " ** running 185 Thu Mar 30 14:40:21 2023\n",
      " ** running 190 Thu Mar 30 14:40:42 2023\n",
      " ** running 195 Thu Mar 30 14:41:07 2023\n",
      " ** running 200 Thu Mar 30 14:41:29 2023\n",
      " ** running 205 Thu Mar 30 14:41:58 2023\n",
      " ** running 210 Thu Mar 30 14:42:23 2023\n",
      " ** running 215 Thu Mar 30 14:42:48 2023\n",
      " ** running 220 Thu Mar 30 14:43:49 2023\n",
      " ** running 225 Thu Mar 30 14:44:18 2023\n",
      " ** running 230 Thu Mar 30 14:44:52 2023\n",
      " ** running 235 Thu Mar 30 14:45:25 2023\n",
      " ** running 240 Thu Mar 30 14:46:00 2023\n",
      " ** running 245 Thu Mar 30 14:46:31 2023\n",
      " ** running 250 Thu Mar 30 14:46:53 2023\n",
      " ** running 255 Thu Mar 30 14:47:26 2023\n",
      " ** running 260 Thu Mar 30 14:47:50 2023\n",
      " ** running 265 Thu Mar 30 14:48:13 2023\n",
      " ** running 270 Thu Mar 30 14:48:32 2023\n",
      " ** running 275 Thu Mar 30 14:48:54 2023\n",
      " ** running 280 Thu Mar 30 14:49:19 2023\n",
      " ** running 285 Thu Mar 30 14:49:42 2023\n",
      " ** running 290 Thu Mar 30 14:50:11 2023\n",
      " ** running 295 Thu Mar 30 14:52:00 2023\n",
      " ** running 300 Thu Mar 30 14:52:36 2023\n",
      " ** running 305 Thu Mar 30 14:52:59 2023\n",
      " ** running 310 Thu Mar 30 14:53:22 2023\n",
      " ** running 315 Thu Mar 30 14:53:43 2023\n",
      " ** running 320 Thu Mar 30 14:54:06 2023\n",
      " ** running 325 Thu Mar 30 14:54:44 2023\n",
      " ** running 330 Thu Mar 30 14:55:11 2023\n",
      " ** running 335 Thu Mar 30 14:55:45 2023\n",
      " ** running 340 Thu Mar 30 14:56:10 2023\n",
      " ** running 345 Thu Mar 30 14:56:42 2023\n",
      " ** running 350 Thu Mar 30 14:57:01 2023\n",
      " ** running 355 Thu Mar 30 14:57:27 2023\n",
      " ** running 360 Thu Mar 30 14:57:51 2023\n",
      " ** running 365 Thu Mar 30 14:58:18 2023\n",
      " ** running 370 Thu Mar 30 14:58:40 2023\n",
      " ** running 375 Thu Mar 30 14:59:06 2023\n",
      " ** running 380 Thu Mar 30 14:59:27 2023\n",
      " ** running 385 Thu Mar 30 14:59:49 2023\n",
      " ** running 390 Thu Mar 30 15:00:17 2023\n",
      " ** running 395 Thu Mar 30 15:00:43 2023\n",
      " ** running 400 Thu Mar 30 15:01:08 2023\n",
      " ** running 405 Thu Mar 30 15:01:38 2023\n",
      " ** running 410 Thu Mar 30 15:02:08 2023\n",
      " ** running 415 Thu Mar 30 15:02:40 2023\n",
      " ** running 420 Thu Mar 30 15:02:59 2023\n",
      " ** running 425 Thu Mar 30 15:03:23 2023\n",
      " ** running 430 Thu Mar 30 15:03:45 2023\n",
      " ** running 435 Thu Mar 30 15:04:14 2023\n",
      " ** running 440 Thu Mar 30 15:04:38 2023\n",
      " ** running 445 Thu Mar 30 15:05:04 2023\n",
      " ** running 450 Thu Mar 30 15:05:29 2023\n",
      " ** running 455 Thu Mar 30 15:06:12 2023\n",
      " ** running 460 Thu Mar 30 15:07:03 2023\n",
      " ** running 465 Thu Mar 30 15:07:25 2023\n",
      " ** running 470 Thu Mar 30 15:07:47 2023\n",
      " ** running 475 Thu Mar 30 15:08:11 2023\n",
      " ** running 480 Thu Mar 30 15:08:44 2023\n",
      " ** running 485 Thu Mar 30 15:09:11 2023\n",
      " ** running 490 Thu Mar 30 15:09:47 2023\n",
      " ** running 495 Thu Mar 30 15:10:14 2023\n",
      " ** running 500 Thu Mar 30 15:10:55 2023\n",
      " ** running 505 Thu Mar 30 15:11:22 2023\n",
      " ** running 510 Thu Mar 30 15:11:48 2023\n",
      " ** running 515 Thu Mar 30 15:12:16 2023\n",
      " ** running 520 Thu Mar 30 15:12:37 2023\n",
      " ** running 525 Thu Mar 30 15:13:04 2023\n",
      " ** running 530 Thu Mar 30 15:13:32 2023\n",
      " ** running 535 Thu Mar 30 15:13:55 2023\n",
      " ** running 540 Thu Mar 30 15:14:20 2023\n",
      " ** running 545 Thu Mar 30 15:14:48 2023\n",
      " ** running 550 Thu Mar 30 15:15:17 2023\n",
      " ** running 555 Thu Mar 30 15:15:48 2023\n",
      " ** running 560 Thu Mar 30 15:16:17 2023\n",
      " ** running 565 Thu Mar 30 15:16:43 2023\n",
      " ** running 570 Thu Mar 30 15:17:24 2023\n",
      " ** running 575 Thu Mar 30 15:17:59 2023\n",
      " ** running 580 Thu Mar 30 15:18:33 2023\n",
      " ** running 585 Thu Mar 30 15:18:59 2023\n",
      " ** running 590 Thu Mar 30 15:19:28 2023\n",
      " ** running 595 Thu Mar 30 15:20:01 2023\n",
      " ** running 600 Thu Mar 30 15:20:27 2023\n",
      " ** running 605 Thu Mar 30 15:20:56 2023\n",
      " ** running 610 Thu Mar 30 15:21:31 2023\n",
      " ** running 615 Thu Mar 30 15:21:59 2023\n",
      " ** running 620 Thu Mar 30 15:22:21 2023\n",
      " ** running 625 Thu Mar 30 15:22:43 2023\n",
      " ** running 630 Thu Mar 30 15:23:09 2023\n",
      " ** running 635 Thu Mar 30 15:23:32 2023\n",
      " ** running 640 Thu Mar 30 15:23:59 2023\n",
      " ** running 645 Thu Mar 30 15:24:24 2023\n",
      " ** running 650 Thu Mar 30 15:24:49 2023\n",
      " ** running 655 Thu Mar 30 15:25:15 2023\n",
      " ** running 660 Thu Mar 30 15:25:38 2023\n",
      " ** running 665 Thu Mar 30 15:26:02 2023\n",
      " ** running 670 Thu Mar 30 15:26:26 2023\n",
      " ** running 675 Thu Mar 30 15:26:54 2023\n",
      " ** running 680 Thu Mar 30 15:27:23 2023\n",
      " ** running 685 Thu Mar 30 15:27:47 2023\n",
      " ** running 690 Thu Mar 30 15:28:11 2023\n",
      " ** running 695 Thu Mar 30 15:28:47 2023\n",
      " ** running 700 Thu Mar 30 15:29:13 2023\n",
      " ** running 705 Thu Mar 30 15:29:39 2023\n",
      " ** running 710 Thu Mar 30 15:30:11 2023\n",
      " ** running 715 Thu Mar 30 15:30:35 2023\n",
      " ** running 720 Thu Mar 30 15:31:05 2023\n",
      " ** running 725 Thu Mar 30 15:31:32 2023\n",
      " ** running 730 Thu Mar 30 15:32:10 2023\n",
      " ** running 735 Thu Mar 30 15:33:08 2023\n",
      " ** running 740 Thu Mar 30 15:33:25 2023\n",
      " ** running 745 Thu Mar 30 15:33:45 2023\n",
      " ** running 750 Thu Mar 30 15:34:16 2023\n",
      " ** running 755 Thu Mar 30 15:34:51 2023\n",
      " ** running 760 Thu Mar 30 15:35:12 2023\n",
      " ** running 765 Thu Mar 30 15:35:36 2023\n",
      " ** running 770 Thu Mar 30 15:36:03 2023\n",
      " ** running 775 Thu Mar 30 15:36:28 2023\n",
      " ** running 780 Thu Mar 30 15:36:52 2023\n",
      " ** running 785 Thu Mar 30 15:37:44 2023\n",
      " ** running 790 Thu Mar 30 15:38:09 2023\n",
      " ** running 795 Thu Mar 30 15:38:33 2023\n",
      " ** running 800 Thu Mar 30 15:38:59 2023\n",
      " ** running 805 Thu Mar 30 15:39:28 2023\n",
      " ** running 810 Thu Mar 30 15:39:51 2023\n",
      " ** running 815 Thu Mar 30 15:40:09 2023\n",
      " ** running 820 Thu Mar 30 15:40:35 2023\n",
      " ** running 825 Thu Mar 30 15:40:59 2023\n",
      " ** running 830 Thu Mar 30 15:41:26 2023\n",
      " ** running 835 Thu Mar 30 15:41:49 2023\n",
      " ** running 840 Thu Mar 30 15:42:11 2023\n",
      " ** running 845 Thu Mar 30 15:42:35 2023\n",
      " ** running 850 Thu Mar 30 15:42:59 2023\n",
      " ** running 855 Thu Mar 30 15:43:32 2023\n",
      " ** running 860 Thu Mar 30 15:44:29 2023\n",
      " ** running 865 Thu Mar 30 15:45:28 2023\n",
      " ** running 870 Thu Mar 30 15:45:55 2023\n",
      " ** running 875 Thu Mar 30 15:46:18 2023\n",
      " ** running 880 Thu Mar 30 15:46:44 2023\n",
      " ** running 885 Thu Mar 30 15:47:05 2023\n",
      " ** running 890 Thu Mar 30 15:47:31 2023\n",
      " ** running 895 Thu Mar 30 15:47:57 2023\n",
      " ** running 900 Thu Mar 30 15:48:26 2023\n",
      " ** running 905 Thu Mar 30 15:48:56 2023\n",
      " ** running 910 Thu Mar 30 15:49:25 2023\n",
      " ** running 915 Thu Mar 30 15:49:52 2023\n",
      " ** running 920 Thu Mar 30 15:50:21 2023\n",
      " ** running 925 Thu Mar 30 15:51:18 2023\n",
      " ** running 930 Thu Mar 30 15:51:45 2023\n",
      " ** running 935 Thu Mar 30 15:52:11 2023\n",
      " ** running 940 Thu Mar 30 15:52:32 2023\n",
      " ** running 945 Thu Mar 30 15:52:59 2023\n",
      " ** running 950 Thu Mar 30 15:53:26 2023\n",
      " ** running 955 Thu Mar 30 15:53:48 2023\n",
      " ** running 960 Thu Mar 30 15:54:14 2023\n",
      " ** running 965 Thu Mar 30 15:54:39 2023\n",
      " ** running 970 Thu Mar 30 15:54:57 2023\n",
      " ** running 975 Thu Mar 30 15:55:25 2023\n",
      " ** running 980 Thu Mar 30 15:55:56 2023\n",
      " ** running 985 Thu Mar 30 15:56:14 2023\n",
      " ** running 990 Thu Mar 30 15:56:36 2023\n",
      " ** running 995 Thu Mar 30 15:57:00 2023\n",
      " ** running 1000 Thu Mar 30 15:57:22 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_fmeasure': tensor(0.2826), 'rouge1_precision': tensor(0.2920), 'rouge1_recall': tensor(0.3101), 'rouge2_fmeasure': tensor(0.0507), 'rouge2_precision': tensor(0.0520), 'rouge2_recall': tensor(0.0572), 'rougeL_fmeasure': tensor(0.1728), 'rougeL_precision': tensor(0.1792), 'rougeL_recall': tensor(0.1907), 'rougeLsum_fmeasure': tensor(0.2536), 'rougeLsum_precision': tensor(0.2627), 'rougeLsum_recall': tensor(0.2783)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "\n",
    "# Load the MuP dataset\n",
    "_, test_dataset = load_dataset(\"allenai/mup\", split=['train', 'validation'])\n",
    "print(\"#Validation\", len(test_dataset))\n",
    "\n",
    "\n",
    "# Initialize the Rouge metric from torchmetrics\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "def generate_summary(article):\n",
    "\n",
    "    prompt = \"Please summarize this paper: \"\n",
    "\n",
    "    summary, history = model.chat(tokenizer, prompt+article, history=[])\n",
    "\n",
    "    return summary\n",
    "\n",
    "num_examples = 0\n",
    "predictions, targets = [], []\n",
    "\n",
    "with open(\"./output-chatglm-dev.json\", \"w\") as json_file:\n",
    "    for example in test_dataset:\n",
    "        num_examples += 1\n",
    "        article = example[\"text\"]\n",
    "        reference_summary = example[\"summary\"]\n",
    "\n",
    "        # Generate summary\n",
    "        generated_summary = generate_summary(article[:1000])\n",
    "        predictions.append(generated_summary)\n",
    "        targets.append(reference_summary)\n",
    "        json.dump(str(num_examples)+\"\\t\"+generated_summary+\"\\t\"+reference_summary, json_file)\n",
    "\n",
    "        if num_examples % 5 == 0:\n",
    "            print(\" ** running\", num_examples, time.asctime())\n",
    "        if num_examples > 999: break\n",
    "\n",
    "    \n",
    "# Compute Rouge scores\n",
    "rouge_score = rouge(predictions, targets)\n",
    "\n",
    "print(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2f18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/yanxia/.cache/huggingface/datasets/allenai___csv/allenai--mup-c30ba3347ec8183d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da91743a2b2c4707a7879385206ef117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Validation 3604\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set the environment variable TOKENIZERS_PARALLELISM to true or false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # or \"false\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "# Initialize the Rouge metric from torchmetrics\n",
    "rouge = ROUGEScore()\n",
    "bertscore = BERTScore()\n",
    "\n",
    "# Check if a GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MuP dataset\n",
    "_, test_dataset = load_dataset(\"allenai/mup\", split=['train', 'validation'])\n",
    "print(\"#Validation\", len(test_dataset))\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "def collate_fn(batch):\n",
    "    articles = [\"Summarize: \" + example[\"text\"] for example in batch]\n",
    "    highlights = [example[\"summary\"] for example in batch]\n",
    "    return articles, highlights\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2759f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/yanxia/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/yanxia/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels_parallel.c -shared -o /home/yanxia/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels_parallel.so\n",
      "Kernels compiled : /home/yanxia/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels_parallel.so\n",
      "Load kernel : /home/yanxia/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 20\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 512, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_3819809/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1653646358.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">22</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_3819809/1653646358.py'</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_3819809/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1653646358.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_summary_batch</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_3819809/1653646358.py'</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yanxia/anaconda3/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_mode.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.clone():                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cast(F, decorate_context)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrap_generator</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, func):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1474</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1471 │   │   │   │   </span>**model_kwargs,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1472 │   │   │   </span>)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1473 │   │   │   # 13. run beam search</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1474 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.beam_search(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1475 │   │   │   │   </span>input_ids,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1476 │   │   │   │   </span>beam_scorer,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1477 │   │   │   │   </span>logits_processor=logits_processor,                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2809</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">beam_search</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2806 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2807 │   │   │   │   │   </span>this_peer_finished = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2808 │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2809 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>sequence_outputs = beam_scorer.finalize(                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2810 │   │   │   </span>input_ids,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2811 │   │   │   </span>beam_scores,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2812 │   │   │   </span>next_tokens,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">beam_search.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">379</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">finalize</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">376 │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">377 │   │   # fill with hypotheses and eos_token_id if the latter fits in</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">378 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, (hypo, best_idx) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(best, best_indices)):              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>379 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>decoded[i, : sent_lengths[i]] = hypo                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">380 │   │   │   </span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">381 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> indices <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">382 │   │   │   │   </span>indices[i, : <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(best_idx)] = torch.tensor(best_idx)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>The expanded size of the tensor <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">)</span> must match the existing size <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">513</span><span style=\"font-weight: bold\">)</span> at \n",
       "non-singleton dimension <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.  Target sizes: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">]</span>.  Tensor sizes: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">513</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_3819809/\u001b[0m\u001b[1;33m1653646358.py\u001b[0m:\u001b[94m22\u001b[0m in \u001b[92m<module>\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_3819809/1653646358.py'\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_3819809/\u001b[0m\u001b[1;33m1653646358.py\u001b[0m:\u001b[94m12\u001b[0m in \u001b[92mgenerate_summary_batch\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_3819809/1653646358.py'\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yanxia/anaconda3/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33mgrad_mode.py\u001b[0m:\u001b[94m27\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mdecorate_context\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.clone():                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 27 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(F, decorate_context)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_wrap_generator\u001b[0m(\u001b[96mself\u001b[0m, func):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1474\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1471 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_kwargs,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1472 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1473 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 13. run beam search\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1474 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.beam_search(                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1475 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1476 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbeam_scorer,                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1477 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2809\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mbeam_search\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2806 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2807 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mthis_peer_finished = \u001b[94mTrue\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2808 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2809 \u001b[2m│   │   \u001b[0msequence_outputs = beam_scorer.finalize(                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2810 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2811 \u001b[0m\u001b[2m│   │   │   \u001b[0mbeam_scores,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2812 \u001b[0m\u001b[2m│   │   │   \u001b[0mnext_tokens,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/yanxia/anaconda3/lib/python3.9/site-packages/transformers/generation/\u001b[0m\u001b[1;33mbeam_search.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m379\u001b[0m in \u001b[92mfinalize\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m376 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m377 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# fill with hypotheses and eos_token_id if the latter fits in\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m378 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i, (hypo, best_idx) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(\u001b[96mzip\u001b[0m(best, best_indices)):              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m379 \u001b[2m│   │   │   \u001b[0mdecoded[i, : sent_lengths[i]] = hypo                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m381 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m indices \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m382 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mindices[i, : \u001b[96mlen\u001b[0m(best_idx)] = torch.tensor(best_idx)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mThe expanded size of the tensor \u001b[1m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1m)\u001b[0m must match the existing size \u001b[1m(\u001b[0m\u001b[1;36m513\u001b[0m\u001b[1m)\u001b[0m at \n",
       "non-singleton dimension \u001b[1;36m0\u001b[0m.  Target sizes: \u001b[1m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m.  Tensor sizes: \u001b[1m[\u001b[0m\u001b[1;36m513\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().to(device)\n",
    "\n",
    "\n",
    "def generate_summary_batch(articles):\n",
    "\n",
    "    inputs = tokenizer(articles, return_tensors=\"pt\", max_length=512, padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        #summary_ids = model.generate(**inputs, max_length=200, num_beams=4, length_penalty=2.0, temperature=0.2)\n",
    "        summary_ids = model.generate(**inputs, num_beams=4, length_penalty=2.0, temperature=0.2)\n",
    "    summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "    return summaries\n",
    "\n",
    "# Evaluate the model with DataLoader\n",
    "num_examples = 0\n",
    "predictions, targets = [], []\n",
    "\n",
    "for articles, reference_summaries in dataloader:\n",
    "    # Generate summaries\n",
    "    generated_summaries = generate_summary_batch(articles)\n",
    "\n",
    "    predictions.extend(generated_summaries)\n",
    "    targets.extend(reference_summaries)\n",
    "    \n",
    "    num_examples += len(articles)\n",
    "    print(\"# running\", num_examples, time.asctime())\n",
    "    if num_examples > 999: break\n",
    "\n",
    "# Dump the data to a JSON file\n",
    "with open(\"./output-chatglm-dev.json\", \"w\") as json_file:\n",
    "    json.dump(predictions, json_file, indent=4)\n",
    "    json.dump(targets, json_file, indent=4)\n",
    "    \n",
    "rouge_score = rouge(predictions, targets)\n",
    "\n",
    "print(rouge_score)\n",
    "\n",
    "bert_score = bertscore(predictions, targets)\n",
    "bert_score = [{key: sum(bert_score[key])/len(bert_score[key])} for key in bert_score]\n",
    "\n",
    "print(bert_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96363c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
