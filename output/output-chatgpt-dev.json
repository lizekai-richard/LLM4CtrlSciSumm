"1\tThe paper discusses federated learning (FL), which is a distributed optimization technique used for large-scale machine learning problems where multiple clients collaborate under the coordination of a central server. FL is suitable for mobile and edge device applications as it allows sensitive individual data to remain on the device and not be directly transferred. However, FL lacks a formal privacy guarantee, and the paper discusses the need for privacy-preserving FL. The authors propose a new privacy-preserving FL framework that uses differential privacy and secure multi-party computation techniques to protect the privacy of client data while still allowing for effective collaboration. The proposed framework is evaluated on a real-world dataset, and the results show that it provides strong privacy guarantees while maintaining high accuracy.\tThis paper studies FL under local differential privacy constraints. They identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high\u0002dimensional compatibility, and develop a gradient-based learning algorithm sqSGD that addresses both concerns. They improve the base algorithm in two ways: First, apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs. Secondly, utilize randomized rotation as a preprocessing step to reduce quantization error. ""2\tThe paper discusses federated learning (FL), which is a distributed optimization technique used for large-scale learning or estimation scenarios where multiple clients collaborate under the coordination of a central server. Each client's raw data is stored locally and not exchanged or transferred, ensuring privacy. FL is suitable for mobile and edge device applications and has been deployed in various industries. However, FL lacks a formal privacy guarantee, and the paper discusses the need for privacy-preserving FL and proposes a new framework for it. The proposed framework uses differential privacy and secure multi-party computation to ensure privacy while maintaining the accuracy of the FL model. The paper concludes by discussing the potential applications of the proposed framework.\tThis paper studies a low communication algorithm for multivariate mean estimation in the federated learning setting with differentially private communication. The algorithm uses quantization and dimension subsampling (only reporting some coordinates of the vector) to lower communication and randomized rotation (essentially applying a random orthogonal matrix) to reduce quantization error. They then apply this algorithm to ERM, using it as a subroutine in SGD. They experimentally explore the behavior of their algorithm on a number of benchmark datasets. They consider how the performance changes as they vary epsilon, the discretization parameter and the number of epochs (in SGD). ""3\tThe paper discusses federated learning (FL), which is a distributed optimization technique used for large-scale learning or estimation scenarios where multiple clients collaborate to solve a machine learning problem under the coordination of a central server. FL is particularly suitable for mobile and edge device applications since individual data is not exchanged or transferred. However, FL lacks a formal privacy guarantee, even though it offers practical privacy improvements over centralizing all the training data. The paper highlights the need for privacy-preserving FL and proposes a new framework that uses differential privacy to provide strong privacy guarantees while maintaining the accuracy of the model.\tThe paper proposed a differentially private training algorithm for federated learning. The target is to achieve communication reduction while keeping differential privacy during training. The proposed algorithm adds a few new components to SGD, including a privacy mechanism, a random rotation to reduce quantization error, a gradient coordinate selection mechanism to reduce communication/computation. Experiments with high \\epsilon local differentially privacy guarantees are conducted. The proposed algorithm outperforms a baseline algorithm.""4\tThe paper discusses the use of self-attention networks (SANs) in natural language processing. SANs use positional encoding to encode order dependencies in language and perform self-attentive functions to learn language representation. SAN-based models have achieved state-of-the-art performance in many natural language processing tasks. The paper highlights the advantages of SANs over complex recurrent or convolutional neural networks.\tThis submission works on the neural machine translation problem. The authors extend the previous works on leveraging language statistics or prior knowledge (SMT model or whatever) in LSTM based NMT models in self-attention based NMT models, Transformer model. The authors propose two alternatives to incorporate prior knowledge, which are the word frequency information for the monolingual data and the prior translation lexicon information for the bilingual data. These resources are integrated into the hidden representations from the self-attention computations and then the two output hidden representations are gated together for upper computations. The experiments are conducted on two typical NMT datasets: WMT14 En->De and WMT17 Zh->En, the results show that the proposed method can improve the NMT model performances. ""5\tThis paper discusses self-attention networks (SANs) and their increasing popularity in natural language processing. SANs use a positional encoding mechanism to encode order dependencies in language and perform self-attentive functions in parallel to learn language representation. SAN-based models have achieved state-of-the-art performance in many natural language processing tasks.\tThis paper proposes a method to introduce **prior knowledge** into Transformer-based sentence encoders, here in the context of neural machine translation (NMT). More concretely, the prior knowledge is represented in the form of a matrix $\\boldsymbol{M}$, where each row denotes a vector of prior knowledge associated with each word $x_i$. The prior knowledge matrix $\\boldsymbol{M}$ is then represented as a (key, value) pair that can be attended by the query matrix $\\boldsymbol{Q}$ (the same query matrix as used in the main NMT component) using a standard Transformer self-attention mechanism. This procedure results in a prior knowledge representation matrix $\\boldsymbol{PK}$, which is then combined with the standard Transformer encoder output using a simple gating mechanism. ""6\tThe paper discusses the increasing popularity of self-attention networks (SANs) in the natural language processing community. SANs use positional encoding to encode order dependencies in language and perform self-attentive functions to learn language representation. SAN-based models have achieved state-of-the-art performance in many natural language processing tasks. The paper highlights the advantages of SANs over traditional recurrent or convolutional neural networks.\tThis paper presents a method for introducing prior knowledge into Transformer models. More specifically, the authors propose to use an additional self-attention block to incorporate prior knowledge about the word frequency and translation lexicon and use a gating mechanism to combine its output with that of the standard sefl-attention block. Experiments are conducted using English-to-German and Chinese-to-English translation datasets, and the results show the effectiveness of the proposed approach.""7\tThe paper discusses the challenges of deploying fully secure cyber-systems due to the complexity of modern-day software technology and the ample time attackers have to explore deployed systems. To address this, researchers have introduced the idea of proactive cyber defenses such as Moving Target Defense (MTD), where the defender shifts between various configurations of the cyber-system to make the attacker's knowledge useless. The paper emphasizes the importance of an optimal movement strategy to ensure that an MTD system is effective at maximizing security and minimizing the impact on the system's performance. The authors propose a game-theoretic formulation to model the cyber-system as a two-player game.\tThis paper proposes the game-theoretic model of Bayesian Stackelberg Markov Games (BSMGs), a generalization of Markov games, as a formalism for studying Moving Target Defense (MTD) systems, a type of defender-attacker game with applications to cybersecurity. An algorithm for finding the Stackelberg equilibrium in BSMGs, called Bayesian Strong Stackelberg Q-Learning (BSS-Q) is proposed, and an OpenAI Gym-style environment for testing the derived policies in particular MTD settings is introduced, which allows for empirical evaluation of the policies' effectiveness. The paper then shows experimental results supporting the BSS-Q algorithm's success at finding the Strong Stackelberg Equilibrium of BSMGs.""8\tThe paper discusses the challenges of deploying fully secure cyber-systems due to the complexity of modern-day software technology and the ample time attackers have to explore deployed systems before exploiting them. To address this, researchers have introduced the concept of Moving Target Defense (MTD), where the defender shifts between various configurations of the cyber-system to make the attacker's knowledge gathered during reconnaissance phase useless at attack time. The paper emphasizes the importance of an optimal movement strategy to ensure that an MTD system is effective at maximizing security and minimizing the impact on the system's performance. The paper proposes a game-theoretic formulation to model the cyber-system as a two-player game.\tThis paper studies the problem of learning how to adapt the defense methods in the domain of cybersecurity. The paper proposes a new model called Bayesian Stackelberg Markov Games (BSMG) to capture the uncertainty of the attacker's types as well as their strategic behaviors. The authors design Bayesian Strong Stackelberg Q-learning that can converge to the optimal movement policy for BSMG. The empirical studies verify the support the theoretical results.""9\tThis paper discusses the concept of Moving Target Defense (MTD) as a proactive cyber defense strategy to counter modern-day software technology's complexity, which makes deploying fully secure cyber-systems impossible. MTD involves shifting between various configurations of the cyber-system to render the attacker's knowledge gathered during the reconnaissance phase useless at attack time. The paper emphasizes the importance of an optimal movement strategy to ensure that an MTD system maximizes security and minimizes the impact on the system's performance. The authors propose a game-theoretic formulation to model the cyber-system as a two-player game, where the defender and attacker make strategic moves to achieve their objectives.\tThis paper introduces a Bayesian Stackelberg Markov Game (BSMG) model that considers a defender\u2019s uncertainty over attackers\u2019 types when implementing defensive strategies. It also proposes to use a Bayesian Strong Stackelberg Q-learning method to learn defense policies by first simulating an adversary to obtain feedback of an attack and then computing the Bayesian Strong Stackelberg Equilibrium for the BSMG with a solver. In this way, this work relaxes the assumption that the defender knows attackers\u2019 types in existing game-theoretic models for moving target defense.""10\tThe paper discusses how Variational Autoencoder (VAE) based frameworks have achieved state-of-the-art performance on unsupervised disentangled representation learning. However, these frameworks still face a trade-off between local orthogonality and data reconstruction, resulting in entangled representations. To address this challenge, the paper proposes a simple yet effective VAE ensemble framework consisting of multiple VAEs. The ensemble is based on the assumption that entangled representations are unique, while disentangled representations are similar up to a signed permutation transformation. Each model in the ensemble is trained on a different random seed and the final representation is obtained by averaging the outputs of all models. The proposed ensemble achieves better disentanglement performance than individual VAE models.\tThis paper proposes a simple and effective technique to improve disentanglement by coupling the latent spaces of different VAE models. It builds on Duan et al. (2019)\u2019s proposed method to rank the representations of different models. By learning a VAE ensemble with linear transformations between the latent spaces and an additional \u201ccross-model\u201d reconstruction loss, the authors show that they can achieve significantly better disentangling.""11\tThe paper discusses how Variational Autoencoder (VAE) based frameworks have achieved state-of-the-art performance in unsupervised disentangled representation learning. However, these frameworks still face a trade-off between local orthogonality and data reconstruction, resulting in entangled representations. To address this challenge, the paper proposes a VAE ensemble framework consisting of multiple VAEs based on the assumption that entangled representations are unique while disentangled representations are similar up to a signed permutation transformation. The proposed framework aims to improve the disentanglement performance of VAEs.\tThe authors introduce a novel VAE-based approach for unsupervised learning of disentangled representations of image data.  The approach trains an ensemble of VAEs along with pair-wise linear transformations between their latent spaces.  The objective includes the ELBO objectives for each VAE as well as two additional pressures:  (i) An L2 similarity objective that pressures samples from each VAE latent space to match under linear transformations samples from the other VAE latent spaces, and (ii) A cross-model decoding objective that encourages decoding accuracy of the linearly transformed latent samples.  The authors provide a theoretical argument that the linear transformations should learn to be orthogonal, and show some experimental results indicating that their model performs well compared to baselines when evaluated with an established disentangling metric.""12\tThis paper discusses the success of Variational Autoencoder (VAE) based frameworks in unsupervised disentangled representation learning. However, it also highlights the trade-off between local orthogonality and data reconstruction, which can lead to entangled representations. To address this challenge, the authors propose a VAE ensemble framework consisting of multiple VAEs that assume entangled representations are unique while disentangled representations are similar up to a signed permutation transformation. The proposed framework aims to improve the model's ability to learn disentangled representations and overcome the trade-off between local orthogonality and data reconstruction.\tThis submission proposes an ensemble framework to improve learning disentangled representations with Variational Autoencoders (VAEs). The approach builds on the assumption that entangled latent representations learned by VAEs show some \u201cuniqueness\u201d in their latent space structure, while disentangled representations exhibit some \u201csimilarity\u201d; an assumption corroborated by recent studies. On that basis, a VAE ensemble approach is proposed where several VAEs are connected through linear mappings between the individual latent spaces to encourage alignment of latent representations and thus disentanglement. A formal derivation of the framework is provided and the formal validity of the underlying assumption demonstrated. Furthermore, empirical evaluation of the proposed approach in comparison to the standard VAE, beta-VAE and FactorVAE on the datasets dSprites (main results, main text) and CelebA (appendix) is performed, yielding improved results on the FactorVAE disentanglement metric (all baseline methods considered) as well as the Distance to Orthogonality (DtO) metric (only standard VAE considered).""13\tThis paper discusses the concept of automated machine learning (AutoML), which seeks to automate the process of selecting the best algorithm for a given dataset. The authors propose a solution that uses a transformer-based language model to process text descriptions of datasets and algorithms, as well as a feature extractor to represent the data itself. By training the model on large-scale data, they aim to exploit human expertise to learn which datasets are similar and what algorithms perform best. The goal is to develop fast and efficient algorithms to accelerate the application of machine learning, making it more accessible to non-experts.\tThe paper proposes an efficient way to automatically choose the best or most suitable pipeline for different datasets. The proposed method can accelerate the AutoML using a pre-trained meta module. In particular, the AutoML job of a new supervised learning task can be accomplished without model evaluations, namely zero-shot / real-time AutoML. The meta module is constructed as a graph structure in which each node represents a dataset used for meta-training. ""14\tThis paper discusses the concept of automated machine learning (AutoML), which seeks to automate the process of selecting the best machine learning algorithm for a given dataset. The authors propose a solution that uses a transformer-based language model and a feature extractor to represent the data and learn which datasets are similar and which algorithms perform best. The goal is to develop fast and efficient algorithms to accelerate the application of machine learning by non-experts. The proposed solution allows the model to process text descriptions of datasets and algorithms, bringing in large-scale data. The authors suggest that their approach can help overcome the challenge of developing fast and efficient algorithms for AutoML.\tThe problem that the authors attempt to solve is to determine what ML pipeline will perform best on any new dataset, without incurring in the extra cost of actually running a large number of such pipelines, as is typically done in AutoML algorithms. The way this paper tackles the problem is to train a neural network that given a new dataset as input, will output a pipeline that is predicted to perform well on that dataset. This neural network is trained on other datasets, for which high performing pipelines are already known. Predicting a pipeline for a new dataset thus only require a forward pass through their NN.""15\tThis paper discusses the concept of automated machine learning (AutoML) and its goal of automating tasks such as selecting algorithms based on past experience and knowledge of datasets. The paper proposes a solution that utilizes a transformer-based language model and a feature extractor to represent the data and text descriptions of datasets and algorithms. This approach allows for the training of the model on large-scale data and the exploitation of human expertise to learn which datasets are similar and which algorithms perform best. The ultimate goal is to develop fast and efficient algorithms to accelerate the application of machine learning.\tThis paper presents a very interesting idea of utilizing the documentation for the data and the operators in the pipeline to generate meta-features for meta-learning. This is a very novel application of graph neural networks GNNs and language models for AutoML meta-learning. This view of meta-learning takes a very intuitive on a very high level. The use of the outputs of existing AutoML systems (such as auto-sklearn, TPOT, etc) is also very intuitive and well motivated. All these intuitive ideas are put together into a novel AutoML recommendation architecture making use of modern deep learning components.""16\tThis paper argues that gradient descent is a major obstacle to achieving compositional learning during neural network optimization. The optimization process tends to favor non-compositional solutions, violating the conditional independence property of compositionality. The authors suggest that approaches that only consider model architecture design are unlikely to achieve complete compositionality. This is the first study to investigate the relationship between compositional learning and gradient descent, and the authors hope it will provide new insights into compositional generalization and inspire new research directions for equipping machine learning models with human-level intelligence. The source code is included in the supplementary material.\tthe paper investigates what neural networks learn when trained with gradient descent, in case parts of the inputs are only partially relevant to the output. The main claim is that GD is what prevents compositionality. In a set of synthetic experiments it is shown that indeed GD learns to use all information in the input, which results in poor generalization ood when only a subset of it was relevant.""17\tThis paper argues that gradient descent is a major obstacle to achieving compositional learning during neural network optimization. The optimization process tends to favor non-compositional solutions, as gradient descent tries to use all available information from the input, violating the conditional independence property of compositionality. The authors suggest that approaches to achieving compositionality through model architecture design alone are unlikely to be successful. This is the first study to investigate the relationship between compositional learning and gradient descent, and the authors hope that it will provide new insights into compositional generalization and inspire new research directions in equipping machine learning models with human-level intelligence. The paper includes source code in the supplementary material.\tThis work analyzes the effect of gradient descent training on the compositionality of the learned model. It is shown that the gradient descent would use all the available information, even when it is redundant to learn the mapping from input to the output. It is then argued that the gradient descent training has the bias against compostionality despite the model architecture. Experiments are conducted on three simple benchmarks to demonstrate that when gradient descent trained model would use redundant information and not generalize compositionally. ""18\tThis paper argues that gradient descent is a major obstacle to achieving compositional learning during neural network optimization. The optimization process tends to favor non-compositional solutions, as gradient descent tries to use all available information from the input, violating the conditional independence property of compositionality. The authors suggest that approaches to achieving compositionality through model architecture design alone are unlikely to be successful. This is the first study to investigate the relationship between compositional learning and gradient descent, and the authors hope that it will provide new insights into compositional generalization and lead to new research directions for equipping machine learning models with human-level intelligence. The source code is included in the supplementary material.\tThis paper addresses the effects of gradient descent methods onto compositionality and compositional generalization of models. The authors claim that the optimization process imposes the models to deviate compositionality, which is defined with conditional independence among random variables of input, predicted output and the ground-truth. Since compositionality is one of important features of human intelligence, it has been interested widely in the field of AI/ML such as vision, language, neuro-symbolic approaches, common sense reasoning, disentangled representation, and the emergence conditions of compositionality. As it has been not much focused on the relationship with optimizers, it is fresh and interesting. However, it is not easy to figure out the position of this paper from two reasons: (1) the definitions on compositionality in this paper are not so compatible with recent related works, which mostly consider certain structures in models [ICLR19, JAIR20] or representative problems such as visual reasoning [CVPR17] and Raven progressive matrices [PNAS17]. (2) The authors do not consider quantitative approaches such as compositionality [ICLR19] or compositional generalization [ICLR20]. ""19\tThis paper discusses the recent attention given to knowledge graph (KG) representation learning for entity alignment, particularly the use of embedding-based methods which are considered more robust for highly-heterogeneous and cross-lingual scenarios. However, despite the significant improvement in these methods, there is little understanding of how they actually work. The paper examines the foundation of these methods, which rely on a small number of pre-aligned entities to connect the embedding spaces of two KGs. The authors define a typical paradigm from existing methods and analyze how the representation discrepancy between potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning.\tThe paper proposes NeoEA, an approach that further constrains KG embedding with ontology knowledge. The paper first tries to summarize the existing embedding-based entity alignment methods, stating that most of the methods choose TransE as scoring functions. But their embedding features are not aligned well compared to the neural-based or composition-based loss function. The paper, therefore, solves this problem by developing a new NeoEA architecture which shows that adding a KG-invariant ontology knowledge can minimize such difference. The experiment shows the new constraints can improve state-of-the-art baselines.""20\tThis paper discusses the recent attention given to knowledge graph (KG) representation learning for entity alignment. Embedding-based methods are considered more robust for highly-heterogeneous and cross-lingual entity alignment scenarios as they do not rely on machine translation or feature extraction. However, there is little understanding of how these methods actually work. Most existing methods rely on a small number of pre-aligned entities to connect the embedding spaces of two KGs, but the rationality of this foundation has not been investigated. The paper defines a typical paradigm from existing methods and analyzes how the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning.\tEntity alignment plays an important role in improving the quality of cross-lingual knowledge graphs. As one of the most important solutions, embedding-based methods aim at learning a semantic space where the unique entity cross knowledge graphs can have the closest distance. Most of research focus on entity-level granular, but discard the whole picture of embedding space of cross-lingual KGs. Besides the aligned entity pairs as the labelled data, this paper extended the labelled data with the conditional neural and basic axioms, which are actually sets of randomly selected entities or entities with the same relation type. Then the final objective is to align the cross-lingual knowledge graphs by both optimizing the distance of labelled entity pairs and neural axioms.""21\tThis paper discusses the recent attention given to knowledge graph (KG) representation learning for entity alignment, particularly the use of embedding-based methods which are considered to be more robust for highly-heterogeneous and cross-lingual scenarios. However, despite the significant improvement made by these methods, there is little understanding of how they actually work. The paper focuses on the foundation of these methods, which relies on pre-aligned entities serving as anchors to connect the embedding spaces of two KGs. The authors analyze how the representation discrepancy between potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning. Overall, the paper aims to provide a better understanding of the rationality behind the foundation of embedding-based entity alignment methods.\tIn the paper, the authors propose to minimize the discrepancy between pairs of (conditional) neural axioms to align the embedding spaces of different KGs. This method is justified by the authors' study of all kinds of OWL2 properties. The author also studied the influence of margin $\\lambda$ on less constrained/long-tail entities. The authors conducted experiments by adding the proposed model on top of the best models for entity alignment. The results are mixed, but the proposed model improves the SEA and RDGCN consistently. ""22\tThe paper discusses the challenges of implementing convolutional neural networks (CNNs) in IoT devices, particularly wearables and biology devices. The two main challenges are the strict form factor requirements, especially thickness, and the limited resources of IoT devices. The paper suggests that lensless imaging systems, which replace focal lenses with phase masks, could be a promising solution to the form factor challenge. The paper also proposes a new approach called \"CNN pruning,\" which reduces the hardware costs of implementing powerful CNNs in IoT devices by removing unnecessary connections and weights. The paper concludes that combining lensless imaging systems and CNN pruning could enable more extensive applications of CNN-powered IoT devices.\tThe paper proposed to adopt differentiable network architecture search (DARTS) for the co-design of the sensor (a lensless camera) and the deep model for visual recognition tasks, so as to maximize the accuracy and minimize the energy consumption. The key idea is to include the sensor configuration, in this case the phase mask of a lensless camera modeled as 2D convolutions, as additional parameters in architecture search.  The proposed method was evaluated on simulated data for a number of vision tasks (image classification, face recognition and head pose estimation), as well as using fabricated masks on a real world camera. The results demonstrated significantly increase recognition performance given the same energy level.  ""23\tThe paper discusses the challenges of implementing convolutional neural networks (CNNs) in Internet of Things (IoT) devices, particularly wearables and biology devices. The two main challenges are the strict form factor requirements, especially thickness, and the limited hardware resources of IoT devices. Lensless imaging systems have emerged as a promising solution to the first challenge, with PhlatCam being an example that replaces focal lenses with phase masks to encode incoming light. The paper proposes a new approach called \"Sparse CNNs\" that reduces the hardware costs of implementing powerful CNNs in IoT devices by using sparse matrices and compressed sensing techniques. The proposed approach is evaluated on various datasets and shows promising results in terms of accuracy and hardware efficiency.\tThis paper presents a method called SACoD to develop a more efficient CNN-powered Phlatcam. The proposed method optimizes both the PhlatCam sensor and the backend CNN model simultaneously.  That is, the coded mask in Phlatcam and neural network weights are regarded as learnable parameters. The coded mask (the optical layer) can be considered as a special convolution layer. As a result, it achieves energy saving, model compressing as well as good accuracy. Extensive experiments and ablation studies are presented to show the effectiveness of the method.""24\tThe paper discusses the challenges of implementing convolutional neural networks (CNNs) in IoT devices, particularly wearables and biology devices. The two main challenges are the strict form factor requirements, especially thickness, and the limited hardware resources of IoT devices. The paper suggests that lensless imaging systems, such as PhlatCam, which replaces focal lenses with phase masks, can be a promising solution for the form factor challenge. Additionally, the paper proposes a new approach called \"CNN pruning,\" which reduces the computational complexity of CNNs, making them more suitable for IoT devices with limited resources. The paper concludes that combining lensless imaging systems and CNN pruning can enable the development of more intelligent and efficient IoT devices.\tSACoD presents a novel attempt to integrate the computational capabilities of a lensless imaging system, PhlatCam, with the search for the optimal convolutional neural network design for a given task. SACoD provides a framework which enables joint optimization of sensor and CNN resulting in IoT devices that achieve higher task accuracy\u2019s with limited resource budgets of a typical IoT system. The authors present a new an optical layer design that enables above described features. Detailed experiments comparing SACoD sensor + CNN with other baseline models covering past papers, demonstrate the superiority of SACoD\u2019s accuracy/efficiency curve over that of separately optimizing CNN arch or sensor/CNN joint-optimizations that do not vary network architecture. Additionally, ablation studies and results from measurements from actual phase masks fabricated help breakdown the accuracy/efficiency benefits of SACoD while analyzing the noise limitations of mask fabrication process.""25\tThis paper discusses the coordination of behaviors in animal groups and the role of individual interactions in this process. Technological advancements have made it possible to track all individuals and their interactions in animal societies, resulting in unprecedented scale and complexity of data. Understanding this data has become a new and challenging problem, and a popular approach is to learn semantic embeddings. The paper highlights the importance of understanding individual interactions in animal groups and the potential of semantic embeddings to aid in this understanding.\tThe authors introduce a novel method for non-negative matrix factorization for timeseries and apply it to longitudinal honey bee interaction data.  The model leverages consistency of individuals over time by forcing the factors (or rather, the residuals of the factors with respect to a global trajectory) to be linear combinations of a small set of temporal basis functions.  These temporal basis functions are functions of the bee\u2019s age.  In other words, the factor embedding of each bee is a vector of linear combinations of 16 learned basis functions of time.  Since all bees use the same 16 temporal basis functions, given these basis functions the lifetime embedding of each bee is encapsulated by a small matrix of numbers, namely the coefficients for the temporal basis functions for each factor (and in practice only two factors were significant, so each bee\u2019s life is embedded in 32-dimensional space).  There are a number of regularizations on the temporal basis functions and the embedding coefficients.""26\tThis paper discusses the coordination of behaviors in animal groups and how individual roles are reflected in their interactions with group members. Technological advances have made it possible to track all individuals and their interactions in animal societies, resulting in unprecedented scale and complexity of data. Understanding this data has become a new and challenging problem, and a popular approach is to learn semantic embeddings. The paper provides references to previous studies and highlights the importance of understanding animal group behavior.\tThe authors present a matrix factorization model to jointly characterize the lifetime interactions of thousands of bees over generations. The problem is fascinating as both a technical and scientific question and the modeling framework appears novel. Although not directly addressed, the authors appear to be trying to solve a *tensor* factorization problem, not just the special case of a matrix (which is of course a 2-d tensor). It would have been interesting to see results comparing their method with a non-negative variant of, say, PARAFAC/CANDECOMP or generalizations thereof. It would have at least have been appropriate to explain why or why not existing tensor methods are not appropriate.""27\tThis paper discusses the coordination of behaviors in animal groups and how individual roles are reflected in interactions with group members. Technological advances have allowed for tracking of all individuals and their interactions in animal societies, resulting in large and complex datasets. Understanding these datasets has become a new and challenging problem, and the paper suggests using semantic embeddings as a popular approach to analyze high-dimensional data.\tThis paper proposes a NMF formulation ||A-FF^T||^2 where A and F are different types of information extracted from social datasets. In the honeybee example the authors highlight, A represents the spatial relationship between bees, and F encodes the age of the bees. The authors setup F to be decomposable into two types of embeddings, one which characterizes the group activity and the other which characterizes the individual activity. ""28\tThis paper discusses the challenge of reducing the acquisition time in magnetic resonance imaging (MRI) by subsampling measurements, which can lead to an underdetermined system of equations. To overcome this challenge, compressed sensing methods have been developed that utilize prior knowledge about the signal to reconstruct the image. Classical compressed sensing methods use convex regularization to enforce sparsity in an appropriate transformation of the image. However, more recently, deep learning techniques have been used to enforce more nuanced forms of regularization. The paper highlights the potential of deep learning methods for compressed sensing in MRI and discusses some of the challenges that need to be addressed in future research.\tReview: This paper proposes data augmentation methods for medical imaging(especially for accelerated MRI) based on the MR physics. The augmentation includes both pixel preserving augmentations/general affine augmentations on both real and imaginary values in the image domain. Then, the augmented images are transformed to k-space domain and the k-space data are down-sampled for the input data generation for the accelerated MRI task. They claim that how to schedule p(the probability of applying combinations of augmentation) over the training is important and the schedules from p=0 and increasing over epochs shows best results, experimentally.""29\tThis paper discusses the use of subsampling in magnetic resonance imaging (MRI) to reduce acquisition time and cost. However, subsampling results in fewer equations than unknowns, making the signal not uniquely identifiable from the measurements. To address this challenge, compressed sensing methods have been developed that utilize prior knowledge about the signal. Classical compressed sensing methods use convex regularization to enforce sparsity in an appropriate transformation of the image. More recently, deep learning techniques have been used to enforce more nuanced forms of sparsity. The paper discusses the advantages and limitations of both approaches and highlights the potential for combining them in future research.\tIn this paper, the authors design a data-augmentation pipeline for the domain of MRI reconstruction (specifically, by proposing sensible guidelines for augmenting k-space data when learning image reconstructions, to preserve the noise characteristics of the image data). They show that this pipeline works as you might expect data augmentation to work: it boosts results for small training sets and becomes increasingly less effective as the training set grows. However, while the problem domain is of interest, there are issues with the presented work.  ""30\tcompression in MRI imaging. This paper discusses the use of deep learning techniques to improve the quality of MRI images obtained through subsampling. The authors propose a novel deep learning architecture that combines a convolutional neural network with a recurrent neural network to reconstruct high-quality MRI images from undersampled measurements. The proposed architecture is shown to outperform existing state-of-the-art methods in terms of image quality and reconstruction time. The authors also discuss the potential impact of this work on improving the accessibility and affordability of MRI imaging for patients.\tThis paper presents a method to use data augmentation to improve accelerated MRI reconstruction when the amount of training data is limited. This is an important problem since MRI data is expensive to obtain. Traditional image augmentation methods can't be applied directly for this problem because MR images are complex valued. Further, the applied transformations need to preserve the noise distribution, without which model performance degrades significantly.""31\tThe paper proposes a new algorithm called deep repulsive clustering (DRC) for effective order learning of ordered data. The algorithm uses an order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. The object instances are then grouped into clusters based on their identity features using a repulsive term. The algorithm also estimates the rank of a test instance by comparing it with references within the same cluster. The proposed algorithm is tested on facial age estimation, aesthetic score regression, and historical color image classification, and it shows excellent performance in clustering ordered data and rank estimation.\tThe novelty of the network structure is marginal. The decomposition way of feature is very common in computer vision. Just utilizing the latent vector of the encoder with only the comparator loss to decompose the feature into two feature types is limited. The authors should show the visual differences between these two feature types. The expression of the article is very clear, but some basic theories need not be explained in detail (Such in Section 3.4)""32\tThe paper proposes a new algorithm called Deep Repulsive Clustering (DRC) for effective order learning of ordered data. The algorithm uses an Order-Identity Decomposition (ORID) network to divide object instance information into order-related and identity features. Object instances are then grouped into clusters based on their identity features using a repulsive term. The algorithm also estimates the rank of a test instance by comparing it with references within the same cluster. The proposed algorithm is tested on facial age estimation, aesthetic score regression, and historical color image classification, and it shows excellent performance in clustering ordered data and rank estimation.\tThis paper considers the problem of order learning, which learns an ordinal classification function. This paper proposes to learn separarted order-relavent and order-irrelavent latent representations to improve the performance of existing methods, which is a very interesting and promising idea. However, the approach lacks novelty and convincing theoretical guarantees, as well as not showing convincing performance even through the insufficient empirical evaluation.""33\tThe paper proposes a new algorithm called deep repulsive clustering (DRC) for effective order learning of ordered data. The algorithm uses an order-identity decomposition (ORID) network to divide object instances into order-related and identity features. The algorithm then groups object instances into clusters based on their identity features using a repulsive term. Additionally, the algorithm estimates the rank of a test instance by comparing it with references within the same cluster. The proposed algorithm is tested on facial age estimation, aesthetic score regression, and historical color image classification and shows excellent performance in clustering ordered data and rank estimation.\t- It is well presented. The idea of splitting the encoding feature space into task related features and non-task related features is probably not new. But the use of it in estimating rank might be new and intuitively it makes sense to use it. They also propose an extension to the clustering algorithm using a repulsive term and propose MAP estimation algorithm to assign a rank based on the output probabilities of the comparator when the max possible rank is known.""34\tThis paper discusses the challenge of achieving sample efficiency in deep reinforcement learning (RL) algorithms, particularly in sparse reward environments where the agent may take a long time to encounter a reward signal. The paper highlights the use of intrinsic rewards as a popular technique to encourage exploration in hard-exploration environments. The authors suggest that further research is needed to improve the efficiency of exploration in RL algorithms, particularly in sparse reward environments.\tThis paper presents RAPID, an exploration algorithm for procedurally generated environments. The paper introduces an exploration scores composed of a local and global score. The local score is computed per-episode, it is the fraction of distinct states visited during an episode, the global score keeps track of the exploratory effort of the agent over the whole training procedure.""35\tThe paper discusses the challenge of achieving sample efficiency in deep reinforcement learning (RL) algorithms, particularly in sparse reward environments where the agent may take a long time to encounter a reward signal. The authors highlight the importance of efficient exploration in such environments and discuss various exploration methods that have been developed to address this challenge. One popular technique is to use intrinsic rewards to encourage exploration. The paper provides a brief overview of the literature on exploration in RL and highlights the need for further research in this area.\tThis paper presents an exploration method for procedurally-generated environments, RAPID, which imitates the past episodes that have a good exploration behavior. First, authors introduce exploration scores, local score for per-episode view of the exploration behavior, and global score for long-term and historical view of exploration. The authors use the weighted sum of these two exploration scores and extrinsic reward as a final episodic exploration score. They rank the state-action pairs based on episodic exploration score and train the agent to imitate behaviors with high score. In experiments, they show the results by comparing state-of-the-art algorithms in several procedurally-generated environments.""36\tThe paper discusses the challenge of achieving sample efficiency in deep reinforcement learning (RL) algorithms, particularly in sparse reward environments where the agent may take a long time to encounter a reward signal. To address this challenge, various exploration methods have been investigated, including the use of intrinsic rewards to encourage exploration. The paper highlights the effectiveness of these methods in hard-exploration environments and discusses their potential for improving sample efficiency in RL algorithms.\tThis paper tackles the problem of improving exploration in deep RL for procedurally-generated environments, where state-of-the-art exploration techniques typically fail. In the proposed approach, called RAPID, each agent-generated episode is evaluated with respect to its local exploration score (for the given episode), global exploration score (across all previous episodes), and extrinsic reward obtained. Episodes with high scores are stored in a replay buffer, and a policy is trained via behavioral cloning on batches of state-action pairs from this buffer. This policy is also used to produce the agent-generated episodes.""37\tThe paper discusses the challenge of improving the generalization capability of machine learning models to unseen problems, specifically in the context of zero-shot learning (ZSL) for image classification. ZSL aims to classify new images from unseen classes with zero training images available, and recent works have introduced learning to map between the semantic space and visual space to enable this. The paper reviews various approaches to ZSL and discusses their strengths and weaknesses, as well as potential future directions for research in this area.\tThis paper focuses on improving zero-shot classification by reducing the bias of the classifier towards seen classes. The bias occurs since the embedding is trained with visual examples from the seen classes, while using only the attribute information from unseen classes for testing. Authors propose an isometric propagation network that build a graph in both visual and semantic space, performs some steps of propagation, and then uses the updated prototypes for training a classifier. They use attention to construct the graph and also use attention to regularize the graph edges between the two spaces to be isometric. Authors also propose to use an episodic training method to improve learning. ""38\tThis paper discusses the challenge of improving the generalization capability of machine learning models to unseen problems, particularly in the context of zero-shot learning (ZSL) for image classification. ZSL aims to classify new images from unseen classes with zero training images available, which requires learning to map between the semantic space and visual space. The paper reviews recent works that have introduced methods for learning this mapping, including approaches based on generative models and adversarial training. The authors also discuss the limitations of current ZSL methods and suggest directions for future research.\tIn this paper Zero Shot Classification is studied using prototypes. Each class is represented with a visual and semantic prototype, and at test time compared to a visual example + prototype for a(n unseen) test class. The most similar test class is chosen. In this work a novel method is proposed to construct the prototypes, which are trained in an episode learning setting. On various benchmarks the proposed method performs better than existing method for the generalized zero-shot classification task (seen + unseen) test classes.""39\tThis paper discusses the challenge of improving the generalization capability of machine learning models to unseen problems, particularly in the context of zero-shot learning (ZSL) for image classification. ZSL aims to classify new images from unseen classes with zero training images available, which requires learning to map between the semantic space and visual space. The paper reviews recent works that have introduced methods for learning this mapping, including approaches based on generative models, attribute-based methods, and graph-based methods. The paper also discusses evaluation metrics for ZSL and identifies open research questions in this area.\tThe authors propose a novel computational pipeline to tackle a well-known problem in zero-shot learning: although multiple visual instances are available for the classes and categories to be recognized, one and only one semantic embedding is available to describe the classes/categories while using side information like attributes or relevant textual information. To cope with that problem, authors learn visual and semantic prototypes which are then adopted to perform gradient descent over a graph in which the topological relationship among similar/dissimilar classes are preserved. In the experimental validation, the proposed method shows its superiority among a number of prior methods in zero-shot learning, including discriminative and generative methods. ""40\tThe paper discusses the challenges of developing a single multi-task model that performs well across multiple targeted tasks in natural language understanding. While this approach offers significant savings in parameter costs and eliminates the need for maintaining multiple models in production, achieving state-of-the-art performance on natural language understanding benchmarks still relies on fine-tuning a new model for every single task. The paper argues that the single-task fine-tuning paradigm is the dominant approach, as training multiple tasks using a single model is extremely challenging and often requires an extensive ensemble of models and task-specific fine-tuning tricks.\tThis paper presents a HyperGrid Transformer approach to fine-tuning, where one takes a pre-trained transformer model and then modifies it by introducing hypernetworks that modify the 2nd FFN in each transformer block by generating additional weights conditioned on input. These hyper-networks are trained on all tasks in GLUE/SuperGLUE datasets simultaneously and are task aware through prefixing of a task specific token to input. This allows one to fine-tune only a small number of parameters and end up with a model that performs quite well on all tasks at the same time, not much worse than fine-tuning the entire transformer model on all of these tasks.""41\tThe paper discusses the challenges of learning a single multi-task model that performs well across multiple targeted tasks in natural language understanding. While this approach can save parameter costs and eliminate the need for maintaining multiple models in production, achieving state-of-the-art performance still relies on fine-tuning a new model for every single task, which is infeasible in many situations. The paper suggests that the single-task fine-tuning paradigm is the dominant approach, but proposes a new approach called \"adapter modules\" that can be added to a pre-trained language model to adapt it to new tasks without requiring extensive fine-tuning or retraining. The paper presents experimental results that demonstrate the effectiveness of adapter modules on a range of natural language understanding tasks.\tThe authors propose HyperGrid Transformers with a decomposable hypernet-work that learns grid-wise projections to specialize regions in weight matrices for different tasks. Usually, people would use different models to solve different tasks respectively. In this paper, the authors focus on using a single model to solve all tasks and it will save a lot of model parameters for natural language understanding. And the authors have done comprehensive experiments on GLUE and SuperGLUE, and prove that the proposed single model can achieve much better performance than baseline and competitive performance with multiple task-specific models.""42\tThe paper discusses the challenges of developing a single multi-task model that performs well across multiple targeted tasks in natural language understanding. While this approach offers significant savings in parameter costs and eliminates the need for maintaining multiple models in production, achieving state-of-the-art performance on natural language understanding benchmarks still relies on fine-tuning a new model for every single task. The paper argues that the single-task fine-tuning paradigm is the dominant approach, as training multiple tasks using a single model is extremely challenging and often requires an extensive ensemble of models and task-specific fine-tuning tricks.\tThis manuscript presents a HyperGrid Transformer, which is engaged in learning a single model to account for multi-tasks in NLP. The core idea of HyperGrid Transformer is to learn task-conditional dynamic weights in a grid-wise manner in the feed-forward layers, where the weights are factorized in local and global components. This idea is simple, materializing the goal of reducing the parameter cost for the used multi-task network. However, the conducted experiments look nice, showing promising performance on GLUE/SuperGLUE. Therefore, from my point of view, this work is worthy of a publication at ICLR. ""43\tThis paper discusses the challenges of autonomous driving in various environmental conditions and how these challenges can affect the decision-making of the vehicle. The authors propose a solution to this issue by analyzing the sensitivity of the learning algorithm to varying image quality inputs and developing an algorithm to improve the performance of \"learning to steer.\" The results show that their approach can enhance learning outcomes by up to 48%, and a comparative study confirms its effectiveness compared to other related techniques.\tThis paper proposed a novel adaptive data augmentation algorithm that produces random perturbations on the training dataset to train an imitation learning-based self-driving network. It starts with a sensitivity analysis of network performance under different types and levels of perturbations. And a novel automated perturbed training dataset selection mechanism is then proposed to improve the performance. Validation has been conducted over simulated data with both seen and unseen perturbation types. ""44\tThis paper addresses the challenges of autonomous driving under various environmental conditions and internal factors associated with sensors. The authors propose an algorithm to improve the performance of \"learning to steer\" by analyzing the sensitivity of the learning algorithm with respect to varying quality in the image input. The results show that their approach is able to enhance the learning outcomes up to 48%. A comparative study confirms the effectiveness of their algorithm as a way to improve the safety and adoption of autonomous driving.\tThis work proposes a new method to improve the generalization of ML models for the task of vehicle steering using a hybrid of data augmentation and adversarial examples. In a nutshell, the proposed method attempts to increase the accuracy of the model by dynamically adding a selection of candidate datasets during training. Each of these \u201ccandidates\u201d is created offline applying a transform (e.g. blur, distortion, and changes in color representation) to the original (base) dataset. During training, the method chooses among the K transformed-datasets those who minimize the mean validation accuracy and based on this selection the steering model is retrained. The approach is evaluated on a driving dataset.""45\tThis paper discusses the challenges of autonomous driving in various environmental conditions and the impact of these conditions on the vehicle's decision-making process. The authors propose an algorithm to improve the performance of the \"learning to steer\" task by analyzing the sensitivity of the learning algorithm to varying image quality inputs. The results show that their approach can enhance learning outcomes by up to 48%, and a comparative study confirms its effectiveness compared to other techniques such as data augmentation and adversarial training. Overall, the paper highlights the importance of addressing the challenges of environmental factors in autonomous driving to ensure safety and wide adoption.\tThis paper presents an algorithm to improve the model generalization of the task of \"learning to steer\". First, the sensitivity of a baseline learning algorithm to degraded images in varying qualities caused by different factors is carried out. Some empirical insights are gained. Then, a new training algorithm is proposed to solve a min-max optimization problem, where the most difficult datasets are chosen and used for training at each iteration. Experiments are conducted to validate the effectiveness of the proposed method. ""46\tThe paper discusses the limitations of traditional approaches to constrained optimization and the need for function approximators that can satisfy hard constraints at test time. The authors propose a new approach called Deep Constraint Completion and Correction (DC3) that uses neural networks as function approximators to quickly and accurately solve non-convex optimization problems while ensuring strict feasibility criteria are met. The paper highlights the potential applications of DC3 in areas such as power systems, weather and climate models, and materials science.\t+ The paper proposes a general framework to deal with constraints in optimization problems using neural networks. In my opinion this is an important problem since there exists no standard method in many existing deep neural network frameworks to deal with constraints, which are also inapplicable even if the constraints are only slightly nontrivial. The paper proposes to deal with equality and inequality constraints differently which may be often easier in large scale settings.""47\tThis paper introduces Deep Constraint Completion and Correction (DC3), a method for using neural networks as function approximators in constrained optimization problems. While neural networks are fast and expressive, they have struggled to perform well in domains where it is necessary to satisfy hard constraints at test time. DC3 addresses this issue by incorporating constraints into the neural network training process and using a correction mechanism to ensure feasibility at test time. The authors demonstrate the effectiveness of DC3 on several optimization problems, including power systems and materials science.\tThere has been an increase of works using deep neural networks to heuristically predict solutions to constrained optimization problems. However, these methods cannot generalize to arbitrary constraints.  In this paper, the authors propose a method to build neural networks that output vectors that satisfy hard equality and inequality constraints. They do this by first having the network predict the underdetermined part of the system defined by the equalities, then doing a series of gradient steps to project the solution onto the space delineated by the inequalities. They evaluate on synthetic quadratic programs and problems derived from a AC power flow application.""48\tThe paper introduces Deep Constraint Completion and Correction (DC3), a method for using neural networks as function approximators in constrained optimization problems. While neural networks are fast and expressive, they have struggled to perform well in domains where hard constraints must be satisfied at test time. DC3 addresses this issue by incorporating constraints into the neural network training process, allowing it to learn to satisfy constraints while still being fast and efficient. The authors demonstrate the effectiveness of DC3 on several optimization problems, including power systems and materials science.\tThis paper proposes a method to strictly enforce hard constraints during a neural network, without compromising differentiability. The method has two stages 1) From a smaller set of predicted variables, compute the remaining ones so that equality constraints are satisfied; 2) Take a few gradient steps (w.r.t soft constraint) in case inequality constraints are violated. They perform experiments on synthetic and also somewhat applied instances of quadratic programs. The results look very promising.""49\tThe paper discusses the challenges of deep neural networks, including their high storage, memory, computing, and energy consumption requirements. Neural network pruning is presented as a promising solution to these challenges, and the central problem of pruning is identified as the weight importance scoring problem. The paper categorizes approaches to the scoring problem into two groups: importance-based and regularization-based. The former focuses on directly measuring the importance of weights, while the latter uses regularization techniques to encourage sparsity in the network. The paper concludes by discussing the limitations of current pruning methods and suggesting directions for future research.\tThe authors propose regularization-based pruning methods with the penalty factors uniformly increased over the training session. The first algorithm (GReg-1) sorts the filters by L1-norm and only applies the increasing regularization to the \u201cunimportant\u201d filters; the second one (GReg-2) applies the increasing regularization to all the filters. The experiments are very extensive and convincing to support the claimed contributions.""50\tThis paper discusses the issue of rising storage, memory footprint, computing resources, and energy consumption associated with the effectiveness of deep neural networks. Neural network pruning is presented as a promising solution to this problem. The central problem of neural network pruning is identified as the weight importance scoring problem, which involves choosing weights to discard. The paper categorizes approaches to the scoring problem into two groups: importance-based and regularization-based. The former focuses on direct importance measures, while the latter uses regularization techniques to encourage sparsity. The paper concludes by discussing the challenges and future directions of neural network pruning research.\tThe paper proposes a new pruning scenario using regularization to better prune the network. The scenario has two-component, the first one proposes a new pruning schedule that does not directly remove the neurons that need to prune from the network. It removes the neurons by adding an L2 regularization and makes the neurons that need to remove gradually decrease to zero. The second one gives the importance score to the neurons. It uses the L2 regularization and studies how the coefficient \\lambda of the regularization term can influence the weight change to derive the neuron's importance in the neuron network. By perturbing the penalty term to the converged network, the algorithm can get the Hessian information to score the neurons but uses less time than calculating the Hessian. The paper also shows many empirical results on various benchmarks to show their advantages when using the new schedule and scoring criterion during the pruning process. The result shows that their method can get better at a fast speed.""51\tThe paper discusses the issue of rising storage, memory footprint, computing resources, and energy consumption in deep neural networks. Neural network pruning is proposed as a solution to alleviate this problem. The central problem of neural network pruning is how to choose weights to discard, i.e., the weight importance scoring problem. The approaches to the scoring problem generally fall into two groups: importance-based and regularization-based. The paper provides a comprehensive review of recent advances in neural network pruning, including different pruning methods, weight importance scoring methods, and their combinations. The paper also discusses the challenges and future directions in neural network pruning research.\tThis paper explores how the basic L2 regularization can be exploited in a growing fashion for better deep network pruning. The authors proposed two algorithms in this work: (1) The first (called GReg-1) is a variant of the L1-norm based filter pruning method [Ref1]. The important/unimportant filters are decided by their L1-norms. Later the unimportant ones are forced to zero through the proposed rising penalty scheme. (2) The second algortihm (called Greg-2) imposes the rising L2 regularization on all the filters. It is theoretically shown in the paper that this makes the parameters to separate to different degrees according to their local curvatures (ie, Hessian values). The method takes advantage of this by driving the weights into two groups with stark magnitude difference and then prunes by the simple L1-norm criterion.  The two methods are demonstrated effective on CIFAR10/100 and ImageNet benchmarks in the comparison with many state-of-the-art methods. ""52\tThe paper discusses the recent interest in model-based reinforcement learning (MBRL) and its impressive gains over model-free methods in data efficiency, zero- and few-shot learning, and strategic thinking. MBRL combines planning and learning using a learned or given model of the world to construct imagined future trajectories or plans. The paper suggests that models will play a key role in generally intelligent artificial agents, and the recent successes of MBRL methods lend evidence to this hypothesis. However, there is a huge variance in the algorithmic choices made to support such advances.\tThe paper investigates how and why planning might be beneficial in model-based reinforcement learning settings. To that end, the authors ask three questions on planning in MBRL: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? In order to answer these questions, the authors investigate the performance of MuZero in a variety of learning challenges while systematically ablating the algorithm to find how each part of the algorithm effects the overall performance.""53\tThe paper discusses the recent interest in model-based reinforcement learning (MBRL) and its impressive gains over model-free methods in data efficiency, zero- and few-shot learning, and strategic thinking. MBRL combines planning and learning using a learned or given model of the world to construct imagined future trajectories or plans. The paper suggests that models will play a key role in generally intelligent artificial agents, with arguments often appealing to model-based aspects of human cognition as proof of their importance. However, there is a huge variance in the algorithmic choices made to support such advances in MBRL methods.\tThis paper tries to disentangle the role of planning in model-based reinforcement learning with a number of different ablations and modifications to MuZero. Specifically, the authors analyze the overall contribution of planning by omitting planning from which it is originally used in MuZero, and investigate different planner settings that can drive performance. In addition, they check the generalization advantage of MBRL. Overall, the paper is well-written, and experiments are conducted appropriately. The results provide some insights that other researchers in the MBRL community can leverage for their future work. My major concern is the lack of direct ablation study that can clearly show the advantage of planning in providing a good learning signal. See the detailed comments below.""54\tThe paper discusses the recent interest in model-based reinforcement learning (MBRL) and its advantages over model-free methods in terms of data efficiency, zero- and few-shot learning, and strategic thinking. MBRL combines planning and learning using a learned or given model of the world to construct imagined future trajectories or plans. The paper suggests that models will play a key role in generally intelligent artificial agents, with arguments often appealing to model-based aspects of human cognition as proof of their importance. However, there is a huge variance in the algorithmic choices made to support such advances in MBRL methods.\tThis paper analyzes the role of planning in the model-based reinforcement learning agent, based on evaluating MuZero on eight tasks (i.e. Ms.Pacman, Hero, Minipacman, Sokoban, 9x9Go, Acrobot, Cheetah, and Humanoid), which have discrete action spaces. The conducted experiments show three major implications: (1) Of the three parts in which search is used (i.e. search at evaluation time, search at training time for exploration, and using search result as a policy target), the role of serving as a policy improvement target was most substantial. (2) Deep tree search did not make a significant contribution to performance, and a simple Monte-Carlo rollout could be performant enough for MBRL. Also, a too small or too large search budget can be harmful to the performance of the MBRL agent. (3) Search at evaluation time was helpful for zero-shot generalization especially when the model is accurate.""55\tThe paper discusses the importance of planning in reinforcement learning algorithms and introduces the concept of implicit planning, also known as model-free planning. Instead of explicitly modeling the environment and using planning algorithms, implicit planning involves incorporating inductive biases in the policy function to enable planning to emerge while training the policy in a model-free manner. The paper highlights value iteration networks (VINs) as a notable example of this approach, which uses a CNN-based VI module to perform planning on a grid-world. The paper also mentions several other recent approaches that explore implicit planning.\tThe authors propose a generalization of Value Iteration Networks to unknown, potentially continuous state spaces. They describe a framework for leveraging a learned graph embedding model (TransE) in combination with a deep RL model and an execution model based on graphical message passing to perform a VI-like operation. The authors show improved performance compared to baselines on a grid-world task with a known MDP, as well as several simple continuous control environments and the Atari game Freeway.""56\tThe paper discusses the importance of planning in reinforcement learning algorithms and introduces the concept of implicit planning or model-free planning. Instead of explicitly modeling the environment and using planning algorithms, these approaches propose inductive biases in the policy function to enable planning to emerge while training the policy in a model-free manner. The paper highlights the value iteration networks (VINs) as an example of this approach, which uses a CNN-based VI module to understand the value iteration algorithm on a grid-world as a convolution of state values and transition probabilities followed by max-pooling. The paper also mentions several recent approaches that explore implicit planning.\tThe paper tackles an open problem of the value-iteration-network-paradigm. The proposed method (XLVIN) has a conceptual edge over traditional value iteration networks in that it can be applied to continuous problems and problems where the state space is either too big or not fully known in advance. The experiments mostly succeed in making the case that XLVINs:""57\tThe paper discusses the importance of planning in reinforcement learning algorithms and introduces the concept of implicit planning, also known as model-free planning. Instead of explicitly modeling the environment and using planning algorithms, implicit planning approaches propose inductive biases in the policy function to enable planning to emerge while training the policy in a model-free manner. The paper highlights the value iteration networks (VINs) as a notable example of this approach, which uses a CNN-based VI module to perform value iteration on a grid-world. The paper also mentions several recent approaches that explore implicit planning.\tThis paper proposed a novel policy prediction model that combines self-supervised contrastive learning, graph representation learning and neural algorithm execution to generalize the Value Iteration Networks to MDPs. The method described in the paper is a combination of existing works in the literature but seems to work well in practice. The experiments evaluate multiple aspects of the proposed model (E.g. number of executor layers, etc.) and show significant performance improvement over the existing approaches.""58\tThe paper discusses the challenge of understanding the inductive bias of neural networks and how it affects their generalization performance. While the training objective of overparameterized neural networks is non-convex and contains multiple global minima with different generalization properties, neural networks trained with gradient-based methods show good test performance across numerous tasks. The paper highlights the major open problem in machine learning of understanding this inductive bias and how it depends on the algorithm, architecture, and data. The paper also discusses recent efforts to tackle this challenge, including the Neural Tangent Kernel (NTK) approximation of neural networks, which reduces to a convex optimization problem. However, the paper notes that the NTK approximation has limitations and proposes a new approach called the Neural Path Kernel (NPK) that overcomes these limitations and provides a better understanding of the inductive bias of neural networks.\tThis paper investigates the problem of learning monotone read-once DNF formulas using convex neural networks. Specifically, the authors explore the distribution-specific PAC setting, where training samples are drawn independently according to the uniform distributions and are labeled according to a target monotone read-once DNF. The main contribution of this study is essentially empirical: convex neural nets, trained with GD for minimizing the cumulative hinge loss, converge to global minima for which neural units coincide with the monomials of the target DNF. This remarkable stability is corroborated by theoretical insights about global minima.""59\tThe paper discusses the challenge of understanding the inductive bias of neural networks and how it affects their generalization performance. While the training objective of overparameterized neural networks is non-convex and contains multiple global minima, neural networks trained with gradient-based methods show good test performance, suggesting an inductive bias towards desirable solutions. The paper explores recent efforts to tackle this challenge, including the use of the Neural Tangent Kernel (NTK) approximation of neural networks, which reduces the problem to a convex optimization problem. However, the paper notes that the NTK approximation has limitations and proposes a new approach called the Neural Implicit Function (NIF) that can capture the inductive bias of neural networks more accurately. The paper concludes by discussing the potential applications of the NIF approach in various machine learning tasks.\tIn this paper the aim in to understand the inductive bias of neural networks learning DNFs. The focus is in convex neural networks and gradient descent. It is shown that under a symmetric initialization, the global minimum that gradient descent converges to is similar to a DNF-recovery solution. Further, experimental evaluation demonstrates that gradient descent can recover read-once DNFs from data. ""60\tThe paper discusses the challenge of understanding the inductive bias of neural networks and how it affects their generalization performance. While neural networks trained with gradient-based methods show good test performance, the non-convex training objective of overparameterized neural networks contains multiple global minima with different generalization properties. The paper explores the Neural Tangent Kernel (NTK) approximation of neural networks, which reduces the problem to a convex optimization problem. However, the NTK approximation has limitations, and the paper proposes a new approach called the Neural Path Kernel (NPK) that captures the inductive bias of neural networks and can be used to analyze their generalization performance. The paper concludes by discussing the potential applications of the NPK approach in various machine learning tasks.\tThe paper considers learning Boolean functions represented by read-once DNFs by using neural networks. The neural network architecture consists of a hidden layer with 2^D components, which is rich enough to express any Boolean functions. Given a whole 2^D instances of some read-once DNF, the authors showed that (1) weights corresponds to the true DNF is the global minimum of the loss minimization problem with the network, (2) they empirically observe that gradient descent with a rounding heuristics finds the true DNF expression, and(3) the solution of a 2-norm minimization recovers the true DNF.""61\tearn a policy that achieves the final goal when the reward signal is sparse and delayed. (C2) The second challenge is how to learn a policy that can generalize to new environments or tasks with similar structures. To address these challenges, researchers have proposed various approaches, including curriculum learning, transfer learning, and meta-learning. However, there is still much work to be done to bridge the gap between human and RL agent learning.\tThis work presents a strategy for improving exploration and efficiency of RL by leveraging the graph structure of an episodic experience buffer. This strategy combines goal-oriented RL with structured exploration. The authors compare their proposed technique to two popular benchmarks for goal-reaching tasks. In addition, the authors provide some theoretical justification for their algorithmic choices.""62\tearn to solve tasks with complex structure and delayed and sparse feedback, such as a complicated navigation task. One approach is to decompose the task into easier sub-tasks and identify intermediate goals to reach the final goal. However, there is still a gap between how humans and reinforcement learning (RL) agents learn. RL agents often have limited access to rewards, leading to two major challenges: (C1) how to effectively learn from sparse and delayed rewards, and (C2) how to generalize learning to new environments. This paper aims to address these challenges by proposing a new RL algorithm called \"Goal-Conditioned Value Iteration\" (GC-VI) that learns to predict the value of a goal-conditioned policy. The authors demonstrate the effectiveness of GC-VI on several benchmark tasks and show that it outperforms other RL algorithms in terms of sample efficiency and generalization to new environments.\tThis paper proposes a new framework, GSRL, to handle the sparse reward challenge and better leverage past experiences. Specifically, it formulates trajectories as a dynamic graph, and generates hindsight-like goals based on sub-group division and attention mechanism. The authors provide theoretical analysis to show the efficiency and converge property of their method. The experimental result shows the proposed method significantly outperforms the baselines. ""63\tearn to solve tasks with complex structure and delayed and sparse feedback, such as a complicated navigation task. One approach is to decompose the task into easier ones, identify intermediate goals, and choose the best route among candidate routes. However, there is still a gap between how humans and reinforcement learning (RL) agents learn. RL agents often have limited access to rewards, leading to two major challenges: (C1) how to effectively learn from sparse and delayed rewards, and (C2) how to generalize learning to new environments. This paper aims to address these challenges by proposing a new RL algorithm called Goal-Conditioned Value Iteration (GC-VI) that learns to predict the value of a goal-conditioned policy. The authors demonstrate the effectiveness of GC-VI on several benchmark tasks and show that it outperforms other state-of-the-art RL algorithms.\tThis paper introduces Graph Structured Reinforcement Learning (GSRL) framework, able to balance exploration and exploitation in RL. Actually, GSRL builds a dynamic graph based on historical trajectories. Then in order to learn from sparse or delayed rewards and  be able to reach a distant goal, it decomposes the main task into a sequence of easier and shorter tasks. An attention strategy has also been proposed that is able to select an appropriate goal for each one of the easiest tasks. Experiments have been conducted on various robotics manipulation tasks showing that GSRL performs better compared to HER and MAP algorithms. ""64\tThe paper discusses the increasing interest in procedural content generation (PCG) environments in reinforcement learning (RL) research. PCG environments generate unique environment instances or levels algorithmically, making them a promising target for evaluating systematic generalization in RL. The paper highlights several PCG environments, including MiniGrid, the Obstacle Tower Challenge, the Procgen Benchmark, and the NetHack Learning Environment. Unlike singleton environments, PCG environments cannot be exploited by memorization and deterministic reset strategies. The paper suggests that PCG environments offer a new avenue for evaluating the generalization capabilities of RL agents.\tThis paper concerns about the use of experience replay in a way that past experience is sampled based on (implicit) levels so as for the agent to better adapt to the current task at hand. The authors defined a replay distribution (where experience is sampled) based on two scores relevant to learning potential and staleness. Due to its formulation, the change of replay distribution can be used as an outer-layer of a learning algorithm without any modification of the underlying learning mode. The authors conducted experiments over a set of benchmark data sets relevant to level-ness and found statistically significant improvements over more than half of the tasks.""65\tThe paper discusses the increasing interest in procedural content generation (PCG) environments in reinforcement learning (RL) research. Unlike singleton environments, PCG environments create novel environment instances or levels algorithmically, making them a promising target for evaluating systematic generalization in RL. The paper mentions several PCG environments such as MiniGrid, the Obstacle Tower Challenge, the Procgen Benchmark, and the NetHack Learning Environment. The unique configurations of underlying factors of variation in each level of PCG environments create a challenge for RL agents to generalize their learned policies. The paper highlights the importance of evaluating RL agents' generalization abilities in PCG environments and proposes a framework for evaluating systematic generalization in RL.\tThe present work considers the problem of learning in procedurally generated environments. This is a class of simulation environments in which each individual environment is created algorithmically where certain environmental factors are varied in each instance (referred to as levels in this work). Learning algorithms in this setting typically use a fixed set of training and evaluation environments. The present work proposes to sample the training environments such that the learning progress of the agent is optimized. This is achieved by proposing an algorithm for level prioritization during training. The performance of the approach is demonstrated on the Procgen Benchmark and two MiniGrid benchmarks and the authors argue that their approach induces an implicit curriculum in sparse reward settings.""66\tThe paper discusses the increasing interest in using environments generated through procedural content generation (PCG) in reinforcement learning (RL) research. Unlike singleton environments, PCG environments create novel instances or levels algorithmically, making them a promising target for evaluating systematic generalization in RL. The paper highlights several PCG environments, including MiniGrid, the Obstacle Tower Challenge, the Procgen Benchmark, and the NetHack Learning Environment. The paper emphasizes that PCG environments offer unique configurations of underlying factors of variation, such as layout, positions of game entities, asset appearances, and different rules governing environment transitions, which can help evaluate the generalization capabilities of RL algorithms.\tThis paper allows agents to set the initial conditions (level) for procedurally generated episodes during exploration to past observed values, and proposes to have agents form an intrinsic curriculum by resampling past levels based on a heuristic measure of expected learning progress. The authors test several heuristic measures and find that the average absolute magnitude of the generalized advantage estimate works well. The authors hypothesize that this intrinsic curriculum will improve optimization/learning relative to an agent that always samples initial conditions from the environment distribution. The authors verify that their prioritization strategy usually improves performance in several Progen Benchmark and MiniGrid environments, usually by a small but statistically significant amount, but sometimes by a large amount. ""67\tThe paper discusses the challenges of leveraging multitask learning and pretraining in machine learning to benefit downstream tasks with small training sets. While these techniques have transformed machine learning, selecting helpful auxiliary tasks, an appropriate parameter sharing architecture, and filtering out detrimental auxiliary data is still an art left to the practitioner. Without careful choices, pretraining may hurt end-task performance or have limited impact. The paper examines prior work that has proposed solutions to these problems, including choosing auxiliary tasks based on their impact on the primary task.\tThe work studies the auxiliary task selection in deep learning to resolve the burden of selecting relevant tasks for pre-training or the multitask learning. By decomposing the auxiliary updates, one can reweight separately the beneficial and harmful directions so that the net contribution to the update of the primary task is always positive. The efficient implementation is experimented in text classification, image classification, and medical imaging transfer tasks.""68\tThe paper discusses the challenges of leveraging multitask learning and pretraining in machine learning to improve performance on downstream tasks with small training sets. While these techniques have shown promise, selecting helpful auxiliary tasks, determining appropriate parameter sharing architecture, and filtering out detrimental auxiliary data can be difficult. Without careful choices, pretraining may even hurt end-task performance. The paper reviews prior work on these problems and proposes solutions to choose auxiliary tasks based on their impact on the primary task.\tLeveraging the power of the data-rich related tasks have been studied (e.g., pre-training and multitask learning). This paper points out that careful utilization of auxiliary task is required to gain enhanced performance in primary tasks. In order to prevent harming the performance of primary tasks, they suggest the method to decompose auxiliary updates into three directions which have positive, negative and neutral impact on the primary task.""69\tThe paper discusses the challenges of leveraging multitask learning and pretraining in machine learning to improve performance on downstream tasks with small training sets. While these techniques have shown promise, selecting appropriate auxiliary tasks, parameter sharing architecture, and filtering out detrimental auxiliary data can be difficult. Without careful choices, pretraining may even hurt end-task performance. The paper reviews prior work on these challenges and proposes solutions to improve the effectiveness of multitask learning and pretraining.\tThe authors present a general formulation of different settings in multitask learning (including pretraining regimes), in a setting where the goal is to get best performance for a pre-specified primary task and additional auxiliary tasks. The main idea is to divide the gradients on the auxiliary task into 2 subspaces: a subspace where the gradients influence performance of the primary task and a subspace where they only influence the auxiliary task without changing the loss on the primary task. Within the subspace that does have influence on the primary task, it is easy to compute directions that have a positive or negative effect on the primary task, which allows to create different learning schemes given the gradients that point toward: i) auxiliary influence only, ii) positive influence on auxiliary tass, iii) negative influence on primary task. Experimental results show improvements over previously identified meta learning methods on 2 natural language datasets and 3 image datasets.""70\tThis paper discusses the development of an embodied learning system that can perform fast-mapping, which is the ability to learn the names of unfamiliar objects after a single exposure. The system is an embodied agent situated in a 3D game environment that observes the world through active perception of raw pixels and learns to respond to linguistic stimuli by executing sequences of motor actions. The goal of this research is to enable language models to exhibit one- or few-shot learning, which is of growing interest in machine learning applications, and to contribute to the study of fast-mapping in child language learning.\tThe authors use a 3D world to explore grounded language learning, in which an agent uses RL to combine novel word-learning with stably acquired meanings to successfully identify and manipulate objects.  They show that a novel, psychologically-inspired memory mechanism is more memory-efficient than Transformers (both of which outperform plain LSTMs) and that it exhibits surprisingly robust generalization to novel action-object pairs.  The results should be of interest to many working in grounded language / multimodal representation learning, and the experiments are thorough and well-motivated. ""71\tThis paper discusses the development of an embodied agent situated in a 3D game environment that can perform fast-mapping, the ability to bind a new word to an unfamiliar object after a single exposure. The agent observes the world via active perception of raw pixels and learns to respond to linguistic stimuli by executing sequences of motor actions. The goal is to enable an embodied learning system to perform fast-mapping, which is of interest to developmental psychologists studying child language learning. The paper highlights the growing interest in language models that exhibit one- or few-shot learning and their potential applications in machine learning.\tThis paper presents experiments for acquiring words via fast-mapping in an embodied environment. The technical contribution is interesting and solid, but the experiments fail to address some important questions that are yet scoped by the claims of the paper (namely, that learning is being done -both- fast and slow, as per the title). Notably, the paper is really well-written and readable, and the experiments on novel category + novel instance recognition are really convincing specifically for fast-mapping (4.1).""72\tThe paper discusses the development of an embodied learning system that can perform fast-mapping, which is the ability to learn the names of unfamiliar objects after a single exposure. The system is an embodied agent situated in a 3D game environment that learns to respond to linguistic stimuli by executing sequences of motor actions. The agent observes the world through active perception of raw pixels and can immediately apply its knowledge to carry out instructions based on the objects it has learned. The paper highlights the importance of one-shot learning in language models and its potential applications in machine learning and developmental psychology.\tAn agent following instructions in a grounded world is a core task in AI. This paper studies agent that accomplish this using memory-based architecture. This paper presents an argument for a multi-modal memory-architecture called DCEM whose key/queries and values are dependent on language and vision modalities respectively (or vice versa). An argument is made that this will be helpful for generalizing to novel language at test-time. Results are presented in a simple 3D domain containing several objects randomly sampled each time from a set of 30 objects. Task contain two types of instructions: \"pick up an object\" and \"place an object on another object\". Interaction proceeds in episodes where each episode contains a discovery phase where the agent learns the phrase associated with each object, and an instruction phase where the agent solves a given instruction. The proposed DCEM model outperforms baselines on various metrics and ablation. Importantly, it is shown that the DCEM can generalize to novel object names. ""73\tThe paper discusses the concept of Few-Shot Learning (FSL), which aims to reduce the need for large labeled datasets by defining a distribution over tasks, each containing a few labeled data points and a set of target data belonging to the same set of classes. FSL methods are commonly trained through episodic meta-training, where the model is repeatedly exposed to batches of tasks sampled from a task-distribution and then tested on a different but similar distribution in the meta-testing phase. The paper highlights the importance of FSL in reducing the burden of labeled data and improving the generalizability of deep learning models.\tThe paper analyses the effect of class imbalance on few-shot learning problems. It draws a number of interesting (but kind of expected) conclusions e.g., the support set imbalance has a larger influence on the FSL performance compared to base class imbalance, a high impact of imbalance on gradient-based meta-learning methods compared to metric learning approaches. The paper is overall  ""74\tThe paper discusses the concept of Few-Shot Learning (FSL), which aims to reduce the need for large labeled datasets in deep learning methods by defining a distribution over tasks, each containing a few labeled data points and a set of target data belonging to the same set of classes. FSL methods are commonly trained through episodic meta-training, where the model is repeatedly exposed to batches of tasks sampled from a task-distribution and then tested on a different but similar distribution in the meta-testing phase. The paper highlights the importance of FSL in reducing the burden of large labeled datasets and improving the generalizability of deep learning models.\tThe authors present a detailed study of few-shot class-imbalance along three axes: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. The authors extensively compare over 10 state-of-the-art few-shot learning methods using backbones of different depths on multiple datasets. The analysis reveals that 1) compared to the balanced task, the performances of their class-imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric-based methods generally suffer less, 2) strategies used to mitigate imbalance in supervised learning can be adapted to the few-shot case resulting in better performances, 3) the effects of imbalance at the dataset level are less significant than the effects at the support set level. ""75\tThe paper discusses the challenges of deep learning methods, which require large labeled datasets to acquire robust and generalizable features. Few-Shot Learning (FSL) aims to reduce this burden by defining a distribution over tasks, with each task containing a few labeled data points and a set of target data belonging to the same set of classes. FSL methods are commonly trained through episodic meta-training, where the model is repeatedly exposed to batches of tasks sampled from a task-distribution and then tested on a different but similar distribution in the meta-testing phase. The paper highlights the importance of FSL in reducing the need for large labeled datasets and improving the generalizability of deep learning models.\tThis paper conducts extensive comparison experiments to study the effect of class-imbalance for many few-shot approaches. A detailed study of few-shot class-imbalance along three axes: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques, are presented. Also, this paper is clearly written and easy to understand. ""76\tThe paper discusses the increasing attention in literature towards machine learning methods, particularly neural networks, for graph-structured input. Graph Convolutional Networks (GCNs) are relatively fast to compute and have shown good predictive performance. However, stacking too many GC layers may be detrimental to the network's ability to represent meaningful topological information due to a too high Laplacian smoothing. The paper proposes a new method called Attention-based Graph Convolutional Networks (AGCNs) that use attention mechanisms to selectively aggregate information from different nodes in the graph. The proposed method is evaluated on several benchmark datasets and shows improved performance compared to traditional GCNs.\tThe paper proposes Polynomial Graph Convolution (PGC), which enjoys a larger-than-one-hop receptive field within a single layer. This is done by first propagating information with a fixed (not learned) propagation matrix (e.g. adjacency matrix or graph Laplacian), and then projecting the information from different topological distances with a learned linear layer. PGC is shown to be theoretically more expressive than linearly stacking simple graph convolutions; experiments on several graph classification tasks show good performance.""77\tThis paper discusses the increasing attention in literature towards machine learning methods, particularly neural networks, for graph-structured input. Graph Convolutional Networks (GCNs) are a popular method based on the definition of a convolution operator in the graph domain, which are relatively fast to compute and have shown good predictive performance. However, stacking too many GC layers may be detrimental to the network's ability to represent meaningful topological information due to a too high Laplacian smoothing. This paper suggests a new approach called Multi-Head Graph Convolutional Networks (MH-GCNs) that uses multiple attention mechanisms to capture different aspects of the input graph and improve the network's ability to represent topological information. The authors demonstrate the effectiveness of MH-GCNs on several benchmark datasets.\tThis work proposes the Polynomial Graph Convolutional Networks (PGCNs), which is built upon the Polynomial Graph Convolution (PGC). The PGC is able to aggregate k-hop information in a single layer and comes with the hyper-parameter k. The PGCNs are composed of a PGC with k=1, followed by a PGC with a chosen k (usually > 1), and a complex readout layer using avg, max, and sum over all nodes. Theoretically, the proposed PGC has two major benefits as claimed: 1) Common graph convolution operators can be represented as special cases of the PGC; 2) A PGC with k = q (q > 1) is more expressive than linearly stacked q PGCs with k=1. The PGCNs are thus more general, expressive, and efficient than existing GNNs. Experimental studies are conducted on common graph classification benchmarks, showing the improved performances of the PGCNs.""78\tThe paper discusses the increasing attention in literature towards machine learning methods, particularly neural networks, for graph-structured input. Graph Convolutional Networks (GCNs) are a popular method that uses a convolution operator in the graph domain and has shown good predictive performance. However, stacking too many GC layers may be detrimental to the network's ability to represent meaningful topological information due to a high Laplacian smoothing. The paper proposes a new method called Graph Attention Networks (GATs) that uses attention mechanisms to weight the importance of neighboring nodes and improve the network's ability to capture important topological information. The authors compare the performance of GATs to GCNs on several benchmark datasets and show that GATs outperform GCNs in terms of accuracy and efficiency.\tThe article presents a novel framework for Graph Convolutional Neural Networks (GCNs). The method called  Polynomial Graph Convolution (PGC) is based on concatenating the powers of a transformed adjacent matrix in a given layer. The paper shows that various popular variants of GNNs can be expressed using the PGC framework.  Theoretical results presented show that PGC with higher degree is more expressive that deeper std. GNNs. Numerical results are presented on graph classification task that illustrate the performance of the method.""79\tThe paper discusses the use of scene graphs (SGs) as an interpretable and structural representation of scenes in both computer vision and computer graphics. SGs summarize entities in a scene and plausible relationships among them, and have found a variety of applications such as image captioning, visual question answering, high-level reasoning tasks, image retrieval, and image generation. However, most prior work on SG generation relies on the availability of expensive and limited labeled datasets. The paper proposes the use of synthetic data to generate SGs, which can overcome the limitations of limited labeled data. The authors demonstrate the effectiveness of their proposed method on several benchmark datasets.\tThis paper introduces a framework to utilize the synthetic data as augmentations in the scene graph generation task, which is able to narrow the domain gap by decomposing it into several discrepancies between the two domains. They are the first to propose the synthetic-to-real transfer learning for SGG. The experimental results show the Sim2SG can improve the baseline models in three different scenarios: CLEVR, Dining-Sim, and Drive-Sim.""80\tn computer vision has been proposed as a solution to this problem. This paper discusses the use of synthetic data for scene graph generation, specifically focusing on the generation of synthetic images and their corresponding scene graphs. The authors propose a method for generating synthetic scene graphs using a combination of generative adversarial networks (GANs) and graph convolutional networks (GCNs). They evaluate their method on several benchmark datasets and show that their approach outperforms existing methods for generating synthetic scene graphs. The authors conclude that their method has the potential to improve the performance of scene graph generation models by providing a larger and more diverse set of training data.\tThe paper addresses the problem of learning scene graphs from synthetic data and unlabeled real data while performing well on real data by narrowing the content and appearance gap between the two domains when training on synthetic data. Scene graphs are extracted in a two-step process, mapping input to an intermediate latent space and generating the final prediction from the latent space. The authors decompose the content gap into two components: (a) label discrepancy, i.e. how much do the label distributions between the two domains differ, and (b) prediction discrepancy, i.e. the difference in distributions of outputs predicted from the latent space for the two domains. They further model the appearance gap by aligning the latent representation for both domains after accounting for the content gap (to avoid spurious influence of differing content distributions as the latent space is expected to comprise content and appearance). Most of these components are intractable and the paper provides approximations. Empirical investigation on two entirely synthetic and one real/synthetic data set provide evidence for the benefit of the method in closing the posed domain gaps as well as the quality of chosen approximations, the influence of the individual content and appearance gap terms, and the effectiveness of the optimization procedure.""81\tn computer vision has been proposed as a solution to this problem. This paper discusses the use of generative adversarial networks (GANs) to generate synthetic scene graphs for training and testing machine learning models. The authors propose a novel GAN architecture called SG-GAN that can generate high-quality scene graphs with diverse structures and relationships. They evaluate SG-GAN on several benchmark datasets and show that it outperforms existing methods for synthetic SG generation. The authors also demonstrate the usefulness of synthetic SGs for improving the performance of machine learning models on downstream tasks such as image captioning and visual question answering. Overall, this paper presents a promising approach for addressing the data scarcity problem in SG-based applications.\tThe paper tackles the problem of sim2real transfer for scene graph inference. It proposes an approach for closing the gap between simulated training data and real test data, to allow models trained purely on simulated data to be deployed on real images. The approach is tested on multiple environments, including transfer from a driving scene simulator to real KITTI scenes.""82\tThis paper discusses the recent success of model-based methods in continuous action space domains, which have achieved higher sample efficiency than previous model-free methods. Model-based methods use a high Update-To-Data (UTD) ratio, which is the number of updates taken by the agent compared to the number of actual interactions with the environment. The state-of-the-art model-based algorithm, Model-Based Policy Optimization (MBPO), achieves higher sample efficiency than the model-free Soft-Actor-Critic (SAC) in the OpenAI MuJoCo benchmark. The paper raises the question of whether it is possible to achieve such high performance without a model and introduces a new algorithm, Model-Free Policy Optimization with Dynamics Regularization (MPODR), which achieves similar performance to MBPO without using a model.\tThis paper proposes Randomized Ensembled Double Q-Learning (REDQ), a new model-free RL algorithm that aims to improve the sample efficiency over existing model-free methods. Experiments on Mujoco show that REDQ achieves better sample efficiency than popular model-free methods such as SAC and is comparable with model-based methods such as MBPO. The paper further provides extensive ablation studies that justify the necessity of the algorithmic components in REDQ and show that improved Q estimation bias may have been the key reason for the performance gain. The paper also provides some theoretical analysis of the Q estimation bias.""83\tThis paper discusses the recent success of model-based methods in continuous action space domains, which have achieved higher sample efficiency than previous model-free methods. Model-based methods achieve this by using a high Update-To-Data (UTD) ratio, which is the number of updates taken by the agent compared to the number of actual interactions with the environment. The paper introduces Model-Based Policy Optimization (MBPO), a state-of-the-art model-based algorithm that updates the agent with a mix of real and fake data and uses a large UTD ratio of 20-40. The paper then raises the question of whether it is possible to achieve similar performance without a model.\tThe paper proposes three techniques that altogether greatly improves the performance of soft actor-critic (SAC), resulting in a new algorithm called REDQ. (1) A higher update-to-data ratio, which speeds up the critic update. (2) Using the average ensemble Q for the policy gradient, therefore reducing its variance. (3) Taking the min of a small subset of the ensemble Qs to compute the target Q, therefore reducing the Q bias. The paper also performs extensive ablation studies to prove the importance of each technique.""84\tThis paper discusses the recent success of model-based methods in continuous action space domains, which have achieved higher sample efficiency than previous model-free methods. The paper specifically focuses on Model-Based Policy Optimization (MBPO), a state-of-the-art model-based algorithm that uses a high Update-To-Data (UTD) ratio to update the agent with a mix of real and \"fake\" data from its model. The paper raises the question of whether it is possible to achieve similar performance without a model and introduces a new model-free algorithm called Model-Free Policy Optimization (MFPO). The paper evaluates MFPO against MBPO and other model-free algorithms in the OpenAI MuJoCo benchmark and shows that MFPO achieves similar performance to MBPO while using a UTD ratio of 1. The paper concludes that MFPO is a promising model-free alternative to model-based methods in continuous action space domains.\tThis work proposes a modification for double Q-learning, termed as randomized ensembled double Q-learning (REDQ). REDQ maintains $N$ different Q functions, and for each update, the target value is a minimization over $M$ randomly chosen Q functions, where $1 \\le M \\le N$. In addition, REDQ adopts a high update-to-data ratio to improve the sample efficiency. Empirical results show that the proposed method outperforms state-of-the-art model-based algorithms in certain tasks with continuous action space.""85\tThis paper presents a new neural architecture called DIDA, which extends deep learning from probability distributions to achieve invariance under permutation of the features. The architecture inherits the properties of universal approximation and is robust with respect to Lipschitz-bounded transformations of the input distribution. The paper also demonstrates the effectiveness of DIDA on two dataset-level tasks, where it learns meta-features to characterize labeled datasets. The first task involves predicting whether two dataset patches are extracted from the same initial dataset, while the second task involves predicting the learning performance achieved by a hyper-parameter. Empirical results show that DIDA outperforms other state-of-the-art methods on both tasks.\tThe method introduces the DIDA architecture to learn from distributions and be invariant to feature ordering and size.  The authors extend the ideas proposed by Maron et al. (2020) to the continuous domain and generalize their results.  The experiments are done on two tasks.  The patch identification (out-of-distribution test) clearly show the invariance to feature and dataset size. Nevertheless, it is not clear whether the method is invariant to feature permutation.  The performance model task shows properties of the architecture to predict global structures of the dataset within their meta-features.""86\tThis paper discusses recent advances in deep learning that achieve classification or regression from distribution samples, which are invariant under permutation of the samples. The paper proposes a new neural architecture called DIDA, which extends these neural architectures to achieve invariance under permutation of the features. DIDA inherits the properties of universal approximation and is robust with respect to Lipschitz-bounded transformations of the input distribution. The paper also demonstrates the merits of the approach on two tasks defined at the dataset level, where DIDA learns meta-features supporting the characterization of a labeled dataset. The first task is predicting whether two dataset patches are extracted from the same initial dataset, and the second task is predicting the learning performance achieved by a hyper-parameter.\tThe paper presents a neural network layer designed to process distribution samples that is invariant to permutations of the samples and the features. The proposed method is compared empirically to DSS, which achieves the same types of invariance but is restricted to point sets rather than discrete or continuous probability distributions. The two tasks used for the empirical evaluation in the paper are: a) patch identification (are two blocks of data extracted from the same original dataset?) and b) model configuration assessment (is one configuration of a learning algorithm going to produce a more accurate model for a particular dataset than another one?). On the first task, the paper compares to models built using Dataset2Vec embeddings as well as DSS. On the second task, the paper compares to handcrafted features as well as DSS. In both tasks, the proposed method produces more accurate predictors than DSS, etc. The paper also has some theoretical results regarding the universality of the proposed architecture and its robustness w.r.t. Lipschitz-bounded transformations. ""87\tThis paper discusses recent advances in deep learning from probability distributions that achieve classification or regression from distribution samples, which are invariant under permutation of the samples. The paper proposes a new neural architecture called DIDA that extends these neural architectures to achieve invariance under permutation of the features as well. DIDA inherits the NN properties of universal approximation, and its robustness with respect to Lipschitz-bounded transformations of the input distribution is established. The paper also demonstrates the merits of the approach on two tasks defined at the dataset level. On both tasks, DIDA learns meta-features supporting the characterization of a (labelled) dataset. The first task consists of predicting whether two dataset patches are extracted from the same initial dataset. The second task consists of predicting whether the learning performance achieved by a hyper-parameter.\tThis paper proposes a novel set/distribution representation architecture DIDA, which leverages pairwise embedding of the set\u2019s elements. The method can be used to represent discrete and continuous distribution representation. The authors also provide the theoretical proofs of the universality of the invariant layers, the local consistency. The experiments show that the architecture improves some dataset representation tasks""88\tThe paper discusses the increasing importance of graph learning in machine learning and data mining applications. Graph representations are commonly used to represent high-dimensional data sets, where each data point is represented as a node and edges are assigned weights to encode similarity between nodes. However, learning meaningful graphs from large data sets at scale remains a challenging problem. The paper highlights recent graph learning methods that use emerging graph signal processing techniques to estimate sparse graph Laplacians, which have shown promising results.\tThis paper studies ways of adding edges to graphs to improve the result of spectral embedding / clustering. It refines existing embeddings using by measuring edges' effect on Laplacian eigenvalues, and adjusting such edges to reduce the distortions. The performance of the algorithm is justified using developments of worst-case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy.""89\tThis paper discusses the increasing importance of graph learning in machine learning and data mining applications. Graph representations are commonly used to represent high-dimensional data sets, with each data point represented as a node and edges assigned weights to encode similarity between nodes. However, learning meaningful graphs from large data sets at scale remains a challenging problem. The paper highlights recent graph learning methods that leverage emerging graph signal processing techniques for estimating sparse graph Laplacians, which have shown promising results.\tThe paper proposes a graph learning method for spectral embedding and associated problems such as clustering and dimension reduction. What differentiates the method from much the existing literature is that it focuses approximating an optimal densification of a very sparse initial graph rather than on sparsification of an initial graph, as is more common. The method is based on iteratively identifying edges to add to the graph so as to best improve the corresponding spectral embedding, so called \"spectrally critical\" edges. The authors motivate spectral criticality in relation to the partial derivatives of an objective function inspired by the log-likelihood of a Gaussian graphical model. In particular, those with the highest partial derivatives will tend to be those which, through their addition to the graph, lead to the greatest increase in this objective. The authors go on to discuss a close connection between spectral criticality and distance distortion when comparing the spectral embedding and the original input space. Since the initial graph is very sparse it can be efficiently determined, and the relatively small number of additional edges which need to be added by the proposed method to obtain a high quality embedding means that the entire procedure can be implemented efficiently.""90\tThe paper discusses the increasing importance of graph learning in machine learning and data mining applications. Graph representations are commonly used to convert high-dimensional data sets into graphs, where each data point is represented as a node and edges are assigned weights to encode similarity between nodes. However, learning meaningful graphs from large data sets at scale remains a challenging problem. The paper suggests that recent graph learning methods leverage emerging graph signal processing techniques for estimating sparse graph Laplacians, which have shown promising results.\tand significance: Learning a graph from data is an important, yet less studied, problem. The proposed algorithm (GRASPEL) is based on a graphical Lasso formulation with the precision matrix restricted to be a graph Laplacian. The algorithm starts with a sparse kNN graph, and recursively adds critical edges (identification of these critical edges based on Lasso and spectral perturbation analysis is the main contribution of the paper). ""91\tThe paper discusses the challenges of designing autonomous reinforcement learning (RL) agents that can persistently exist in the world and solve diverse tasks. While RL has been successful in achieving specific goals in complex and uncertain environments, it requires a task-specific reward function. To build a universal reward function and generate diverse goals for training, raw sensory inputs such as images have been considered as common goals. However, this exacerbates the challenge of designing autonomous RL agents that can deal with such perceptual inputs. The paper proposes a framework that combines unsupervised representation learning with goal-conditioned RL to enable agents to learn a universal reward function and generate diverse goals for training. The proposed framework is evaluated on several benchmark tasks and shows promising results.\tThis paper proposes a method for combining intrinsic motivation on a state space with goal-conditioned reinforcement learning (GCRL), where goals are defined in some \u201cperceptual space,\u201d such as text or images, which describe the current state. The authors assume access to a renderer that maps states to perceptual goals, but do not assume that the renderer is differentiable. The authors propose to train an intrinsically motivated latent-conditioned policy, using similar techniques as past work in which a policy maximizes the mutual information between a latent variable and the current state. The goal-conditioned policy is then trained to effectively imitate the latent-conditioned policy by maximizing the same reward as the latent-conditioned policy, conditioned on only the rendered version of the final state reached by the latent-conditioned policy. The authors demonstrate that the overall method outperforms past GCRL methods on a variety of tasks (Atari, MuJoCo manipulation and locomotion, and toy tasks).""92\tThe paper discusses the challenges of designing autonomous reinforcement learning (RL) agents that can solve diverse tasks in complex and uncertain environments. While RL has been successful in driving agents to achieve specific goals in individual tasks, building a universal reward function and generating diverse goals for training is necessary for agents to exist persistently in the world. The paper suggests using raw sensory inputs, such as images, as common goals for agents to practice on and achieve, but this poses further challenges for designing autonomous RL agents that can deal with such perceptual inputs.\tThis paper proposes an unsupervised learning objective for learning perceptual goal-conditioned policies. The goal is to enable unsupervised discovery of high-level behaviors in tandem with a perceptual-goal conditioned policy that can achieve these behaviors. The learning proceeds by training one policy to exhibit diverse behaviors; the states induced by these behaviors are then rendered and used as target goal states for a separate goal-conditioned policy.""93\tThe paper discusses the challenges of designing autonomous agents that can solve diverse tasks in complex and uncertain environments using reinforcement learning (RL). RL involves learning a specific policy for individual tasks relying on task-specific rewards, but to achieve persistent existence and solve diverse tasks, a universal reward function and a mechanism to automatically generate diverse goals for training are needed. The paper suggests using raw sensory inputs such as images as common goals for agents to practice on and achieve, which presents further challenges for designing autonomous RL agents that can deal with such perceptual inputs.\tThis paper proposes a new solution to the problem of learning goal-conditioned policies without hand-crafted rewards. Prior work in this domain learn an embedding space to compute reward between current state and goal. In contrast, this paper utilizes unsupervised skill discovery from [1] to obtain a discriminator that identifies which states belong to a particular skill. Then, the final state of a given skill's execution is used as a goal input to a goal-conditioned policy, which is rewarded if it generates states that the discriminator identifies with this skill. The paper aims to validate the benefit of such a reward over other embedding-distance based reward functions on a variety of environments.""94\tThe paper discusses the use of reinforcement learning (RL) in real-world sequential decision-making problems such as medical domains, personalized recommendations, hardware placements, and database optimization. However, in some applications, it is desirable to restrict the agent from adjusting its policy frequently due to the high cost and risks associated with changing the deployed policy. The paper proposes an RL algorithm that admits a low switching cost, allowing the deployed policy to interact with the environment without changing frequently. The approach is illustrated with examples from robotics, education, and dialogue systems. The paper cites a study by Gu et al. (2017) that trained robotic manipulation by decoupling the training and experience collecting.\tIn many real world applications for RL such as medicine, there are limits on the number of policies from which we can simulate data. This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Experiments on a medical treatment environment and Atari show that the approach obtains similar performance to on-policy RL with fewer changes of the simulation policy.""95\tThe paper discusses the use of reinforcement learning (RL) in real-world sequential decision-making problems such as medical domains, personalized recommendations, hardware placements, and database optimization. In some applications, it is desirable to restrict the agent from adjusting its policy frequently due to the high cost and risk associated with changing the deployed policy. The paper proposes an RL algorithm that admits a low switching cost, allowing the deployed policy to interact with the environment without changing frequently. The approach is illustrated with examples from robotics, education, and dialogue systems. The paper cites Gu et al. (2017) as an example of training robotic manipulation by decoupling the training and experience collecting.\tIn the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance. This study is done in the context of deep reinforcement learning. A few generic baselines solutions are provided as well as a more complex solution that empirically outperforms the baselines.""96\tThe paper discusses the use of reinforcement learning (RL) in real-world sequential decision-making problems such as medical domains, personalized recommendations, hardware placements, and database optimization. However, in some applications, it is desirable to restrict the agent from adjusting its policy frequently due to the high cost and risks associated with changing the deployed policy. The paper proposes an RL algorithm that admits a low switching cost, allowing the deployed policy to interact with the environment without changing frequently. The approach is illustrated with examples from robotics, education, and dialogue systems. The paper also references a study by Gu et al. (2017) that trained robotic manipulation by decoupling the training and experience collecting.\tThis paper studies RL with low switching cost under the deep RL setting. It points out several naive algorithms like switching after a certain number of steps and then propose a new heuristic. This heuristic learns a new policy offline using the experience replay the behavior collected and switches the behavior policy once the similarity of the feature embeddings of the current state by these two policies becomes large. The paper also makes an attempt to provide a theoretical justification for a better understanding of the heuristic. This method might outperform the naive algorithms by some margin, if any. It would be a more interesting manuscript if some stronger results could be provided from the perspective of any of theory, experiments, or applications.""97\tThis paper presents a new stochastic optimization algorithm called Homotopy-Stochastic Gradient Descent (H-SGD) that combines homotopy methods and stochastic gradient descent (SGD) to solve finite-sum problems. The algorithm is designed to solve problems where only noisy function values and gradients are available via a stochastic first-order oracle. These types of problems are common in machine learning and deep learning applications where the dimensionality of the datasets makes full function and gradient evaluations too expensive. The proposed algorithm is analyzed theoretically and compared to other stochastic optimization algorithms. Results show that H-SGD outperforms other algorithms in terms of convergence rate and computational efficiency.\tThis paper proposes homotopy SGD (H-SGD) which solves a sequence of unconstrained problems with a homotopy map and homotopy parameter. The authors analyze the algorithm for solving nonconvex problems satisfying PL condition. The analysis works with a generic homotopy map and homotopy parameter satisfying certain conditions (given in Sec 3.1). The authors show linear convergence to a neighborhood of the minimizer. The theoretical results are validated with experiments with clear explanations.""98\tThis paper presents a new stochastic optimization algorithm called Homotopy-Stochastic Gradient Descent (H-SGD), which combines homotopy methods and stochastic gradient descent (SGD) to solve finite-sum problems. The algorithm is designed to solve problems where only noisy function values and gradients are available via a stochastic first-order oracle. These types of problems are common in machine learning and deep learning applications where the dimensionality of the datasets makes full function and gradient evaluations too expensive. The paper provides theoretical development and analysis of the algorithm and shows its effectiveness in solving various optimization problems.\tThis paper proposed a Homotopy-Stochastic Gradient Descent (H-SGD) algorithm by applying homotopy strategy to explore the nice local structures of problems. H-SGD can gradually approximate to the target objective function and enjoys a global linear convergence to reach a neighborhood of a minimizer. As verified by the author, the assumption of this paper is weaker than its predecessors, Karimi et al., 2016; Vaswani et al., 2019. Further, the numerical experiments verified the effectiveness of H-SGD on regression and classification tasks.""99\tThis paper presents a new stochastic optimization algorithm called Homotopy-Stochastic Gradient Descent (H-SGD), which combines homotopy methods and stochastic gradient descent (SGD) to solve finite-sum problems. The algorithm is designed to solve problems where only noisy function values and gradients are available through a stochastic first-order oracle. These types of problems are common in machine learning and deep learning applications where the dimensionality of the datasets makes full function and gradient evaluations too expensive. The proposed algorithm is analyzed theoretically and compared to other existing algorithms. The results show that H-SGD outperforms other algorithms in terms of convergence rate and computational efficiency.\t1. It seems to me the proposed Homotopy-SGD is not a practical algorithm, as in each iteration the algorithm has to solve a nontrivial (possibly nonconvex) subproblem. In other words, each subproblem can be as difficult as the original problem. This leads to an essential question that what is the practical motivation of this algorithm?""100\tThis paper discusses the challenge of estimating 3D geometry from sparse observations, also known as shape completion, which is important for robotics and autonomous driving. While recent methods have achieved success using observations to infer parameters of an implicit 3D geometric representation, they require relatively dense observations. The paper introduces a novel methodology that enables state-of-the-art shape completion from highly sparse observations by leveraging the geometric information contained in the observations. The method is evaluated on benchmark datasets and outperforms existing methods in terms of accuracy and efficiency.\tThis paper proposes a way of reconstructing a surface from sparse point clouds via a \"meta learning\" approach. Specifically, the authors view each shape in a collection as a \"domain\", and predicting the SDF values of points in R^3 to reconstruct a given shape (the reconstructed surface is the isosurface of the SDF field) as the \"task\" for that domain. Then, they use a network to predict a distribution over \"task-specific\" latent vectors that characterizes the reconstruction task for a given shape. Given a latent-vector sampled for one shape, they pass it to a decoder that predicts the SDF value at any point in R^3 for that shape.""101\tThis paper discusses the challenge of estimating 3D geometry from sparse observations, also known as shape completion, which is important for robotics and autonomous driving. While recent methods have achieved success using implicit 3D geometric representations, they require relatively dense observations. The paper introduces a novel methodology that enables state-of-the-art shape completion from highly sparse observations by leveraging the geometric information of the object's symmetry. The method is evaluated on various datasets and outperforms existing methods in terms of accuracy and efficiency.\tThis paper introduces a meta-learning approach for the neural implicit representation of 3D shapes. The main idea, in my understanding, is to consider the points in the input point cloud as few-shot examples of the object so that each of them can be encoded in a way to best approximate the entire object information. The experiments show that the network can reconstruct the entire shape well even with a very small number of the input points, such as 50 and 100. For better reconstruction, the authors also proposed to use some implicit function regularizations, which are introduced in a previous work (Gropp et al., 2020)."