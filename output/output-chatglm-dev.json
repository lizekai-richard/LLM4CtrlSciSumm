"1\tThis paper provides an overview of Federated Learning (FL), a distributed optimization approach to large-scale learning and estimation scenarios where multiple clients collaborate under the coordination of a central server. It discusses the background of FL, its benefits over centralization of all training data, and the lack of formal privacy guarantee. The paper also presents an example of how FL can be used in a mobile and edge device application.\tThis paper studies FL under local differential privacy constraints. They identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high\u0002dimensional compatibility, and develop a gradient-based learning algorithm sqSGD that addresses both concerns. They improve the base algorithm in two ways: First, apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs. Secondly, utilize randomized rotation as a preprocessing step to reduce quantization error. ""2\tThis paper provides an overview ofFederated Learning (FL), a distributed machine learning approach that allows multiple clients to collaborate on a machine learning problem, while offering practical privacy improvements over centralized training. The paper discusses the background of FL, the benefits of using it in industries, and the challenges that arise when implementing it. The paper also presents a detailed analysis of the privacy guarantee of FL, and discusses the potential ways to improve it.\tThis paper studies a low communication algorithm for multivariate mean estimation in the federated learning setting with differentially private communication. The algorithm uses quantization and dimension subsampling (only reporting some coordinates of the vector) to lower communication and randomized rotation (essentially applying a random orthogonal matrix) to reduce quantization error. They then apply this algorithm to ERM, using it as a subroutine in SGD. They experimentally explore the behavior of their algorithm on a number of benchmark datasets. They consider how the performance changes as they vary epsilon, the discretization parameter and the number of epochs (in SGD). ""3\tThis paper discusses the topic ofFederated Learning (FL), which is a distributed optimization application that allows multiple clients to collaborate on a machine learning problem under the coordination of a central server. The server collects minimal information from the clients for immediate aggregation, which provides practical privacy improvements over centralizing all the training data. However, FL lack a formal privacy guarantee, making it important to consider privacy concerns when implementing it. This paper also discusses the applications of FL in industries, such as mobile and edge device applications, and the potential issues that arise when implementing it in practice.\tThe paper proposed a differentially private training algorithm for federated learning. The target is to achieve communication reduction while keeping differential privacy during training. The proposed algorithm adds a few new components to SGD, including a privacy mechanism, a random rotation to reduce quantization error, a gradient coordinate selection mechanism to reduce communication/computation. Experiments with high \\epsilon local differentially privacy guarantees are conducted. The proposed algorithm outperforms a baseline algorithm.""4\tThis paper focuses on self-attention networks (SANs) in the context of natural language processing. SANs are a type of neural network that learn to capture order dependencies in a language by using a positional encoding mechanism. The learned embedding is then added to the corresponding word embedding to obtain an input representation. SANs are optimized iteratively to model language knowledge on the whole training dataset, which has achieved state-of-the-art performance in many natural language processing tasks. This paper provides an overview of the paper, including the background of self-attention networks, the positional encoding mechanism, the input representation, the optimization process, and the achieved performance.\tThis submission works on the neural machine translation problem. The authors extend the previous works on leveraging language statistics or prior knowledge (SMT model or whatever) in LSTM based NMT models in self-attention based NMT models, Transformer model. The authors propose two alternatives to incorporate prior knowledge, which are the word frequency information for the monolingual data and the prior translation lexicon information for the bilingual data. These resources are integrated into the hidden representations from the self-attention computations and then the two output hidden representations are gated together for upper computations. The experiments are conducted on two typical NMT datasets: WMT14 En->De and WMT17 Zh->En, the results show that the proposed method can improve the NMT model performances. ""5\tThis paper discusses the development of self-attention networks (SANs) as a new type of neural network for natural language processing. SANs are designed to learn a language representation by encoding order dependencies in the language using a positional encoding mechanism and then adding the learned representation to word embeddings to obtain an input representation. The paper provides an overview of the development of SANs, including their history and background, and discusses some of the recent research in this area. It also describes the use of SANs in various natural language processing tasks and their performance compared to other methods. The paper ends by discussing the future directions and challenges of using SANs in natural language processing.\tThis paper proposes a method to introduce **prior knowledge** into Transformer-based sentence encoders, here in the context of neural machine translation (NMT). More concretely, the prior knowledge is represented in the form of a matrix $\\boldsymbol{M}$, where each row denotes a vector of prior knowledge associated with each word $x_i$. The prior knowledge matrix $\\boldsymbol{M}$ is then represented as a (key, value) pair that can be attended by the query matrix $\\boldsymbol{Q}$ (the same query matrix as used in the main NMT component) using a standard Transformer self-attention mechanism. This procedure results in a prior knowledge representation matrix $\\boldsymbol{PK}$, which is then combined with the standard Transformer encoder output using a simple gating mechanism. ""6\tThis paper discusses the development of self-attention networks (SANs) as a powerful machine learning technique for natural language processing. SANs are a type of neural network that use the positional encoding mechanism to learn a representation of the language that is parallel to the training process. This representation is then used to improve the performance of SAN-based models on a wide range of natural language processing tasks. The paper also discusses the challenges and limitations of usingSANs in natural language processing, and provides future directions for the technique.\tThis paper presents a method for introducing prior knowledge into Transformer models. More specifically, the authors propose to use an additional self-attention block to incorporate prior knowledge about the word frequency and translation lexicon and use a gating mechanism to combine its output with that of the standard sefl-attention block. Experiments are conducted using English-to-German and Chinese-to-English translation datasets, and the results show the effectiveness of the proposed approach.""7\tThis paper discusses the idea of proactive cyber defense, which is the use of technology to shift between various configurations of a cyber system in order to prevent an attacker from gathering valuable information during the reconnaissance phase of an attack. The idea of moving Target Defense, which is a type of proactive cyber defense, is introduced, and the paper\u8ba8\u8bbas the importance of considering an optimal movement strategy in order to maximize security and minimize the impact on the system's performance. The paper also discusses the challenges of implementing MTD systems and the potential benefits of such technology.\tThis paper proposes the game-theoretic model of Bayesian Stackelberg Markov Games (BSMGs), a generalization of Markov games, as a formalism for studying Moving Target Defense (MTD) systems, a type of defender-attacker game with applications to cybersecurity. An algorithm for finding the Stackelberg equilibrium in BSMGs, called Bayesian Strong Stackelberg Q-Learning (BSS-Q) is proposed, and an OpenAI Gym-style environment for testing the derived policies in particular MTD settings is introduced, which allows for empirical evaluation of the policies' effectiveness. The paper then shows experimental results supporting the BSS-Q algorithm's success at finding the Strong Stackelberg Equilibrium of BSMGs.""8\tThis paper presents a review of proactive cyber defense systems, including Moving Target Defense (MTD). The paper\u8ba8\u8bba\u4e86\u4e3a\u4ec0\u4e48 MTD \u662f\u6709\u6548\u7684\uff0c\u4ee5\u53ca\u5b83\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u7a7a\u95f4\u3002 The paper also discusses the optimal movement strategy for MTD systems, which is important to ensure that they are effective at maximizing security and minimizing the impact on the system's performance. Additionally, the paper\u8ba8\u8bba\u4e86 MTD \u4e0e\u5176\u4ed6\u9632\u5fa1\u6280\u672f\u7684\u5173\u7cfb\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6700\u65b0\u7684\u7814\u7a76\u3002\tThis paper studies the problem of learning how to adapt the defense methods in the domain of cybersecurity. The paper proposes a new model called Bayesian Stackelberg Markov Games (BSMG) to capture the uncertainty of the attacker's types as well as their strategic behaviors. The authors design Bayesian Strong Stackelberg Q-learning that can converge to the optimal movement policy for BSMG. The empirical studies verify the support the theoretical results.""9\tThis paper discusses the idea of proactive cyber defense, which is the use of technology to shift between various configurations of a cyber system in order to make the attacker's knowledge of the system's configuration useless at attack time. The idea of moving target defense is to shift between different configurations of the cyber system in order to maximize security and minimize the impact on the system's performance. The paper also discusses the importance of considering an optimal movement strategy in MTD systems and the game-theoretic formulation of the cyber system as a two-player game.\tThis paper introduces a Bayesian Stackelberg Markov Game (BSMG) model that considers a defender\u2019s uncertainty over attackers\u2019 types when implementing defensive strategies. It also proposes to use a Bayesian Strong Stackelberg Q-learning method to learn defense policies by first simulating an adversary to obtain feedback of an attack and then computing the Bayesian Strong Stackelberg Equilibrium for the BSMG with a solver. In this way, this work relaxes the assumption that the defender knows attackers\u2019 types in existing game-theoretic models for moving target defense.""10\tThe paper discusses the state-of-the-art performance of unsupervised disentanglement achieved using Variational Autoencoder (VAE) based frameworks. The paper shows thatVAE impermanent choices that encourage a PCA-like behavior locally on data samples lead to the success of such frameworks. However, the disentanglement performance of VAE based frameworks still faces a trade-off between local orthogonality and data construction. The paper proposes a simple yet effective VAE ensemble framework consisting of multiple VAEs, which address this challenge by allowing each model to learn its own unique entangled representation and the disentangled representations to be similar but signed permutationally similar. The proposed framework is also shown to be effective in handling large amounts of data.\tThis paper proposes a simple and effective technique to improve disentanglement by coupling the latent spaces of different VAE models. It builds on Duan et al. (2019)\u2019s proposed method to rank the representations of different models. By learning a VAE ensemble with linear transformations between the latent spaces and an additional \u201ccross-model\u201d reconstruction loss, the authors show that they can achieve significantly better disentangling.""11\tThis paper discusses the use of Variational Autoencoder (VAE) frameworks for unsupervised disentanglement. The paper provides a theoretical analysis of how well VAE-based frameworks perform on this task, showing that they achieve state-of-the-art performance due to the use ofVAE impermanent training choices that encourage local PCA-like behavior on similar data. The paper also discusses the trade-off between local orthogonality and data construction, and proposes a simple yet effective VAE ensemble framework consisting of multiple VAEs. The framework allows each model to learn its own unique entanglement representation, and the disentangled representations are similar up to a signed permutation transformation. This paper provides an overview of the state-of-the-art in VAE-based disentanglement and suggests new directions for future research.\tThe authors introduce a novel VAE-based approach for unsupervised learning of disentangled representations of image data.  The approach trains an ensemble of VAEs along with pair-wise linear transformations between their latent spaces.  The objective includes the ELBO objectives for each VAE as well as two additional pressures:  (i) An L2 similarity objective that pressures samples from each VAE latent space to match under linear transformations samples from the other VAE latent spaces, and (ii) A cross-model decoding objective that encourages decoding accuracy of the linearly transformed latent samples.  The authors provide a theoretical argument that the linear transformations should learn to be orthogonal, and show some experimental results indicating that their model performs well compared to baselines when evaluated with an established disentangling metric.""12\tThis paper discusses the success of using Variational Autoencoder (VAE) frameworks for unsupervised disentanglement. The paper argues that the success of these frameworks is due to the choice ofVAEimplications that encourage a PCA-like behavior locally on data similarities. The paper also discusses the trade-off between local orthogonality and data construction, which can lead to models with the same architecture and hyperparameter setting learning entangled representations. The paper proposes a simple yet effectiveVAE ensemble framework that consists of multipleVAEs. The framework assumes that entangled representations are unique and similar up to a signed permutation transformation. Each model in the ensemble is trained using its own unique set ofVAEimplications. The paper shows that the proposed framework can effectively address the challenge of learning entangled representations while also achieving state-of-the-art performance on the disentanglement task.\tThis submission proposes an ensemble framework to improve learning disentangled representations with Variational Autoencoders (VAEs). The approach builds on the assumption that entangled latent representations learned by VAEs show some \u201cuniqueness\u201d in their latent space structure, while disentangled representations exhibit some \u201csimilarity\u201d; an assumption corroborated by recent studies. On that basis, a VAE ensemble approach is proposed where several VAEs are connected through linear mappings between the individual latent spaces to encourage alignment of latent representations and thus disentanglement. A formal derivation of the framework is provided and the formal validity of the underlying assumption demonstrated. Furthermore, empirical evaluation of the proposed approach in comparison to the standard VAE, beta-VAE and FactorVAE on the datasets dSprites (main results, main text) and CelebA (appendix) is performed, yielding improved results on the FactorVAE disentanglement metric (all baseline methods considered) as well as the Distance to Orthogonality (DtO) metric (only standard VAE considered).""13\tThis paper discusses the development of automated machine learning (AutoML) solutions that use human expertise to learn which datasets are similar and what algorithms perform best. The paper\u4ecb\u7ecd\u4e86 a Transformer-based language model and feature extractor, which are used to represent the data and allow for fast and efficient training of AutoML systems. The paper also discusses the challenges of developing fast and efficient algorithms for machine learning and the use of human expertise in AutoML to overcome these challenges. The paper concludes by discussing the potential of AutoML to enable widespread use of machine learning by non-experts.\tThe paper proposes an efficient way to automatically choose the best or most suitable pipeline for different datasets. The proposed method can accelerate the AutoML using a pre-trained meta module. In particular, the AutoML job of a new supervised learning task can be accomplished without model evaluations, namely zero-shot / real-time AutoML. The meta module is constructed as a graph structure in which each node represents a dataset used for meta-training. ""14\tThe paper discusses the challenges of developing fast and efficient algorithms for machine learning and how automated machine learning (AutoML) can help overcome these challenges. The paper\u4ecb\u7ecd\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u4f7f\u7528Transformer-based language model\u548cfeature extractor\u6765\u81ea\u52a8\u5904\u7406\u6587\u672c\u63cf\u8ff0\u548c\u6570\u636e\u8868\u793a\uff0c\u4ee5\u53ca\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u8bad\u7ec3AutoML\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5 paper \u8fd8\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528\u4eba\u7c7b expertise \u6765\u6307\u5bfc AutoML \u7cfb\u7edf\u5b66\u4e60\u76f8\u4f3c\u7684datasets\u548c\u6700\u4f73\u7b97\u6cd5\u3002\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u6280\u672f\uff0c\u8be5 paper \u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u52a0\u901f\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5e76\u63d0\u9ad8\u51c6\u786e\u7387\u3002\tThe problem that the authors attempt to solve is to determine what ML pipeline will perform best on any new dataset, without incurring in the extra cost of actually running a large number of such pipelines, as is typically done in AutoML algorithms. The way this paper tackles the problem is to train a neural network that given a new dataset as input, will output a pipeline that is predicted to perform well on that dataset. This neural network is trained on other datasets, for which high performing pipelines are already known. Predicting a pipeline for a new dataset thus only require a forward pass through their NN.""15\tThe paper discusses the use of automated machine learning (AutoML) to automate the process of designing and training\u673a\u5668\u5b66\u4e60\u7b97\u6cd5 for new supervised learning tasks. The author presents a system that uses a transformer-based language model and a feature extractor to process text descriptions of datasets and algorithms, and allows for large-scale data representation for training the AutoML model. The system is designed to exploit human expertise to learn which datasets are similar and what algorithms perform best. The paper discusses the challenges of developing fast and efficient algorithms for Machine Learning, and presents an automated solution to address these challenges.\tThis paper presents a very interesting idea of utilizing the documentation for the data and the operators in the pipeline to generate meta-features for meta-learning. This is a very novel application of graph neural networks GNNs and language models for AutoML meta-learning. This view of meta-learning takes a very intuitive on a very high level. The use of the outputs of existing AutoML systems (such as auto-sklearn, TPOT, etc) is also very intuitive and well motivated. All these intuitive ideas are put together into a novel AutoML recommendation architecture making use of modern deep learning components.""16\tThis paper investigates the relationship between compositional learning and gradient descent in neural network optimization. The authors find that gradient descent imposing a bias toward non-compositional solutions, which\u8fdd\u53cdCompositionality property. This property states that the input to a composition is independent of the input to the next layer. The study suggests that only considering model architecture design in compositionality learning is unlikely to achieve complete compositionality. The authors hope that this study provides novel insights into compositional generalization and forms a basis for new research directions to equip machine learning models with such skills for human-level intelligence. The source code is included in supplementary material.\tthe paper investigates what neural networks learn when trained with gradient descent, in case parts of the inputs are only partially relevant to the output. The main claim is that GD is what prevents compositionality. In a set of synthetic experiments it is shown that indeed GD learns to use all information in the input, which results in poor generalization ood when only a subset of it was relevant.""17\tThis paper discusses the issue of how gradient descent can make compositionality learning difficult during neural network optimization. The paper argues that gradient descent tries to use all available and redundancy information from the input, which\u8fdd\u53cdCompositionality's conditional independence property, resulting in non-compositional solutions. The paper suggests that only considering model architecture design is not enough to achieve complete compositionality. The study provides novel insights into compositional generalization and forms a basis for new research directions to equip machine learning models with such skills for human-level intelligence. The source code is included in the supplementary material.\tThis work analyzes the effect of gradient descent training on the compositionality of the learned model. It is shown that the gradient descent would use all the available information, even when it is redundant to learn the mapping from input to the output. It is then argued that the gradient descent training has the bias against compostionality despite the model architecture. Experiments are conducted on three simple benchmarks to demonstrate that when gradient descent trained model would use redundant information and not generalize compositionally. ""18\tThis paper discusses the issue of how gradient descent can make compositionality learning difficult during neural network optimization. The paper finds that gradient descent is causing a bias towards non-compositional solutions, which is caused by trying to use all available and redundant information from the input, which\u8fdd\u53cd\u4e86Compositionality\u7684 conditional independence property. The paper suggests that only considering model architecture design is not enough to achieve complete compositionality. This is the first work to investigate the relation between compositional learning and gradient descent. The paper also includes source code in the supplementary material.\tThis paper addresses the effects of gradient descent methods onto compositionality and compositional generalization of models. The authors claim that the optimization process imposes the models to deviate compositionality, which is defined with conditional independence among random variables of input, predicted output and the ground-truth. Since compositionality is one of important features of human intelligence, it has been interested widely in the field of AI/ML such as vision, language, neuro-symbolic approaches, common sense reasoning, disentangled representation, and the emergence conditions of compositionality. As it has been not much focused on the relationship with optimizers, it is fresh and interesting. However, it is not easy to figure out the position of this paper from two reasons: (1) the definitions on compositionality in this paper are not so compatible with recent related works, which mostly consider certain structures in models [ICLR19, JAIR20] or representative problems such as visual reasoning [CVPR17] and Raven progressive matrices [PNAS17]. (2) The authors do not consider quantitative approaches such as compositionality [ICLR19] or compositional generalization [ICLR20]. ""19\tThis paper discusses the recent rise in interest in using knowledge graph (KG) representation learning for entity alignment. Embeddingbased methods, which do not rely on the quality of machine translation or feature extraction, are considered to be more robust for highly-heterogeneous and cross-lingual entity alignment scenarios. Despite the significant improvement in performance, there is little understanding of how the KG representation learning methods work. This paper defines a typical paradigm for how these methods work and analyzes how the representation discrepancy between two potentially-aligned entities are implicitly bounded by a predefined margin in the scoring function for embedding learning.\tThe paper proposes NeoEA, an approach that further constrains KG embedding with ontology knowledge. The paper first tries to summarize the existing embedding-based entity alignment methods, stating that most of the methods choose TransE as scoring functions. But their embedding features are not aligned well compared to the neural-based or composition-based loss function. The paper, therefore, solves this problem by developing a new NeoEA architecture which shows that adding a KG-invariant ontology knowledge can minimize such difference. The experiment shows the new constraints can improve state-of-the-art baselines.""20\tThis paper discusses the recent trend of using knowledge graph (KG) representation learning for entity alignment, which is considered to be more robust for cross-lingual and highly-heterogeneous entity alignment scenarios. The paper also examines the existing methods for KG representation learning and their limitations. The paper defines a typical paradigm for entity alignment methods, and analyzes how the representation discrepancy between two potentially-aligned entities are implicitly bounded by a predefined margin in the scoring function for embedding learning. The paper also suggests possible improvements to the existing methods.\tEntity alignment plays an important role in improving the quality of cross-lingual knowledge graphs. As one of the most important solutions, embedding-based methods aim at learning a semantic space where the unique entity cross knowledge graphs can have the closest distance. Most of research focus on entity-level granular, but discard the whole picture of embedding space of cross-lingual KGs. Besides the aligned entity pairs as the labelled data, this paper extended the labelled data with the conditional neural and basic axioms, which are actually sets of randomly selected entities or entities with the same relation type. Then the final objective is to align the cross-lingual knowledge graphs by both optimizing the distance of labelled entity pairs and neural axioms.""21\tThis paper discusses the recent development in Knowledge Graph (KG) representation learning for entity alignment, which is a powerful approach for handling highly heterogeneous and cross-lingual entity alignment scenarios. The paper highlights the benefits of using embedding-based methods, which do not rely on the quality of machine translation or feature extraction, and how they work by using a small number of pre-aligned entities as anchors to connect the embedding spaces of two KGs. The paper also analyzes the rationality of the foundation that pre-aligns entities to make the embedding learning process more efficient.\tIn the paper, the authors propose to minimize the discrepancy between pairs of (conditional) neural axioms to align the embedding spaces of different KGs. This method is justified by the authors' study of all kinds of OWL2 properties. The author also studied the influence of margin $\\lambda$ on less constrained/long-tail entities. The authors conducted experiments by adding the proposed model on top of the best models for entity alignment. The results are mixed, but the proposed model improves the SEA and RDGCN consistently. ""22\tThe paper discusses the increasing demand for intelligent IoT devices, such as wearables and biology devices, and the challenges that arise when applying CNN-powered devices to these applications. The paper also discusses the use of lensless imaging systems, which can be used to overcome some of the form factor requirements for existing lens-based imaging systems. The paper also discusses the hardware costs associated with powerful CNNs and the limited resources available for IoT devices. Finally, the paper\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848 for these challenges and suggests that future research should focus on developing more efficient and cost-effective CNNs and lensless imaging systems.\tThe paper proposed to adopt differentiable network architecture search (DARTS) for the co-design of the sensor (a lensless camera) and the deep model for visual recognition tasks, so as to maximize the accuracy and minimize the energy consumption. The key idea is to include the sensor configuration, in this case the phase mask of a lensless camera modeled as 2D convolutions, as additional parameters in architecture search.  The proposed method was evaluated on simulated data for a number of vision tasks (image classification, face recognition and head pose estimation), as well as using fabricated masks on a real world camera. The results demonstrated significantly increase recognition performance given the same energy level.  ""23\tThe paper provides an overview of the current state of the art in the field of intelligent IoT devices and the challenges that remain in order to widely apply them in various applications. The paper highlights the need for CNNs to be powerful and efficient in order to process large amounts of data quickly, but also notes that such devices require significant hardware resources. Additionally, the paper discusses the challenges of providing reliable and secure communication with such devices, which can be addressed through the use of secure protocols like HTTPS. The paper also provides an overview of the potential applications of CNN-powered IoT devices in medicine and biology, as well as the challenges that arise when trying to apply these devices in such settings.\tThis paper presents a method called SACoD to develop a more efficient CNN-powered Phlatcam. The proposed method optimizes both the PhlatCam sensor and the backend CNN model simultaneously.  That is, the coded mask in Phlatcam and neural network weights are regarded as learnable parameters. The coded mask (the optical layer) can be considered as a special convolution layer. As a result, it achieves energy saving, model compressing as well as good accuracy. Extensive experiments and ablation studies are presented to show the effectiveness of the method.""24\tThis paper discusses the current challenges in the development of intelligent IoT devices, specifically wearables and biology devices such as swallow endoscopes. The paper highlights the need for thick and form factor-\u4e25\u5389\u7684 imaging systems, which are often difficult to implement with existing lens-based systems. To overcome this challenge, the paper suggests using lensless imaging systems, such as Phlat Cam, which encode the incoming light instead of focusing it. This allows for more flexible and compact devices. Additionally, the paper discusses the need for significant hardware costs for powerful CNNs, which is a\u9650\u5236 IoT devices resources. To overcome this challenge, the paper suggests using more efficient CNN architectures and using off-the-shelf hardware.\tSACoD presents a novel attempt to integrate the computational capabilities of a lensless imaging system, PhlatCam, with the search for the optimal convolutional neural network design for a given task. SACoD provides a framework which enables joint optimization of sensor and CNN resulting in IoT devices that achieve higher task accuracy\u2019s with limited resource budgets of a typical IoT system. The authors present a new an optical layer design that enables above described features. Detailed experiments comparing SACoD sensor + CNN with other baseline models covering past papers, demonstrate the superiority of SACoD\u2019s accuracy/efficiency curve over that of separately optimizing CNN arch or sensor/CNN joint-optimizations that do not vary network architecture. Additionally, ablation studies and results from measurements from actual phase masks fabricated help breakdown the accuracy/efficiency benefits of SACoD while analyzing the noise limitations of mask fabrication process.""25\tThis paper provides an overview of the importance of animal behavior in organizing large groups and the challenges of understanding high-dimensional data from animal societies. The paper discusses the use of semantic embeddings to understand animal behavior, the scale and complexity of the datasets used to track individual interactions in animal societies, and the challenges of understanding these data. The paper also highlights the potential of using animal behavior to inform policy and decision-making.\tThe authors introduce a novel method for non-negative matrix factorization for timeseries and apply it to longitudinal honey bee interaction data.  The model leverages consistency of individuals over time by forcing the factors (or rather, the residuals of the factors with respect to a global trajectory) to be linear combinations of a small set of temporal basis functions.  These temporal basis functions are functions of the bee\u2019s age.  In other words, the factor embedding of each bee is a vector of linear combinations of 16 learned basis functions of time.  Since all bees use the same 16 temporal basis functions, given these basis functions the lifetime embedding of each bee is encapsulated by a small matrix of numbers, namely the coefficients for the temporal basis functions for each factor (and in practice only two factors were significant, so each bee\u2019s life is embedded in 32-dimensional space).  There are a number of regularizations on the temporal basis functions and the embedding coefficients.""26\tThis paper provides an overview of the emerging field of animal behavior analysis, specifically focused on the use of semantic embeddings to understand high-dimensional data from animal societies. The paper highlights the challenges and opportunities of analyzing animal behavior data, and discusses the current state-of-the-art in this field. The paper also discusses the potential applications of this technology in the field of animal care and management, as well as in other areas of science and medicine.\tThe authors present a matrix factorization model to jointly characterize the lifetime interactions of thousands of bees over generations. The problem is fascinating as both a technical and scientific question and the modeling framework appears novel. Although not directly addressed, the authors appear to be trying to solve a *tensor* factorization problem, not just the special case of a matrix (which is of course a 2-d tensor). It would have been interesting to see results comparing their method with a non-negative variant of, say, PARAFAC/CANDECOMP or generalizations thereof. It would have at least have been appropriate to explain why or why not existing tensor methods are not appropriate.""27\tThis paper provides an overview of the emerging field of animal behavior data analysis, which is the use of computer science and mathematics to analyze and understand large-scale data from animal societies. The paper discusses the benefits of using data analysis techniques to study animal behavior, including the ability to understand complex social interactions and the role of individuals in the group. The paper also highlights the challenges of analyzing animal behavior data, including the high-dimensionality of the data and the need for a deep understanding of the animal behavior. Finally, the paper provides a summary of the key findings and future directions for the field of animal behavior data analysis.\tThis paper proposes a NMF formulation ||A-FF^T||^2 where A and F are different types of information extracted from social datasets. In the honeybee example the authors highlight, A represents the spatial relationship between bees, and F encodes the age of the bees. The authors setup F to be decomposable into two types of embeddings, one which characterizes the group activity and the other which characterizes the individual activity. ""28\tThis paper discusses the use of compressed sensing in magnetic resonance imaging (MRI), where the acquisition time is reduced by subsampling the measurements, but the signal is not uniquely identifiable from the measurements due to the fewer equations than unknowns. It highlights the history of the problem, the classical compressed sensing methods, and the recent use of deep learning techniques to enforce more nuanced forms of sparsity.\tReview: This paper proposes data augmentation methods for medical imaging(especially for accelerated MRI) based on the MR physics. The augmentation includes both pixel preserving augmentations/general affine augmentations on both real and imaginary values in the image domain. Then, the augmented images are transformed to k-space domain and the k-space data are down-sampled for the input data generation for the accelerated MRI task. They claim that how to schedule p(the probability of applying combinations of augmentation) over the training is important and the schedules from p=0 and increasing over epochs shows best results, experimentally.""29\tThis paper discusses the use of compressed sensing in magnetic resonance imaging (MRI) to overcome the problem of insufficient measurements to uniquely identify the signal. The paper defines the problem of limited measurements in MRI and\u4ecb\u7ecd the classical compressed sensing methods for reducing the number of measurements. The paper also discusses the recent use of deep learning techniques to enforce more nuanced forms of sparsity in compressed sensing. The paper ends by highlighting the potential applications of compressed sensing in MRI and suggests further research directions.\tIn this paper, the authors design a data-augmentation pipeline for the domain of MRI reconstruction (specifically, by proposing sensible guidelines for augmenting k-space data when learning image reconstructions, to preserve the noise characteristics of the image data). They show that this pipeline works as you might expect data augmentation to work: it boosts results for small training sets and becomes increasingly less effective as the training set grows. However, while the problem domain is of interest, there are issues with the presented work.  ""30\tThis paper provides an overview of the recent activities in the field of compressed sensing in magnetic resonance imaging (MRI). The paper discusses the challenges of using subsampling in MRI and how compressed sensing can overcome these challenges by utilizing prior knowledge about the signal. The paper also discusses classical compressed sensing methods and deep learning techniques that have been used to enforce more nuanced forms of prior knowledge in MRI reconstruction.\tThis paper presents a method to use data augmentation to improve accelerated MRI reconstruction when the amount of training data is limited. This is an important problem since MRI data is expensive to obtain. Traditional image augmentation methods can't be applied directly for this problem because MR images are complex valued. Further, the applied transformations need to preserve the noise distribution, without which model performance degrades significantly.""31\tThis paper proposes a new algorithm for effective order learning, called the deep repulsive clustering (DRC) algorithm. The DRC algorithm works on ordered data and uses a novel order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. The ORID network then groups object instances into clusters according to their identity features using a repulsive term. The paper also presents experimental results on facial age estimation, aesthetic score regression, and historical color image classification, which demonstrate that the DRC algorithm can cluster ordered data effectively and also yield excellent rank estimation performance.\tThe novelty of the network structure is marginal. The decomposition way of feature is very common in computer vision. Just utilizing the latent vector of the encoder with only the comparator loss to decompose the feature into two feature types is limited. The authors should show the visual differences between these two feature types. The expression of the article is very clear, but some basic theories need not be explained in detail (Such in Section 3.4)""32\tThis paper proposes a new algorithm for clustering and rank estimation of ordered data, called the deep repulsive clustering (DRC) algorithm. The DRC algorithm uses a novel order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature, which can be used to group object instances into clusters. The ORID network also allows for the estimation of the rank of a test instance by comparing it with references within the same cluster. The DRC algorithm was applied to three different data sets: facial age estimation, aesthetic score regression, and historical color image classification, and the experimental results show that the algorithm can effectively cluster ordered data and also yield excellent rank estimation performance.\tThis paper considers the problem of order learning, which learns an ordinal classification function. This paper proposes to learn separarted order-relavent and order-irrelavent latent representations to improve the performance of existing methods, which is a very interesting and promising idea. However, the approach lacks novelty and convincing theoretical guarantees, as well as not showing convincing performance even through the insufficient empirical evaluation.""33\tThis paper proposes a new algorithm for effective order learning in data that is organized in a meaningful way by its order. The algorithm is called the deep repulsive clustering (DRC) algorithm and it uses a novel network architecture called the order-identity decomposition (ORID) network to split the information of an object instance into an order-related feature and an identity feature. The ORID network then groups object instances into clusters based on their identity features using a repulsive term. The paper presents experimental results on a variety of tasks, including facial age estimation, aesthetic score regression, and historical color image classification, which show that the DRC algorithm can effectively cluster ordered data and also achieve excellent rank estimation performance.\t- It is well presented. The idea of splitting the encoding feature space into task related features and non-task related features is probably not new. But the use of it in estimating rank might be new and intuitively it makes sense to use it. They also propose an extension to the clustering algorithm using a repulsive term and propose MAP estimation algorithm to assign a rank based on the output probabilities of the comparator when the max possible rank is known.""34\tThis paper discusses the challenges and recent advances in deep reinforcement learning (RL) for sparse reward environments. RL algorithms often require tremendous number of samples to achieve reasonable performance, which becomes more pronounced in sparse reward environments. One of the most popular techniques to address this issue is to use intrinsic rewards to encourage exploration. This paper also investigates and demonstrates the effectiveness of various exploration methods, including early\u505c\u6b62\uff0c deep exploration, and exploration-based learning. The paper also discusses the limitations and future directions of this technology.\tThis paper presents RAPID, an exploration algorithm for procedurally generated environments. The paper introduces an exploration scores composed of a local and global score. The local score is computed per-episode, it is the fraction of distinct states visited during an episode, the global score keeps track of the exploratory effort of the agent over the whole training procedure.""35\tThis paper discusses the challenges and opportunities in deep reinforcement learning (RL) for sparse reward environments. The paper first defines RL and the need for efficient exploration in such environments. It then presents several methods for encouraging exploration, including intrinsic rewards, state-based exploration, and exploration-exploitation. The paper also discusses the limitations and challenges of these methods, such as the need for efficient\u8ba1\u7b97 and the need for a well-defined environment. Finally, the paper\u603b\u7ed3\u51faS the research direction and future work needed to address these challenges.\tThis paper presents an exploration method for procedurally-generated environments, RAPID, which imitates the past episodes that have a good exploration behavior. First, authors introduce exploration scores, local score for per-episode view of the exploration behavior, and global score for long-term and historical view of exploration. The authors use the weighted sum of these two exploration scores and extrinsic reward as a final episodic exploration score. They rank the state-action pairs based on episodic exploration score and train the agent to imitate behaviors with high score. In experiments, they show the results by comparing state-of-the-art algorithms in several procedurally-generated environments.""36\tThis paper discusses the challenges and techniques for efficient exploration in deep reinforcement learning (RL) in sparse reward environments. The paper\u5148\u4ecb\u7ecd\u4e86Deep Reinforcement Learning(DL)\u5728\u5404\u4e2a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u4e86DL\u7b97\u6cd5\u5728 samples  efficiency\u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\u3002\u7136\u540e\u63d0\u51fa\u4e86\u51e0\u79cd\u6709\u6548\u7684 exploration\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528 intrinsic rewards \u6765\u9f13\u52b1 exploration\u3001\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u7b49\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u8be5 paper \u8fd8\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u4ee5\u53ca\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002\tThis paper tackles the problem of improving exploration in deep RL for procedurally-generated environments, where state-of-the-art exploration techniques typically fail. In the proposed approach, called RAPID, each agent-generated episode is evaluated with respect to its local exploration score (for the given episode), global exploration score (across all previous episodes), and extrinsic reward obtained. Episodes with high scores are stored in a replay buffer, and a policy is trained via behavioral cloning on batches of state-action pairs from this buffer. This policy is also used to produce the agent-generated episodes.""37\tThis paper presents an overview of zeroshot learning, a machine learning technique that aims to improve the generalization performance of a model to new tasks and classes, where there is no training data available. The main goal of zeroshot learning is to learn a mapping between input space and unseen classes, without using any prior knowledge or hand-designed features. The paper provides an overview of the history and current state of zeroshot learning, the challenges and limitations of the technique, and the recent works that have addressed these issues. It also discusses the potential applications of zeroshot learning in various fields, such as image classification, natural language processing, and reinforcement learning.\tThis paper focuses on improving zero-shot classification by reducing the bias of the classifier towards seen classes. The bias occurs since the embedding is trained with visual examples from the seen classes, while using only the attribute information from unseen classes for testing. Authors propose an isometric propagation network that build a graph in both visual and semantic space, performs some steps of propagation, and then uses the updated prototypes for training a classifier. They use attention to construct the graph and also use attention to regularize the graph edges between the two spaces to be isometric. Authors also propose to use an episodic training method to improve learning. ""38\tThis paper discusses the topic of zeroshot learning, a type of machine learning method that aims to generalize to new tasks and classes without having access to any training data. The paper highlights the challenges and opportunities that arise when trying to apply zeroshot learning to image classification, where the goal is to classify new images from unseen classes. The paper discusses the benefits and limitations of zeroshot learning, and presents recent research in the field.\tIn this paper Zero Shot Classification is studied using prototypes. Each class is represented with a visual and semantic prototype, and at test time compared to a visual example + prototype for a(n unseen) test class. The most similar test class is chosen. In this work a novel method is proposed to construct the prototypes, which are trained in an episode learning setting. On various benchmarks the proposed method performs better than existing method for the generalized zero-shot classification task (seen + unseen) test classes.""39\tThis paper discusses the problem of improving the generalization capabilities of machine learning models to unseen problems and the recent advances in zeroshot learning. The paper begins by introducing the main challenges in machine learning, including the need to improve the generalization capabilities of models to unseen data and the lack of training data. It then discusses the traditional approaches to zeroshot learning, such as learning to map between semantic space and visual space, and recent advances in this field, such as learning to classify with no training data. The paper ends by discussing the potential applications of zeroshot learning in various fields.\tThe authors propose a novel computational pipeline to tackle a well-known problem in zero-shot learning: although multiple visual instances are available for the classes and categories to be recognized, one and only one semantic embedding is available to describe the classes/categories while using side information like attributes or relevant textual information. To cope with that problem, authors learn visual and semantic prototypes which are then adopted to perform gradient descent over a graph in which the topological relationship among similar/dissimilar classes are preserved. In the experimental validation, the proposed method shows its superiority among a number of prior methods in zero-shot learning, including discriminative and generative methods. ""40\tThis paper provides an overview of the current state of multi-task learning and its challenges in handling multiple tasks. It discusses the advantages of learning a single model that performs well across multiple targeted tasks, including significant parameter savings and the ability to eliminate the need for maintain multiple models in production. However, achieving state-of-the-art performance on natural language understanding benchmarks still relies on fine-tuning a new model for every single task, which is infeasible in many situations. The single-task fine-tuning paradigm is well-established as the dominant approach for handling multiple tasks, with training multiple tasks using a single model being the most effective.\tThis paper presents a HyperGrid Transformer approach to fine-tuning, where one takes a pre-trained transformer model and then modifies it by introducing hypernetworks that modify the 2nd FFN in each transformer block by generating additional weights conditioned on input. These hyper-networks are trained on all tasks in GLUE/SuperGLUE datasets simultaneously and are task aware through prefixing of a task specific token to input. This allows one to fine-tune only a small number of parameters and end up with a model that performs quite well on all tasks at the same time, not much worse than fine-tuning the entire transformer model on all of these tasks.""41\tThis paper discusses the challenges and opportunities of learning a single multi-task model that performs well across multiple targeted tasks. It explains the importance of having a\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u4e2a\u4efb\u52a1\uff0c saving parameter costs and eliminating the need for maintaining multiple models in production. However, achieving state-of-the-art performance on natural language understanding benchmarks still relies on fine-tuning a new model for every single task, which is infeasible in many situations. The single-task fine-tuning paradigm is the dominant approach for training multiple tasks using a single model, and it is well-established as the most efficient and effective way to achieve state-of-the-art performance on multiple tasks.\tThe authors propose HyperGrid Transformers with a decomposable hypernet-work that learns grid-wise projections to specialize regions in weight matrices for different tasks. Usually, people would use different models to solve different tasks respectively. In this paper, the authors focus on using a single model to solve all tasks and it will save a lot of model parameters for natural language understanding. And the authors have done comprehensive experiments on GLUE and SuperGLUE, and prove that the proposed single model can achieve much better performance than baseline and competitive performance with multiple task-specific models.""42\tThis paper discusses the challenges and opportunities of training a single multi-task model that performs well across multiple targeted tasks. It highlights the significant cost savings that can be achieved by using this paradigm, as well as the importance of considering the performance of different models on different tasks when developing a language model. The paper also discusses the limitations of the current state-of-the-art in natural language understanding, including the challenges of training models on large amounts of text and the need for ensemble models and task-specific fine-tuning. The paper\u6700\u540e presents a summary of the main findings and suggests future directions for studying multi-task learning.\tThis manuscript presents a HyperGrid Transformer, which is engaged in learning a single model to account for multi-tasks in NLP. The core idea of HyperGrid Transformer is to learn task-conditional dynamic weights in a grid-wise manner in the feed-forward layers, where the weights are factorized in local and global components. This idea is simple, materializing the goal of reducing the parameter cost for the used multi-task network. However, the conducted experiments look nice, showing promising performance on GLUE/SuperGLUE. Therefore, from my point of view, this work is worthy of a publication at ICLR. ""43\tThis paper discusses the challenges that autonomous driving vehicles face when driving in different lighting, weather, and visibility conditions, as well as how to address these challenges by analyzing the sensitivity of the learning algorithm to varying quality of the input data. The paper proposes an algorithm to improve the overall performance of the task of learning to steer, which can enhance the learning outcomes up to 48%. A comparative study between the approach and other related techniques is also presented, which confirm the effectiveness of the algorithm as a way to address these challenges.\tThis paper proposed a novel adaptive data augmentation algorithm that produces random perturbations on the training dataset to train an imitation learning-based self-driving network. It starts with a sensitivity analysis of network performance under different types and levels of perturbations. And a novel automated perturbed training dataset selection mechanism is then proposed to improve the performance. Validation has been conducted over simulated data with both seen and unseen perturbation types. ""44\tThis paper discusses the challenges associated with autonomous driving, including the need to be able to drive in different lighting, weather, and visibility conditions in different environments. It also mentions the importance of analyzing the sensitivity of the learning algorithm to varying quality of the image input for autonomous driving. The paper proposes an algorithm to improve the overall performance of the task of learning to steer, which can enhance the learning outcomes up to 48%. The study compared the effectiveness of the proposed approach with other related techniques and found it to be effective.\tThis work proposes a new method to improve the generalization of ML models for the task of vehicle steering using a hybrid of data augmentation and adversarial examples. In a nutshell, the proposed method attempts to increase the accuracy of the model by dynamically adding a selection of candidate datasets during training. Each of these \u201ccandidates\u201d is created offline applying a transform (e.g. blur, distortion, and changes in color representation) to the original (base) dataset. During training, the method chooses among the K transformed-datasets those who minimize the mean validation accuracy and based on this selection the steering model is retrained. The approach is evaluated on a driving dataset.""45\tThis paper discusses the challenges associated with autonomous driving, specifically the need to be able to drive in different lighting, weather, and visibility conditions in different environments. To address this issue, the paper proposes an algorithm that analyzes the sensitivity of the learning algorithm to varying quality of the image input, and uses the results to improve the overall performance of the task of learning to steer. The paper also discusses related techniques such as data augmentation and adversarial training, and demonstrates the effectiveness of our approach in comparison to these techniques. The results of the study show that our approach is able to enhance the learning outcomes up to 48%.\tThis paper presents an algorithm to improve the model generalization of the task of \"learning to steer\". First, the sensitivity of a baseline learning algorithm to degraded images in varying qualities caused by different factors is carried out. Some empirical insights are gained. Then, a new training algorithm is proposed to solve a min-max optimization problem, where the most difficult datasets are chosen and used for training at each iteration. Experiments are conducted to validate the effectiveness of the proposed method. ""46\tThis paper discusses the use of neural networks for constrained optimization and how they can be used to overcome some of the challenges of traditional optimization methods. The paper also\u4ecb\u7ecd\u4e86 Deep Constraint Completion and Correction (DC3) algorithm, which uses neural networks to satisfy constraints in a faster and more efficient way than traditional optimization methods. The paper also discusses the potential applications of DC3 in various domains such as power systems, weather and climate models, and materials science.\t+ The paper proposes a general framework to deal with constraints in optimization problems using neural networks. In my opinion this is an important problem since there exists no standard method in many existing deep neural network frameworks to deal with constraints, which are also inapplicable even if the constraints are only slightly nontrivial. The paper proposes to deal with equality and inequality constraints differently which may be often easier in large scale settings.""47\tThe paper discusses the use of neural networks for constrained optimization and the challenges that arise when trying to satisfy hard constraints at test time. It\u6307\u51fa traditional approaches to optimization are often expensive to run and function approximators are ideal for solving large problems, but deep learning has struggled to perform well in domains where strict feasibility criteria must be satisfied. The paper proposes a new approach called Deep Constraint Completion and Correction (DC3), which can operate in settings where traditional optimizers are slow and where strict feasibility criteria must be satisfied, using fast neural network approximators. The paper provides an overview of the problem and the proposed solution and discusses the potential benefits and limitations of the approach.\tThere has been an increase of works using deep neural networks to heuristically predict solutions to constrained optimization problems. However, these methods cannot generalize to arbitrary constraints.  In this paper, the authors propose a method to build neural networks that output vectors that satisfy hard equality and inequality constraints. They do this by first having the network predict the underdetermined part of the system defined by the equalities, then doing a series of gradient steps to project the solution onto the space delineated by the inequalities. They evaluate on synthetic quadratic programs and problems derived from a AC power flow application.""48\tThis paper discusses the use of deep learning for constrained optimization, specifically highlighting the challenges that arise when trying to solve problems where strict feasibility criteria must be satisfied. The paper proposes a new method, Deep Constraint Completion and Correction (DC3), which uses a deep neural network to approximate the function that\u6ee1\u8db3 constraints. The paper argues that this method is able to perform well in settings where traditional optimizers are slow and where strict feasibility criteria must be satisfied.\tThis paper proposes a method to strictly enforce hard constraints during a neural network, without compromising differentiability. The method has two stages 1) From a smaller set of predicted variables, compute the remaining ones so that equality constraints are satisfied; 2) Take a few gradient steps (w.r.t soft constraint) in case inequality constraints are violated. They perform experiments on synthetic and also somewhat applied instances of quadratic programs. The results look very promising.""49\tThis paper discusses the issue of how to select the most important weights in a deep neural network for pruning, a challenge that has been faced by developers of these networks in recent years. The paper argues that importance-based approaches, which score the weights based on their effect on the output of the network, are more effective at selecting the most relevant weights for pruning. Regularization-based approaches, which use regularization techniques to prevent overfitting, are also considered, but are often more complex to implement and may not provide as good a performance. The paper provides a summary of the main findings and discusses the potential implications of these approaches for neural network design.\tThe authors propose regularization-based pruning methods with the penalty factors uniformly increased over the training session. The first algorithm (GReg-1) sorts the filters by L1-norm and only applies the increasing regularization to the \u201cunimportant\u201d filters; the second one (GReg-2) applies the increasing regularization to all the filters. The experiments are very extensive and convincing to support the claimed contributions.""50\tThis paper discusses the issue of how to choose weights in a deep neural network to avoid overfitting, which has become a significant problem in recent years due to the effectiveness of deep neural networks. The paper discusses two approaches to this problem: importance-based and regularization-based. The importance-based approach focuses on calculating the importance of each weight based on its impact on the output of the network. The regularization-based approach involves adding an additional parameter, such as a weight decay or a neural network penalty, to the network to prevent overfitting. The paper presents a review of the existing literature on this issue and discusses the advantages and disadvantages of each approach. It also provides an overview of the recent advances in this field and the challenges that remain.\tThe paper proposes a new pruning scenario using regularization to better prune the network. The scenario has two-component, the first one proposes a new pruning schedule that does not directly remove the neurons that need to prune from the network. It removes the neurons by adding an L2 regularization and makes the neurons that need to remove gradually decrease to zero. The second one gives the importance score to the neurons. It uses the L2 regularization and studies how the coefficient \\lambda of the regularization term can influence the weight change to derive the neuron's importance in the neuron network. By perturbing the penalty term to the converged network, the algorithm can get the Hessian information to score the neurons but uses less time than calculating the Hessian. The paper also shows many empirical results on various benchmarks to show their advantages when using the new schedule and scoring criterion during the pruning process. The result shows that their method can get better at a fast speed.""51\tThis paper discusses the issue of how to choose weights in a deep neural network to reduce storage, memory footprint, and computational resources while still achieving good performance. The paper presents a number of different approaches to this problem, including importance-based and regularization-based methods. The importance-based methods focus on measuring the importance of each weight in the network, while the regularization-based methods use techniques like dropout or weight decay to encourage the network to have more balanced weights. The paper\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b0\u7684\u60f3\u6cd5\uff0c\u4f8b\u5982\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6765\u5ea6\u91cf\u6743\u91cd\u7684\u91cd\u8981\u6027\uff0c\u6216\u8005\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u6765\u6c42\u89e3\u6743\u91cd\u6700\u4f73\u7ec4\u5408\u3002 The paper also discusses the challenges and limitations of these methods, such as the issue of overfitting, and the need for additional data or additional regularization techniques. The paper concludes by suggesting further research in this area.\tThis paper explores how the basic L2 regularization can be exploited in a growing fashion for better deep network pruning. The authors proposed two algorithms in this work: (1) The first (called GReg-1) is a variant of the L1-norm based filter pruning method [Ref1]. The important/unimportant filters are decided by their L1-norms. Later the unimportant ones are forced to zero through the proposed rising penalty scheme. (2) The second algortihm (called Greg-2) imposes the rising L2 regularization on all the filters. It is theoretically shown in the paper that this makes the parameters to separate to different degrees according to their local curvatures (ie, Hessian values). The method takes advantage of this by driving the weights into two groups with stark magnitude difference and then prunes by the simple L1-norm criterion.  The two methods are demonstrated effective on CIFAR10/100 and ImageNet benchmarks in the comparison with many state-of-the-art methods. ""52\tThis paper discusses the recent interest in Model-Based Reinforcement Learning (MBRL) and its applications in data efficiency, zero- and few-shot learning, and strategic thinking. It also discusses the importance of planning in MBRL and the wide range of algorithms used to support it. The paper suggests that models will play a key role in generally intelligent artificial agents and argues against the idea that model-based aspects of human cognition prove the importance of models. It also notes that there is huge variance in the algorithmic choices made to support MBRL and highlights the need for further research in this area.\tThe paper investigates how and why planning might be beneficial in model-based reinforcement learning settings. To that end, the authors ask three questions on planning in MBRL: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? In order to answer these questions, the authors investigate the performance of MuZero in a variety of learning challenges while systematically ablating the algorithm to find how each part of the algorithm effects the overall performance.""53\tThis paper discusses the latest advances in Model-Based Reinforcement Learning (MBRL) and how it can lead to impressive gains in data efficiency, zero- and few-shot learning, and strategic thinking. MBRL methods combine planning and learning in a variety of ways, with planning referring to the process of using a learned or given model of the world to construct imagined future trajectories or plans. The paper also discusses some of the challenges and areas for future research in MBRL. The paper argues that models will play a key role in generally intelligent artificial agents, and that MBRL methods can provide evidence of this by their success in these areas.\tThis paper tries to disentangle the role of planning in model-based reinforcement learning with a number of different ablations and modifications to MuZero. Specifically, the authors analyze the overall contribution of planning by omitting planning from which it is originally used in MuZero, and investigate different planner settings that can drive performance. In addition, they check the generalization advantage of MBRL. Overall, the paper is well-written, and experiments are conducted appropriately. The results provide some insights that other researchers in the MBRL community can leverage for their future work. My major concern is the lack of direct ablation study that can clearly show the advantage of planning in providing a good learning signal. See the detailed comments below.""54\tThis paper discusses the field of Model-Based Reinforcement Learning (MBRL), which combines planning and learning to improve performance in various tasks. The paper discusses the recent advances in MBRL, including gains in data efficiency, zero- and few-shot learning, and strategic thinking. The paper also discusses the different ways in which MBRL methods combine planning and learning, such as using learned models to construct imagined future trajectory or plans. The paper also touches on the potential of models to play a key role in generally intelligent artificial agents.\tThis paper analyzes the role of planning in the model-based reinforcement learning agent, based on evaluating MuZero on eight tasks (i.e. Ms.Pacman, Hero, Minipacman, Sokoban, 9x9Go, Acrobot, Cheetah, and Humanoid), which have discrete action spaces. The conducted experiments show three major implications: (1) Of the three parts in which search is used (i.e. search at evaluation time, search at training time for exploration, and using search result as a policy target), the role of serving as a policy improvement target was most substantial. (2) Deep tree search did not make a significant contribution to performance, and a simple Monte-Carlo rollout could be performant enough for MBRL. Also, a too small or too large search budget can be harmful to the performance of the MBRL agent. (3) Search at evaluation time was helpful for zero-shot generalization especially when the model is accurate.""55\tThis paper explores the use of inductive biases in the policy function to enable implicit planning in reinforcement learning algorithms. It discusses the importance of planning in reinforcement learning and the limitations of explicit planning algorithms. Additionally, it presents several recent approaches to implicit planning, including value iteration networks (VINs) and deep learning-based planning. VINs use CNNs to understand the relationship between state values and transition probabilities in a grid-world environment, which inspired the use of a VIN module in deep planning algorithms. The paper also discusses the challenges and potential applications of implicit planning.\tThe authors propose a generalization of Value Iteration Networks to unknown, potentially continuous state spaces. They describe a framework for leveraging a learned graph embedding model (TransE) in combination with a deep RL model and an execution model based on graphical message passing to perform a VI-like operation. The authors show improved performance compared to baselines on a grid-world task with a known MDP, as well as several simple continuous control environments and the Atari game Freeway.""56\tThis paper discusses the recent research in implicit planning, also known as model-free planning, and its application in reinforcement learning. The paper presents an overview of the importance of planning in reinforcement learning and discusses the traditional approach of explicit planning algorithms. However, recent research has explored the use of inductive biases in the policy function to enable planning to emerge while training the policy in a model-free manner. An example of this approach is the value iteration networks (VINs), which observe that the value iteration (VI) algorithm can be understood as a convolution of state values and transition probabilities followed by max-pooling, which inspired the use of a CNN-based VI module. The paper also discusses the challenges and future directions of implicit planning and its application in reinforcement learning.\tThe paper tackles an open problem of the value-iteration-network-paradigm. The proposed method (XLVIN) has a conceptual edge over traditional value iteration networks in that it can be applied to continuous problems and problems where the state space is either too big or not fully known in advance. The experiments mostly succeed in making the case that XLVINs:""57\tThis paper discusses the recent research in implicit planning, also known as model-free planning, and the approaches that have been proposed to enable it.  implicit planning is a type of planning that involves planning without explicit models of the environment. This has been explored in the context of reinforcement learning, where policies are trained without using any explicit models of the environment. Some notable examples of this research include value iteration networks (VINs), which observe that the value iteration (VI) algorithm can be understood as a convolution of state values and transition probabilities followed by max-pooling, and Niu et al.'s work on policy iteration with inductive biases. This research has shown that by introducing inductive biases in the policy function, planning can emerge while training the policy in a model-free manner.\tThis paper proposed a novel policy prediction model that combines self-supervised contrastive learning, graph representation learning and neural algorithm execution to generalize the Value Iteration Networks to MDPs. The method described in the paper is a combination of existing works in the literature but seems to work well in practice. The experiments evaluate multiple aspects of the proposed model (E.g. number of executor layers, etc.) and show significant performance improvement over the existing approaches.""58\tThis paper discusses the problem of overparameterized neural networks and the inductive bias they exhibit towards desirable solutions. The training objective of these networks is non-convex and contains multiple global minima with different generalization properties, which makes it difficult to achieve good generalization performance. However, neural networks trained using gradient-based methods do show good test performance, suggesting an inductive bias towards desirable solutions. This inductive bias is one of the major open problems in machine learning, and there have been significant efforts to understand and overcome it. One line of work focuses on using the Neural Tangent Kernel (NTK) approximation of neural networks, which reduces to a convex optimization problem, to achieve better generalization performance. However, it has been shown that the NTK approach does not always lead to improved generalization performance.\tThis paper investigates the problem of learning monotone read-once DNF formulas using convex neural networks. Specifically, the authors explore the distribution-specific PAC setting, where training samples are drawn independently according to the uniform distributions and are labeled according to a target monotone read-once DNF. The main contribution of this study is essentially empirical: convex neural nets, trained with GD for minimizing the cumulative hinge loss, converge to global minima for which neural units coincide with the monomials of the target DNF. This remarkable stability is corroborated by theoretical insights about global minima.""59\tThis paper discusses the problem of overparameterized neural networks and the inductive bias they exhibit towards desirable solutions. The training objective of these networks is non-convex and contains multiple global minima with different generalization properties, which makes it difficult to achieve good generalization performance. However, neural networks trained using gradient-based methods do show good test performance, suggesting an inductive bias towards desirable solutions. This inductive bias is one of the major open problems in machine learning and has been a focus of significant efforts in recent years. One line of work considering the Neural Tangent Kernel ( NTK ) approximation of neural networks, which reduces to a convex optimization problem, has shown that the NTK is able to capture this inductive bias and achieve good performance on a specific task. However, it is still an open question as to how this bias affects the generalization performance of neural networks.\tIn this paper the aim in to understand the inductive bias of neural networks learning DNFs. The focus is in convex neural networks and gradient descent. It is shown that under a symmetric initialization, the global minimum that gradient descent converges to is similar to a DNF-recovery solution. Further, experimental evaluation demonstrates that gradient descent can recover read-once DNFs from data. ""60\tThis paper explores the problem of overparameterized neural networks and its impact on the generalization performance of trained models. The training objective of these networks is non-convex and contains multiple global minima with different generalization properties. Despite this, models trained using gradient-based methods often show good test performance, suggesting an inductive bias towards desirable solutions. This inductive bias is studied and a solution is proposed to address it. One line of work considers the Neural Tangent Kernel ( NTK ) approximation of neural networks, which reduces to a convex optimization problem. However, it has been shown that the NTK approximation does not always have the best generalization performance, and a better approach is needed. Efforts are made to understand the impact of different factors such as algorithm, architecture, and data on the generalization performance of trained models.\tThe paper considers learning Boolean functions represented by read-once DNFs by using neural networks. The neural network architecture consists of a hidden layer with 2^D components, which is rich enough to express any Boolean functions. Given a whole 2^D instances of some read-once DNF, the authors showed that (1) weights corresponds to the true DNF is the global minimum of the loss minimization problem with the network, (2) they empirically observe that gradient descent with a rounding heuristics finds the true DNF expression, and(3) the solution of a 2-norm minimization recovers the true DNF.""61\tThis paper\u63a2\u8ba8\u4e86\u4eba\u7c7b\u5982\u4f55\u5728\u9762\u5bf9\u590d\u6742\u7684\u4efb\u52a1\u548c\u5ef6\u8fdf\u548c\u7a00\u758f\u53cd\u9988\u65f6\u5b66\u4e60\u3002\u4ee5\u5bfc\u822a\u4efb\u52a1\u4e3a\u4f8b\uff0c\u63d0\u51fa\u4e86\u4eba\u7c7b\u901a\u5e38\u4f1a\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u7b80\u5355\u4efb\u52a1\u3001\u8bc6\u522b\u5e76\u5f62\u6210\u4e00\u4e9b\u4e2d\u95f4\u76ee\u6807\u6765\u5e2e\u52a9\u8fbe\u5230\u6700\u7ec8\u76ee\u6807\uff0c\u6700\u540e\u9009\u62e9\u6700\u4f73\u8def\u5f84\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cRL\u4ee3\u7406\u53ea\u6709\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u53cd\u9988\uff0c\u8fd9\u5e26\u6765\u4e86\u4e24\u4e2a\u6311\u6218\u3002\u7b2c\u4e00\u4e2a\u6311\u6218\u662f\u5982\u4f55\u6709\u6548\u5730\u5b66\u4e60\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4ee3\u7406\u9700\u8981\u5b66\u4e60\u5982\u4f55\u4ece\u7f3a\u4e4f\u53cd\u9988\u7684\u4fe1\u606f\u4e2d\u63a8\u65ad\u51fa\u6709\u7528\u7684\u4fe1\u606f\u3002\u7b2c\u4e8c\u4e2a\u6311\u6218\u662f\u5982\u4f55\u6709\u6548\u5730\u5229\u7528\u8fd9\u4e9b\u53cd\u9988\u3002\tThis work presents a strategy for improving exploration and efficiency of RL by leveraging the graph structure of an episodic experience buffer. This strategy combines goal-oriented RL with structured exploration. The authors compare their proposed technique to two popular benchmarks for goal-reaching tasks. In addition, the authors provide some theoretical justification for their algorithmic choices.""62\tThis paper discusses the ability of humans to learn complex tasks with delayed and sparse feedback. The paper takes a complex navigation task as an example, where the goal is to go from a start state to an end state. Humans often take a straightforward approach to solving this task by breaking it down into a sequence of easier tasks, identifying intermediate goals that help to get to the final goal, and then choosing a route with the highest return among a poll of candidate routes that lead to the final goal. The paper also discusses the challenges that RL agents face in real-world applications, where they only have access to sparse and delayed rewards. The first challenge is how to effectively learn from these sparse and delayed rewards, and the paper discusses two methods for\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a feedback from past rewards and the use of reinforcement learning algorithms that are able to learn from a larger set of rewards. The paper also discusses the potential applications of these methods in real-world problems.\tThis paper proposes a new framework, GSRL, to handle the sparse reward challenge and better leverage past experiences. Specifically, it formulates trajectories as a dynamic graph, and generates hindsight-like goals based on sub-group division and attention mechanism. The authors provide theoretical analysis to show the efficiency and converge property of their method. The experimental result shows the proposed method significantly outperforms the baselines. ""63\tThis paper discusses the challenges that RL algorithms face in learning tasks with complex structure and delayed and sparse feedback. The author takes a complex navigation task as an example, where the goal is to go from a start state to an end state. The author suggests that humans can learn to solve this task by breaking down the task into a sequence of easier ones, identifying intermediate goals that help to get to the final goal, and choosing a route with the highest return among a poll of candidate routes. The author also suggests that in many real-world applications, an RL agent only has access to sparse and delayed rewards, which leads to two major challenges. The first challenge is how can an agent effectively learn from these sparse and delayed rewards. The second challenge is how can an agent learn to make accurate choices in a complex task with a high degree of uncertainty.\tThis paper introduces Graph Structured Reinforcement Learning (GSRL) framework, able to balance exploration and exploitation in RL. Actually, GSRL builds a dynamic graph based on historical trajectories. Then in order to learn from sparse or delayed rewards and  be able to reach a distant goal, it decomposes the main task into a sequence of easier and shorter tasks. An attention strategy has also been proposed that is able to select an appropriate goal for each one of the easiest tasks. Experiments have been conducted on various robotics manipulation tasks showing that GSRL performs better compared to HER and MAP algorithms. ""64\tThis paper discusses the increasing interest in usingProcedural content generation (PCG) inRL research, specifically in creating novel environments such as MiniGrid, theObstacle Tower Challenge, theProcgen Benchmark, and theNetHack Learning Environment. PCG environments are created algorithmically, and they offer a promising target for evaluating systematic generalization in RL. Unlike singleton environments, which are exploitable by memorization and deterministic reset strategies, PCG environments create novel environment instances or levels that exhibit unique underlying factors of variation. This makes them a promising target for evaluating systematic generalization in RL. The paper also discusses the challenges of creating and evaluating PCG environments, and the potential future directions of this research.\tThis paper concerns about the use of experience replay in a way that past experience is sampled based on (implicit) levels so as for the agent to better adapt to the current task at hand. The authors defined a replay distribution (where experience is sampled) based on two scores relevant to learning potential and staleness. Due to its formulation, the change of replay distribution can be used as an outer-layer of a learning algorithm without any modification of the underlying learning mode. The authors conducted experiments over a set of benchmark data sets relevant to level-ness and found statistically significant improvements over more than half of the tasks.""65\tThis paper discusses the increasing interest in usingProcedural content generation (PCG) inRL research, specifically in creating new environments such asMiniGrid, theObstacle Tower Challenge, theProcgen Benchmark, and theNetHack Learning Environment. Unlike singleton environments, which areExploitable by memorization and deterministic reset strategies, PCG environments create novel environmentinstances or levels that exhibit a unique configuration of underlying factors of variation. This makes them a promising target for evaluating systematic generalization in RL. The paper also discusses the potential benefits and limitations of using PCG environments inRL research.\tThe present work considers the problem of learning in procedurally generated environments. This is a class of simulation environments in which each individual environment is created algorithmically where certain environmental factors are varied in each instance (referred to as levels in this work). Learning algorithms in this setting typically use a fixed set of training and evaluation environments. The present work proposes to sample the training environments such that the learning progress of the agent is optimized. This is achieved by proposing an algorithm for level prioritization during training. The performance of the approach is demonstrated on the Procgen Benchmark and two MiniGrid benchmarks and the authors argue that their approach induces an implicit curriculum in sparse reward settings.""66\tThis paper discusses the increasing interest inProcedural content generation (PCG) inRL research, specifically in the use of PCG environments as a promising target for evaluating systematic generalization inRL. PCG environments create novel environment instances or levels algorithmically, which exhibit unique configuration of underlying factors of variation, making them a promising target for evaluating systematic generalization inRL. This paper provides an overview of the history of PCG and discusses the recent surge in the use of PCG environments inRL research. It also discusses some of the challenges and limitations of using PCG environments for evaluating systematic generalization inRL.\tThis paper allows agents to set the initial conditions (level) for procedurally generated episodes during exploration to past observed values, and proposes to have agents form an intrinsic curriculum by resampling past levels based on a heuristic measure of expected learning progress. The authors test several heuristic measures and find that the average absolute magnitude of the generalized advantage estimate works well. The authors hypothesize that this intrinsic curriculum will improve optimization/learning relative to an agent that always samples initial conditions from the environment distribution. The authors verify that their prioritization strategy usually improves performance in several Progen Benchmark and MiniGrid environments, usually by a small but statistically significant amount, but sometimes by a large amount. ""67\tThis paper explores the problems and limitations of multitask learning and pretraining in the context of machine learning. It emphasizes the importance of carefully selecting theAuxiliary tasks, building appropriate parameter sharing architectures, and filtering out data that might be detrimental to the primary tasks. Despite these advances, leveraging the mixing of tasks is still an art left to the practitioners, and there is room for further research to address these issues. The paper proposes some potential solutions, such as choosingauxiliary tasks based on their impact on the primary task, building models that share parameters across tasks, and filtering out data that might be detrimental to the primary task. The paper also examines the limitations of these solutions and discusses the potential implications of these choices for future work in machine learning.\tThe work studies the auxiliary task selection in deep learning to resolve the burden of selecting relevant tasks for pre-training or the multitask learning. By decomposing the auxiliary updates, one can reweight separately the beneficial and harmful directions so that the net contribution to the update of the primary task is always positive. The efficient implementation is experimented in text classification, image classification, and medical imaging transfer tasks.""68\tThis paper discusses the problems and challenges of using multitask learning and pretraining in machine learning, specifically how to select helpfulauxiliary tasks, an appropriate parameter sharing architecture, and filter outauxiliary data that might be detrimental to the primary tasks. It also discusses prior work that has addressed these issues and proposed solutions. The paper argues that careful choices are required to effectively use multitask learning and pretraining in machine learning, and that there is still much to be learned about how to best leverage these techniques.\tLeveraging the power of the data-rich related tasks have been studied (e.g., pre-training and multitask learning). This paper points out that careful utilization of auxiliary task is required to gain enhanced performance in primary tasks. In order to prevent harming the performance of primary tasks, they suggest the method to decompose auxiliary updates into three directions which have positive, negative and neutral impact on the primary task.""69\tThis paper discusses the use of multitask learning and pretraining in machine learning, and how they have transformed the field by allowing downstream tasks with small training sets to benefit from statistical regularities from data-rich related tasks. The paper also examines the challenges of leveraging the mixing of tasks, such as selecting helpful auxiliary tasks, an appropriate parameter sharing architecture, and filtering out detrimental auxiliary data. Prior work has proposed solutions for these challenges, such as choosingauxiliary tasks based on their impact on the primary task or using filter methods to remove detrimental data. The paper also discusses the potential limitations of pretraining and how it may not always be effective in improving end-task performance.\tThe authors present a general formulation of different settings in multitask learning (including pretraining regimes), in a setting where the goal is to get best performance for a pre-specified primary task and additional auxiliary tasks. The main idea is to divide the gradients on the auxiliary task into 2 subspaces: a subspace where the gradients influence performance of the primary task and a subspace where they only influence the auxiliary task without changing the loss on the primary task. Within the subspace that does have influence on the primary task, it is easy to compute directions that have a positive or negative effect on the primary task, which allows to create different learning schemes given the gradients that point toward: i) auxiliary influence only, ii) positive influence on auxiliary tass, iii) negative influence on primary task. Experimental results show improvements over previously identified meta learning methods on 2 natural language datasets and 3 image datasets.""70\tThis paper describes a new embodied learning system that can perform fast-mapping, the ability to bind a new word to an unfamiliar object after a single exposure, using active perception and motor action. The system is developed in a 3D game environment and can learn the names of entirely unfamiliar objects in a single exposure and then apply this knowledge to carry out instructions based on those objects. The system observes the world through active perception of raw pixels and learns to respond to linguistic stimuli by executing sequences of motor action. This paper provides an overview of the system's capabilities and how they compare to existing language learning systems.\tThe authors use a 3D world to explore grounded language learning, in which an agent uses RL to combine novel word-learning with stably acquired meanings to successfully identify and manipulate objects.  They show that a novel, psychologically-inspired memory mechanism is more memory-efficient than Transformers (both of which outperform plain LSTMs) and that it exhibits surprisingly robust generalization to novel action-object pairs.  The results should be of interest to many working in grounded language / multimodal representation learning, and the experiments are thorough and well-motivated. ""71\tThis paper discusses the development of an embodied agent that can perform fast-mapping, the ability to bind a new word to an unfamiliar object after a single exposure, in a 3D game environment. The agent learns to respond to linguistic stimuli by executing sequences of motor action, observed through active perception of raw pixels. The goal of the paper is to enable this kind of learning system to perform fast-mapping in the physical world.\tThis paper presents experiments for acquiring words via fast-mapping in an embodied environment. The technical contribution is interesting and solid, but the experiments fail to address some important questions that are yet scoped by the claims of the paper (namely, that learning is being done -both- fast and slow, as per the title). Notably, the paper is really well-written and readable, and the experiments on novel category + novel instance recognition are really convincing specifically for fast-mapping (4.1).""72\tThis paper discusses the development of an embodied agent that can perform fast-mapping, the ability to bind a new word to an unfamiliar object after a single exposure, in a 3D game environment. The agent learns to respond to linguistic stimuli by executing sequences of motor action, using active perception of raw pixels to observe the world. The paper\u4ecb\u7ecd\u4e86\u4e00\u4e2a embodied learning system , an agent that can learn and respond to language stimuli, in a 3D game environment. The goal of the paper is to enable this system to perform fast-mapping.\tAn agent following instructions in a grounded world is a core task in AI. This paper studies agent that accomplish this using memory-based architecture. This paper presents an argument for a multi-modal memory-architecture called DCEM whose key/queries and values are dependent on language and vision modalities respectively (or vice versa). An argument is made that this will be helpful for generalizing to novel language at test-time. Results are presented in a simple 3D domain containing several objects randomly sampled each time from a set of 30 objects. Task contain two types of instructions: \"pick up an object\" and \"place an object on another object\". Interaction proceeds in episodes where each episode contains a discovery phase where the agent learns the phrase associated with each object, and an instruction phase where the agent solves a given instruction. The proposed DCEM model outperforms baselines on various metrics and ablation. Importantly, it is shown that the DCEM can generalize to novel object names. ""73\tThis paper discusses the recent development of few-Shot Learning (FSL) and its application in deep learning. FSL aims to reduce the training data requirements of deep learning models by defining a distribution over tasks, with each task containing a few labeled data points (support set) and a set of target data (query set) belonging to the same set of classes. The paper provides an overview of the background and motivation of FSL, discusses common techniques for training FSL models, and presents some recent results in deep learning. The paper also highlights the challenges and future directions of FSL, such as improving the performance of FSL models and developing more efficient training and evaluation strategies.\tThe paper analyses the effect of class imbalance on few-shot learning problems. It draws a number of interesting (but kind of expected) conclusions e.g., the support set imbalance has a larger influence on the FSL performance compared to base class imbalance, a high impact of imbalance on gradient-based meta-learning methods compared to metric learning approaches. The paper is overall  ""74\tThis paper provides an overview of few-Shot Learning (FSL) and its use in deep learning. FSL is a technique that can learn from small amounts of labeled data, even if these data are not sufficient to train the model on its own. The paper discusses the advantages and challenges of FSL, including the need for large labeled datasets, the limitations of traditional deep learning methods, and the use of episodic meta-training to overcome these limitations. The paper also provides an example of how FSL can be used to train a deep learning model for image classification.\tThe authors present a detailed study of few-shot class-imbalance along three axes: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. The authors extensively compare over 10 state-of-the-art few-shot learning methods using backbones of different depths on multiple datasets. The analysis reveals that 1) compared to the balanced task, the performances of their class-imbalance counterparts always drop, by up to 18.0% for optimization-based methods, although feature-transfer and metric-based methods generally suffer less, 2) strategies used to mitigate imbalance in supervised learning can be adapted to the few-shot case resulting in better performances, 3) the effects of imbalance at the dataset level are less significant than the effects at the support set level. ""75\tThis paper discusses the advantages and limitations of few-Shot Learning (FSL) and episodic meta-training for training deep learning models. FSL is a technique that reduces the requirements for large labeled datasets by defining a distribution over tasks, with each task containing a few labeled data points (support set) and a set of target data (query set) belonging to the same set of classes. The paper discusses how to use episodic meta-training to train FSL models, and the advantages and limitations of this approach. The paper also provides an overview of the current state of FSL and future directions for this technology.\tThis paper conducts extensive comparison experiments to study the effect of class-imbalance for many few-shot approaches. A detailed study of few-shot class-imbalance along three axes: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques, are presented. Also, this paper is clearly written and easy to understand. ""76\tThis paper discusses the development of graph-based machine learning methods and their applications in the field of natural language processing. The paper starts by introducing the concept of graph-based machine learning, which involves using graph data structures to train machine learning models. Specifically, it focuses on neural networks, which are a type of machine learning model that are particularly well-suited to graph-structured data.\n\nThe paper then discusses several graph-based machine learning algorithms, including Graph Convolutional Networks (GCNs), which are a type of neural network specifically designed for graph data. GCNs are based on the definition of a convolution operator in the graph domain, and they have shown good predictive performance for a range of tasks, such as text classification and\u81ea\u7136\u8bed\u8a00\u751f\u6210.\n\nThe paper also discusses the challenges and limitations of using graph-based machine learning, including the issue of high computational complexity due to the need to process large amounts of graph data, and the need to ensure that the network representation of the graph is able to capture meaningful information.\n\nFinally, the paper concludes by highlighting the potential applications of graph-based machine learning in the field of natural language processing, including text classification, sentiment analysis, and machine translation.\tThe paper proposes Polynomial Graph Convolution (PGC), which enjoys a larger-than-one-hop receptive field within a single layer. This is done by first propagating information with a fixed (not learned) propagation matrix (e.g. adjacency matrix or graph Laplacian), and then projecting the information from different topological distances with a learned linear layer. PGC is shown to be theoretically more expressive than linearly stacking simple graph convolutions; experiments on several graph classification tasks show good performance.""77\tThis paper provides an overview of graph-structured machine learning and its recent applications, including graph convolutional neural networks (GCNs), which are a type of neural networks designed to learn relationships between graph nodes and edges. The paper covers the history of graph-structured machine learning, the relevant research areas, and the recent advances and challenges in GCNs. The paper also discusses the performance of GCNs on a range of tasks, including image and speech recognition, and provides a comparison with other graph-based neural networks. Finally, the paper concludes by highlighting the potential of GCNs for various use cases in the field of graph processing.\tThis work proposes the Polynomial Graph Convolutional Networks (PGCNs), which is built upon the Polynomial Graph Convolution (PGC). The PGC is able to aggregate k-hop information in a single layer and comes with the hyper-parameter k. The PGCNs are composed of a PGC with k=1, followed by a PGC with a chosen k (usually > 1), and a complex readout layer using avg, max, and sum over all nodes. Theoretically, the proposed PGC has two major benefits as claimed: 1) Common graph convolution operators can be represented as special cases of the PGC; 2) A PGC with k = q (q > 1) is more expressive than linearly stacked q PGCs with k=1. The PGCNs are thus more general, expressive, and efficient than existing GNNs. Experimental studies are conducted on common graph classification benchmarks, showing the improved performances of the PGCNs.""78\tThis paper presents an overview of Graph Convolutional Networks (GCNs), a type of neural network designed for graph-structured data. GCNs are particularly well-suited for tasks such as node classification, edge prediction, and network visualization, due to their ability to effectively process and analyze graph data. The paper provides an introduction to GCNs, highlighting their design principles, including the use of neighborhood aggregation to capture graph-level features, and the use of convolutional layers to learn graph-based relationships. The paper also discusses some of the challenges and limitations of GCNs, such as the need for hand-designed features and the potential for overfitting. Finally, the paper concludes by providing a summary of the main findings and future directions for GCN research.\tThe article presents a novel framework for Graph Convolutional Neural Networks (GCNs). The method called  Polynomial Graph Convolution (PGC) is based on concatenating the powers of a transformed adjacent matrix in a given layer. The paper shows that various popular variants of GNNs can be expressed using the PGC framework.  Theoretical results presented show that PGC with higher degree is more expressive that deeper std. GNNs. Numerical results are presented on graph classification task that illustrate the performance of the method.""79\tThis paper discusses the use of scene graphs (SGs) in computer vision and computer graphics for interpretable and structural representation of scenes. SGs are a visual representation of a scene that summarize entities in the scene and plausible relationships among them. They have been used in a variety of applications such as image captioning, visual question answering, high level reasoning tasks, image retrieval, and image generation. However, most prior work on SG generation relies on the availability of expensive and limited number of labeled datasets such as the Visual Genome ( Krishna et al., 2017) and Visual Relationship Dataset (VRD) (Lu et al., 2016). Synthetic data, in this context, refers to creating data sets using artificial intelligence techniques to simulate or generate additional data, usually for training or testing machine learning models. The paper discusses the challenges and opportunities associated with the use of SGs in synthetic data generation and presents some recent advances in this field. The paper also highlights the need for further research in this area to address the limitations of current methods and to develop more efficient and effective ways of generating high quality SG data.\tThis paper introduces a framework to utilize the synthetic data as augmentations in the scene graph generation task, which is able to narrow the domain gap by decomposing it into several discrepancies between the two domains. They are the first to propose the synthetic-to-real transfer learning for SGG. The experimental results show the Sim2SG can improve the baseline models in three different scenarios: CLEVR, Dining-Sim, and Drive-Sim.""80\tThis paper discusses the use of Scene Graphs (SGs) in computer vision and computer graphics to represent and analyze scenes. SGs are structural representations of scenes that summarize entities and relationships in the scene and provide a way to perform high-level reasoning tasks on images. The paper begins by discussing the importance of SGs in computer vision and computer graphics, and the various applications of SGs in image processing and analysis. It then presents an overview of the current state of the art in SG generation and discusses the limitations of previous work. The paper then continues with a discussion of how synthetic data can be used to overcome these limitations and provide more robust and effective SG generation algorithms. The paper\u6700\u540e concludes by discussing the future directions and potential applications of SGs in computer vision and computer graphics.\tThe paper addresses the problem of learning scene graphs from synthetic data and unlabeled real data while performing well on real data by narrowing the content and appearance gap between the two domains when training on synthetic data. Scene graphs are extracted in a two-step process, mapping input to an intermediate latent space and generating the final prediction from the latent space. The authors decompose the content gap into two components: (a) label discrepancy, i.e. how much do the label distributions between the two domains differ, and (b) prediction discrepancy, i.e. the difference in distributions of outputs predicted from the latent space for the two domains. They further model the appearance gap by aligning the latent representation for both domains after accounting for the content gap (to avoid spurious influence of differing content distributions as the latent space is expected to comprise content and appearance). Most of these components are intractable and the paper provides approximations. Empirical investigation on two entirely synthetic and one real/synthetic data set provide evidence for the benefit of the method in closing the posed domain gaps as well as the quality of chosen approximations, the influence of the individual content and appearance gap terms, and the effectiveness of the optimization procedure.""81\tThis paper provides an overview of Scene Graphs (SGs), a structural representation of scenes, and their applications in computer vision and computer graphics. It discusses the background and motivation of SGs, the representation of entities and relationships in a scene, the existing works on SG generation, and the limitations of using labeled datasets for SG generation. The paper also proposes a new approach to generate SGs from synthetic data, which overcome the limitations of the existing methods. The paper\u6700\u540e\u8ba8\u8bba\u4e86 the potential applications of SGs in various fields, including image captioning, visual question answering, high-level reasoning tasks, image retrieval, and image generation.\tThe paper tackles the problem of sim2real transfer for scene graph inference. It proposes an approach for closing the gap between simulated training data and real test data, to allow models trained purely on simulated data to be deployed on real images. The approach is tested on multiple environments, including transfer from a driving scene simulator to real KITTI scenes.""82\tThis paper discusses the topic of model-based methods in continuous action space domains and their ability to achieve higher sample efficiency than previous model-free methods. The paper presents a review of the existing literature on model-based methods and their performance in different domains, such as games, natural language processing, and reinforcement learning. The paper also discusses the advantages and limitations of using a model in a model-based method and the challenges that arise when trying to build a large-scale model. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c such as developing more efficient models and improving the performance of model-based methods in real-world environments.\tThis paper proposes Randomized Ensembled Double Q-Learning (REDQ), a new model-free RL algorithm that aims to improve the sample efficiency over existing model-free methods. Experiments on Mujoco show that REDQ achieves better sample efficiency than popular model-free methods such as SAC and is comparable with model-based methods such as MBPO. The paper further provides extensive ablation studies that justify the necessity of the algorithmic components in REDQ and show that improved Q estimation bias may have been the key reason for the performance gain. The paper also provides some theoretical analysis of the Q estimation bias.""83\tThis paper discusses the issue of whether it is possible to achieve high performance without a model in continuous action space domains. It presents a review of the literature on model-free and model-based methods and their\u5bf9\u6bd4\uff0c and examines the Update-To-Data ( UTD ) ratio as a measure of sample efficiency. The paper also discusses the advantages and disadvantages of each method and the limitations of using a UTD of 1. The conclusion of the paper states that it is possible to achieve high performance without a model, but the results may not be as accurate as with a model, and that the UTD ratio is a useful measure for comparing the performance of different methods.\tThe paper proposes three techniques that altogether greatly improves the performance of soft actor-critic (SAC), resulting in a new algorithm called REDQ. (1) A higher update-to-data ratio, which speeds up the critic update. (2) Using the average ensemble Q for the policy gradient, therefore reducing its variance. (3) Taking the min of a small subset of the ensemble Qs to compute the target Q, therefore reducing the Q bias. The paper also performs extensive ablation studies to prove the importance of each technique.""84\tThis paper discusses the state-of-the-art in model-based methods in continuous action space domains and their ability to achieve higher sample efficiency than previous model-free methods. The paper focuses on Model-Based Policy Optimization (MBPO), a recent algorithm that attains high sample efficiency by using a large Update-To-Data (UTD) ratio. MBPO uses a mix of real data from the environment and \u201c fake\u201d data from its model to update the agent's policy, achieving a UTD ratio of 20-40. The paper also discusses the comparison of MBPO with Soft-Actor-Critic (SAC), a model-free method, and\u6307\u51fa\u662f\u5426\u4e5f possible to achieve such high performance without a model.\tThis work proposes a modification for double Q-learning, termed as randomized ensembled double Q-learning (REDQ). REDQ maintains $N$ different Q functions, and for each update, the target value is a minimization over $M$ randomly chosen Q functions, where $1 \\le M \\le N$. In addition, REDQ adopts a high update-to-data ratio to improve the sample efficiency. Empirical results show that the proposed method outperforms state-of-the-art model-based algorithms in certain tasks with continuous action space.""85\tThis paper discusses recent advances in deep learning from probability distributions and how they have achieved invariance under permutation of the samples and features. The paper proposes a new architecture called DIDA that inherits the NN properties of universal approximation and is robust to Lipschitz-bounded transformations of the input distribution. The paper also demonstrates the merits of DIDA on two tasks: predicting whether two dataset patches are extracted from the same initial dataset and predicting whether the learning performance achieved by a hyper-parameter is improved.\tThe method introduces the DIDA architecture to learn from distributions and be invariant to feature ordering and size.  The authors extend the ideas proposed by Maron et al. (2020) to the continuous domain and generalize their results.  The experiments are done on two tasks.  The patch identification (out-of-distribution test) clearly show the invariance to feature and dataset size. Nevertheless, it is not clear whether the method is invariant to feature permutation.  The performance model task shows properties of the architecture to predict global structures of the dataset within their meta-features.""86\tThis paper discusses recent advances in deep learning from probability distributions, which have successfully achieve classification or regression from distribution samples, making them invariant under permutation of the samples. The paper proposes an architecture called DIDA, which inherits the NN properties of universal approximation and is robust to Lipschitz-bounded transformations of the input distribution. The paper also empirically demonstrates the merits of DIDA on two tasks, which are defined at the dataset level: predicting whether two dataset patches are extracted from the same initial dataset and predicting whether the learning performance achieved by a hyper-parameter is higher or lower than its true performance.\tThe paper presents a neural network layer designed to process distribution samples that is invariant to permutations of the samples and the features. The proposed method is compared empirically to DSS, which achieves the same types of invariance but is restricted to point sets rather than discrete or continuous probability distributions. The two tasks used for the empirical evaluation in the paper are: a) patch identification (are two blocks of data extracted from the same original dataset?) and b) model configuration assessment (is one configuration of a learning algorithm going to produce a more accurate model for a particular dataset than another one?). On the first task, the paper compares to models built using Dataset2Vec embeddings as well as DSS. On the second task, the paper compares to handcrafted features as well as DSS. In both tasks, the proposed method produces more accurate predictors than DSS, etc. The paper also has some theoretical results regarding the universality of the proposed architecture and its robustness w.r.t. Lipschitz-bounded transformations. ""87\tThis paper discusses recent advances in deep learning from probability distributions, which have successfully achieve classification or regression from distribution samples, with invariance under permutation of the samples being a key feature. The paper proposes an architecture called DIDA, which inherits the NN properties of universal approximation and is robust to Lipschitz-bounded transformations of the input distribution. The paper also demonstrates the merits of DIDA on two tasks defined at the dataset level: predicting whether two dataset patches are extracted from the same initial dataset and predicting whether the learning performance achieved by a hyper-parameter is improved.\tThis paper proposes a novel set/distribution representation architecture DIDA, which leverages pairwise embedding of the set\u2019s elements. The method can be used to represent discrete and continuous distribution representation. The authors also provide the theoretical proofs of the universality of the invariant layers, the local consistency. The experiments show that the architecture improves some dataset representation tasks""88\tThis paper discusses the importance of graph learning in machine learning and data mining, and presents a overview of the challenges and recent advances in the field. Graph learning is the process of converting high-dimensional data into graph representations, which can be used to leverage the underlying structure and relationships between data points. The paper presents a discussion of the common types of graph representations and the challenges of learning meaningful graphs from large data sets. One of the recent advances in graph learning is the use of graph signal processing techniques for estimating sparse graph Laplacians, which can be used to address the issue of high computational complexity in learning meaningful graphs from large data sets. The paper also discusses some of the recent applications of graph learning in machine learning and data mining, such as image and speech recognition, and\u63a8\u8350\u7cfb\u7edf.\tThis paper studies ways of adding edges to graphs to improve the result of spectral embedding / clustering. It refines existing embeddings using by measuring edges' effect on Laplacian eigenvalues, and adjusting such edges to reduce the distortions. The performance of the algorithm is justified using developments of worst-case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy.""89\tThis paper discusses the importance of graph learning in machine learning and data mining, and presents some of the recent advances in the field. Graph learning refers to the use of graphs to represent data and perform tasks such as classification and regression. The paper highlights the challenges that remain in learning meaningful graphs from large data sets, and discusses some recent graph learning methods that have been proposed to address these issues. The paper also highlights the importance of using appropriate graph representation techniques to ensure that the resulting graph is meaningful and useful for the task at hand.\tThe paper proposes a graph learning method for spectral embedding and associated problems such as clustering and dimension reduction. What differentiates the method from much the existing literature is that it focuses approximating an optimal densification of a very sparse initial graph rather than on sparsification of an initial graph, as is more common. The method is based on iteratively identifying edges to add to the graph so as to best improve the corresponding spectral embedding, so called \"spectrally critical\" edges. The authors motivate spectral criticality in relation to the partial derivatives of an objective function inspired by the log-likelihood of a Gaussian graphical model. In particular, those with the highest partial derivatives will tend to be those which, through their addition to the graph, lead to the greatest increase in this objective. The authors go on to discuss a close connection between spectral criticality and distance distortion when comparing the spectral embedding and the original input space. Since the initial graph is very sparse it can be efficiently determined, and the relatively small number of additional edges which need to be added by the proposed method to obtain a high quality embedding means that the entire procedure can be implemented efficiently.""90\tThis paper discusses the recent development in graph learning and its applications in machine learning and data mining. It starts by introducing the basic concepts of graph representation and graph learning, and then discusses the challenges in learning meaningful graphs from large data sets at scale. Some recent graph learning methods leverage graph signal processing techniques for estimating sparse graph Laplacians, which show very promising results. The paper also provides an overview of the recent research in this field and discusses the future directions of graph learning.\tand significance: Learning a graph from data is an important, yet less studied, problem. The proposed algorithm (GRASPEL) is based on a graphical Lasso formulation with the precision matrix restricted to be a graph Laplacian. The algorithm starts with a sparse kNN graph, and recursively adds critical edges (identification of these critical edges based on Lasso and spectral perturbation analysis is the main contribution of the paper). ""91\tThis paper discusses the challenges and opportunities in developing autonomous reinforcement learning (RL) agents that can solve diverse tasks in complex and uncertain environments. The paper presents the background of RL, the importance of building a universal reward function, and the need to automatically generate diverse goals for training. It also discusses the challenges such as the need to deal with raw sensory inputs and the design of autonomous RL agents that can learn from diverse tasks. The paper\u6700\u540e suggests some possible solutions to these challenges and highlights the future directions of this field.\tThis paper proposes a method for combining intrinsic motivation on a state space with goal-conditioned reinforcement learning (GCRL), where goals are defined in some \u201cperceptual space,\u201d such as text or images, which describe the current state. The authors assume access to a renderer that maps states to perceptual goals, but do not assume that the renderer is differentiable. The authors propose to train an intrinsically motivated latent-conditioned policy, using similar techniques as past work in which a policy maximizes the mutual information between a latent variable and the current state. The goal-conditioned policy is then trained to effectively imitate the latent-conditioned policy by maximizing the same reward as the latent-conditioned policy, conditioned on only the rendered version of the final state reached by the latent-conditioned policy. The authors demonstrate that the overall method outperforms past GCRL methods on a variety of tasks (Atari, MuJoCo manipulation and locomotion, and toy tasks).""92\tThis paper presents an overview of reinforcement learning (RL) and its application in complex and uncertain environments, such as computer games and real robot control. The paper also discusses the challenges of designing autonomous agents that can solve diverse tasks and deal with raw sensory inputs, such as images, which are considered as common goals for agents to practice on and achieve. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848 for these challenges and provides an outlook on the future of RL in autonomous agents.\tThis paper proposes an unsupervised learning objective for learning perceptual goal-conditioned policies. The goal is to enable unsupervised discovery of high-level behaviors in tandem with a perceptual-goal conditioned policy that can achieve these behaviors. The learning proceeds by training one policy to exhibit diverse behaviors; the states induced by these behaviors are then rendered and used as target goal states for a separate goal-conditioned policy.""93\tThis paper discusses the use of reinforcement learning (RL) to drive autonomous agents to achieve diverse tasks in complex and uncertain environments. The paper highlights the need to build a universal reward function and design a mechanism to automatically generate diverse goals for training, as this is necessary to ensure that the autonomous agents have the ability to solve diverse tasks and remain persistent in the world. The paper also discusses the challenges of designing such an agent, including the need to deal with raw sensory inputs such as images, as well as the need to ensure that the agent is able to learn from multiple perspectives and goals.\tThis paper proposes a new solution to the problem of learning goal-conditioned policies without hand-crafted rewards. Prior work in this domain learn an embedding space to compute reward between current state and goal. In contrast, this paper utilizes unsupervised skill discovery from [1] to obtain a discriminator that identifies which states belong to a particular skill. Then, the final state of a given skill's execution is used as a goal input to a goal-conditioned policy, which is rewarded if it generates states that the discriminator identifies with this skill. The paper aims to validate the benefit of such a reward over other embedding-distance based reward functions on a variety of environments.""94\tThis paper discusses the use of reinforcement learning (RL) for modeling real-worldSequential decision-making problems such as medical domains, personalized recommendations, hardware placements, and database optimization. It discusses the desire to restrict the agent from adjusting its policy frequently in these applications, which can cause high costs and risks. Gu et al. (2017) discusses a way to train robotic manipulation by decoupling the training and experience collecting processes. This allows for more efficient and effective training while still causing less risk in the real-world.\tIn many real world applications for RL such as medicine, there are limits on the number of policies from which we can simulate data. This paper proposes an approach that adaptively decides when to update the simulation policy, based on the difference between it and the current learned policy. Experiments on a medical treatment environment and Atari show that the approach obtains similar performance to on-policy RL with fewer changes of the simulation policy.""95\tThis paper discusses the use of reinforcement learning (RL) for modeling real-worldSequential decision-making problems such as medical domains, personalized recommendations, hardware placements, and database optimization. It also discusses the desire to restrict the agent from adjusting its policy frequently in these applications. Gu et al. (2017) proposed decoupling the training and experience collecting in robotic manipulation training, which helped to reduce the number of policy adjustments needed and reduce the costs and risks associated with\u9891\u7e41\u66f4\u6539 policy.\tIn the RL context, this paper aims at designing a generic solution for reducing the number of policy switches during training (called switching cost) while maintaining the performance. This study is done in the context of deep reinforcement learning. A few generic baselines solutions are provided as well as a more complex solution that empirically outperforms the baselines.""96\tThis paper discusses the use of reinforcement learning (RL) for modeling real-worldSequential decision-making problems such as medical domains, personalized recommendations, hardware placements, database optimization, and other applications. It explains the importance of\u9650\u5236agent\u9891\u7e41\u8c03\u6574 policy in these settings, as\u9891\u7e41\u66f4\u6539 policy \u53ef\u80fd\u5bfc\u81f4\u9ad8\u6602\u7684\u6210\u672c\u548c\u98ce\u9669\u3002 Gu et al. (2017)\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8bad\u7ec3\u548c\u7ecf\u9a8c\u6536\u96c6\uff0c\u8bad\u7ec3 robotic manipulation \u6280\u80fd\u3002 This paper provides an overview of the main findings of this paper and discusses the implications of these findings for future research in the field of reinforcement learning.\tThis paper studies RL with low switching cost under the deep RL setting. It points out several naive algorithms like switching after a certain number of steps and then propose a new heuristic. This heuristic learns a new policy offline using the experience replay the behavior collected and switches the behavior policy once the similarity of the feature embeddings of the current state by these two policies becomes large. The paper also makes an attempt to provide a theoretical justification for a better understanding of the heuristic. This method might outperform the naive algorithms by some margin, if any. It would be a more interesting manuscript if some stronger results could be provided from the perspective of any of theory, experiments, or applications.""97\tThis paper focuses on the theoretical development and analysis of a stochastic optimization algorithm called Homotopy-Stochastic Gradient Descent (H-SGD) which is based on the combination of homotopy methods and stochastic gradient descent. The algorithm is designed to solve finite-sum problems of the form w\u2217\u2208argminw\u2208Rdf(w)\u2211j=1fj(w),(1)wheref: Rd\u2192Ris continuously differentiable, bounded below, and not necessarily convex. The algorithm is specifically designed to handle problems where the function value and gradient information are noisy due to the use of a stochastic first-order oracle. The paper analyzes the algorithm's performance and stability under various conditions, including the presence of noise, differentiability, and non- convexity of the objective function. The paper also presents a proof of existence and non-emptiness of the minimum value of the objective function obtained by the algorithm. The H-SGD algorithm has been applied to various machine learning and deep learning problems and has shown promising results.\tThis paper proposes homotopy SGD (H-SGD) which solves a sequence of unconstrained problems with a homotopy map and homotopy parameter. The authors analyze the algorithm for solving nonconvex problems satisfying PL condition. The analysis works with a generic homotopy map and homotopy parameter satisfying certain conditions (given in Sec 3.1). The authors show linear convergence to a neighborhood of the minimizer. The theoretical results are validated with experiments with clear explanations.""98\tThis paper focuses on the theoretical development and analysis of a stochastic optimization algorithm, called Homotopy-Stochastic Gradient Descent (H-SGD), which is based on the combination of homotopy methods and stochastic gradient descent. The algorithm is designed to solve finite-sum problems of the form w\u2217\u2208argminw\u2208Rdf(w)\u22481N\u2211j=1fj(w),(1)wheref: Rd\u2192R is continuously differentiable, bounded below, and not necessarily convex. The algorithm is specifically designed to handle problems where the function values and gradient are noisy from a stochastic first-order oracle. These problems arise in machine learning and deep learning applications, where the dimensionality of the datasets is high and full function and gradient evaluations are too expensive. The paper\u5e76\u5bf9H-SGD\u8fdb\u884c\u4e86\u6027\u80fd\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u6b64\u7c7b\u95ee\u9898\u65f6\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u6700\u540e\uff0c\u8be5\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b0\u7684\u7814\u7a76\u95ee\u9898\u548c\u6269\u5c55\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\tThis paper proposed a Homotopy-Stochastic Gradient Descent (H-SGD) algorithm by applying homotopy strategy to explore the nice local structures of problems. H-SGD can gradually approximate to the target objective function and enjoys a global linear convergence to reach a neighborhood of a minimizer. As verified by the author, the assumption of this paper is weaker than its predecessors, Karimi et al., 2016; Vaswani et al., 2019. Further, the numerical experiments verified the effectiveness of H-SGD on regression and classification tasks.""99\tThis paper focuses on the theoretical development and analysis of a stochastic optimization algorithm, called Homotopy-Stochastic Gradient Descent (H-SGD), based on the combination of homotopy methods and stochastic gradient descent. The algorithm is specifically designed to solve finite-sum problems of the form w\u2217 \u2208 arg min w\u2208Rd f ( w ) = 1N N\u2211 j=1 fj ( w ) , where f : Rd \u2192 R is continuously differentiable, bounded below and not necessarily convex. The paper assumes that we have access to noisy function values and gradients of the objective function via a stochastic first-order oracle, as in (Nemirovski et al., 2009) and ( Ghadimi & Lan, 2013). The problems of this form typically arise in machine learning and deep learning applications, where the dimensionality of the datasets makes the full function and gradient evaluations too expensive. This class of problems is generally approximately solvable using the combination of homotopy methods and stochastic gradient descent. The paper provides a detailed analysis of the algorithm, including its properties, such as its expected runtime, and its ability to solve problems of different scales. The paper also provides a case study where the algorithm is used to solve a famous optimization problem in machine learning, the binary classification problem of multi-class classification.\t1. It seems to me the proposed Homotopy-SGD is not a practical algorithm, as in each iteration the algorithm has to solve a nontrivial (possibly nonconvex) subproblem. In other words, each subproblem can be as difficult as the original problem. This leads to an essential question that what is the practical motivation of this algorithm?""100\tThis paper presents a novel methodology for shape completion from highly sparse observations. The goal is to infer the 3D geometry of a target object from only a few measurements, which is a challenging task due to the lack of sufficient data. The paper discusses the existing methods for shape completion and presents a new approach that leverages the geometric information in the observations to achieve state-of-the-art accuracy. The new method is based on a combination of image recognition and deep learning techniques and can be implemented using a variety of programming languages and frameworks. The paper also provides a detailed analysis of the performance of the new method and its limitations and opportunities.\tThis paper proposes a way of reconstructing a surface from sparse point clouds via a \"meta learning\" approach. Specifically, the authors view each shape in a collection as a \"domain\", and predicting the SDF values of points in R^3 to reconstruct a given shape (the reconstructed surface is the isosurface of the SDF field) as the \"task\" for that domain. Then, they use a network to predict a distribution over \"task-specific\" latent vectors that characterizes the reconstruction task for a given shape. Given a latent-vector sampled for one shape, they pass it to a decoder that predicts the SDF value at any point in R^3 for that shape.""101\tThis paper discusses a new method for solving the shape completion problem, which is the task of inferring 3D geometry from sparse observations. The method uses an implicit 3D geometric representation of the target object and leverages the prior knowledge of the object's shape to improve the accuracy of the estimate. Despite the fact that the method requires relatively sparse observations, it can achieve state-of-the-art accuracy on real-world objects. Some key components of the method include a novel representation of the target object, a powerful prior knowledge\u6316\u6398\u6280\u672f\uff0c and a efficient estimation algorithm. The method is applied to a real-world case study to demonstrate its effectiveness.\tThis paper introduces a meta-learning approach for the neural implicit representation of 3D shapes. The main idea, in my understanding, is to consider the points in the input point cloud as few-shot examples of the object so that each of them can be encoded in a way to best approximate the entire object information. The experiments show that the network can reconstruct the entire shape well even with a very small number of the input points, such as 50 and 100. For better reconstruction, the authors also proposed to use some implicit function regularizations, which are introduced in a previous work (Gropp et al., 2020).""102\tThis paper discusses a new method for shape completion from highly sparse observations. The method uses an implicit 3D geometric representation to infer the shape of unseen objects, while requiring only relatively sparse observations. It achieve state-of-the-art accuracy on a real-world dataset, but requires some additional setup and computational resources. The method is inspired by the idea that we can leverage the geometric information in the observations to improve the accuracy of shape completion.\tThis paper tackles the task of point cloud completion, aiming to infer an implicit occupancy representation given a sparse input point cloud. Following recent practices, the approach represents the shape via a latent-variable conditioned occupancy function $f_{\\phi}(x, h)$ that infers occupancy of a point $x$ given latent $h$. The central task addressed here is to be able to infer a posterior distribution over the latent variable given some observed points $D$ i.e. $p_{\\theta}(h|D)$.""103\tThis paper presents a review of deep neural networks (DNNs), their application in complex real-world problems, and their vulnerability to adversarial examples. DNNs have become the standard model for solving many complex problems, such as image classification, speech recognition, and natural language processing. Despite their excellent learning capabilities, DNNs are still vulnerable to adversarial examples, which can subvert their predictions. Adversarial examples can transfer across different models and remain destructive in the physical world, raises safety concerns in autonomous driving and medical diagnosis. The paper also discusses some of the recent advances in\u9632\u8303adversarial examples\u7684\u65b9\u6cd5.\tThis paper studies the use of channel suppression in improving robustness to adversarial examples. The authors make a convincing illustration in section 3 on how adversarial examples tend to activate more channels compared to natural examples, and adversarial training is not effective in reducing them. This provides a convincing motivation to their design of the Channel-wise Activation Suppression (CAS) module. Their CAS module is also effective in improving adversarial robustness when used in conjunction with different adversarial defense methods, including adversarial training, TRADES, and MART. ""104\tThis paper discusses the problems faced by deep neural networks (DNNs), specifically their vulnerability to adversarial examples and their ability to transfer across models. The paper also examines the safety concerns associated with using DNNs in autonomous driving and medical diagnosis. The paper provides an overview of the current state of the art in DNNs and discusses possible solutions to these problems.\tThis paper investigates the adversarial robustness from the activation perspective. Specifically, the authors analyzed the difference in the magnitude and distribution of activation between adversarial examples and clean examples: the activation magnitudes of adversarial examples are higher and the activation channels are more uniform by adversarial examples. Based on the above interesting findings, the authors claim that different channels of intermediate layers contribute differently to the class prediction and propose a Channel-wise Activation Suppressing (CAS) method to suppress redundant activations, which can improve the DNN robustness. ""105\tThis paper discusses the security challenges associated with deep neural networks (DNNs), including their vulnerability to adversarial examples, which can subvert the model's predictions. The paper also discusses the recent advances in\u9632Adversarial\u653b\u51fb\u6280\u672f\uff0c including\u6a21\u578b\u81ea\u9002\u5e94\u9632\u5fa1(MAT)\u548c\u5bf9\u6297\u751f\u6210\u6a21\u578b(GAN)\u7b49\u3002 The paper also provides an overview of the current state-of-the-art in DNN security, including the most recent attacks and\u9632\u5fa1\u65b9\u6cd5\u3002\u6700\u540e\uff0c the paper concludes by highlighting the importance of further research in DNN security and the need for\u52a0\u5f3a\u5b89\u5168\u9632\u8303\u63aa\u65bd\u3002\tThe authors studied the behavior of adversarial examples from the channel view of activations, which is very novel. They focused on the magnitude and frequency of activations and found that state-of-the-art adversarial defense (adversarial training) only addressed the magnitude issue but the frequency distribution issue remains. This provided a novel perspective for us to understand why state-of-the-art adversarial training method works to a certain extent but not so good. Then, the authors proposed a Channel-wise Activation Suppressing (CAS) to address the frequency distribution to further improve the adversarial robustness. CAS is generic, effective, and can be easily incorporated into many existing defense methods. ""106\tThis paper focuses on explaining the success of neural networks in many application domains, including vision, speech, and video games, by showing that gradient descent with random\u521d\u59cb\u5316 and without explicit regularization enjoys good generalization performance despite being highly overparametrized. The paper proposes the Neural Tangent Kernel (NTK), which characterized the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random\u521d\u59cb\u5316. The NTK flow is a kernel gradient flow of a functional that is constrained on a menifo, which allows for a better understanding of the performance benefits of random\u521d\u59cb\u5316. The paper also discusses the limitations of the NTK and the potential applications of the NTK in other fields.\tThis paper studies the optimization and generalization properties of a two-layer linear network. The considered setting is over-parameterized linear regression where the input dimension is D, number of samples is n<D, and the target dimension is m. The hidden width is h. The paper has two main results. The first result is exponential convergence of gradient flow to global minimum, where the convergence rate depends on the (m+n-1)-th singular value of an \"imbalance\" matrix. The second result shows that the solution found is close to the minimum L2 norm solution if certain orthogonality assumption is approximately satisfied at initially; then it was shown that if the width h is sufficiently large, then under a random initialization scheme, the solution found is close to the minimum L2 norm solution with a distance $1/\\sqrt{h}$.""107\tThis paper discusses a recent attempt to explain the success of neural networks in many application domains, including vision, speech, and video games, by the use of the Neural Tangent Kernel (NTK). The NTK is an implicit regularization effect that occurs during gradient descent with random initialized neural networks. The paper provides an analysis of the performance of such neural networks and suggests that the NTK can explain the good generalization performance that these networks enjoy despite being highly overparametrized. The paper also discusses potential applications of the NTK in other areas of machine learning.\tThis paper analyzes the convergence of gradient descent optimizing overparametrized linear nn, and proves a exponential convergence rate. Moreover, the paper proposes the distance of the optimizer to the smallest norm solution, which is justified in other papers such as Montanari, etc. as the generalizable solution. Thus the solution that SGD outputs has good generalization as well.""108\tThis paper discusses a recent technique called the Neural Tangent Kernel (NTK), which describes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. The NTK technique characterized the behavior of the network during training, and it was shown to improve generalization performance while maintaining high overparametrization. The paper also\u63a2\u8ba8\u4e86 NTK \u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u65b9\u5411\u3002\tThis paper proves the convergence rate of gradient flow for training two-layer linear networks. In particular, this paper discusses the connection between initialization, optimization, generalization, and overparameterization. The results show that gradient flow can converge to the global minimum at a rate depending on the level of imbalance of the initialization. Moreover, the authors show that random initialization and overparameterization can implicitly constrain the gradient flow trajectory to converge to a point lying in a low-dimensional manifold, thus guarantees good generalization ability.""109\tThis paper provides an overview of the recent progress in the field of neural networks and their applications in computer vision and natural language processing. The paper discusses the universal approximation property of neural networks, the problems that arise when training deep neural networks using efficient algorithms, and the recent work on over-parameterized networks with random neurons. The paper also highlights the potential of using machine learning techniques like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for tasks such as image classification and natural language processing.\tThe paper shows that the kernel derived from deep fully-connected networks on the sphere have the same approximation properties as their two-layer counterpart for ReLU activations. This implies the limitations of the kernel framework for studying the benefits of such deep networks. The authors derive the asymptotic eigenvalue decay of dot-product kernels from differentiability properties of the kernel function.""110\tThis paper discusses the universal approximation property of neural networks and the recent advances in understanding the role of deep networks for approximating certain classes of functions. It also discusses the limitations of current approaches to learning deep networks and the over-parameterized networks with random neurons that have been proposed as alternative methods. The paper also provides an overview of the current state of the art in neural network research.\tThis paper analyzed the expressive power of kernels by studying the reproducing kernel Hilbert space (RKHS) associated with the kernels. Specifically, the authors analyzed the eigenvalue decays in terms of the power series expansions of the kernel function around some points, which is related to the RKHS of the kernel. This analysis can be used to recover some previous results. Besides, using this analysis, the authors have shown several interesting results, including that NTK (which corresponds to fully-connected ReLU networks with infinite width, small learning rate, and proper initialization) with any depth has the same RKHS. The main result also has other corollaries about other kinds of kernels, e.g., Laplace kernel and infinitely differentiable kernels. Experiments were done to validate the theoretical results on synthetic datasets and MNIST/Fashion-MNIST.""111\tThis paper discusses the universal approximation property of neural networks and the recent advances in using deep networks to approximate certain classes of functions. It also highlights the limitations of current approaches and the need for new ones that can be learned using efficient algorithms. The paper also discusses over-parameterized networks with random neurons, which display universal approximation properties and provide efficient algorithms based on kernel methods or their approximations.\tRecently, there are a large number of deep learning theory papers related to the property of neural tangent kernel. This paper shows that for ReLU, the kernels derived from deep fully-connected networks have the same approx. properties as their shallow two-layer counterpart. This highlights the limitation of the kernel framework for understanding the benefits of deep networks from such perspective.""112\tThis paper studies the problem of function approximation errors in deep reinforcement learning (DRL). It explains that while DRL algorithms have achieved success in a variety of control tasks, they still exhibit biases caused by function approximation errors. The paper first analyzes the existing biases in Q-learning, the traditional algorithm used inDRL, and demonstrates that these biases are present in allDRL algorithms. It then discusses how these biases can be addressed by using other algorithms, such as deep Q-networks (DQN), which adopt neural networks to approximate the cumulative future reward. The paper also highlights the importance of using a proper model for the environment, which can help to reduce the influence of function approximation errors. Finally, the paper concludes by discussing the potential applications of this work in the field ofDRL.\tThe authors propose a deterministic policy-gradient algorithm that extends the TD3 algorithm (Fujimoto 2018). The main claim is that it reduces overestimation issues in a more effective way. Two Q-critics are maintained with separate parameters, but updated using the same transitions. Then a convex combination of these critics is used in the deterministic policy gradient update. The mixture parameter is learned on a slower time-scale to minimize this convex combination over states (instead of taking the minimum of the 2 critics per batch as in TD3). Another contribution in the paper is the \u201cUnbiased\u201d variant of the algorithm (UAD3), which addresses the off-policy nature of the replay mechanism of the AD3 algorithm described above. My understanding is that this is simply a version of the algorithm that does not use any replay mechanism and samples the state iid from the on-policy distribution, so it isn\u2019t a novel idea in itself.""113\tThis paper discusses the problem of overestimation bias in deep reinforcement learning (DRL), which is caused by the consistent maximize of a noisy value estimate in Q-learning. It was first studied by Mannor et al. (2007) and is a known issue inDRL. The paper highlights the importance of including a noise term in the value estimate and discusses the potential solutions, such as using neural networks to approximate the cumulative future reward. Additionally, the paper discusses the limitations of DQN, a popularDRL algorithm, and\u6307\u51fa that it is possible to address overestimation bias with other algorithms or by incorporating additional features into the model.\tThis paper proposes new value-based deep reinforcement learning algorithms (AD3 and UAD3) to address the overestimation bias issue of Q learning. The main contributions of this paper are three folds: 1) The authors propose a weighted sum of two state-action value functions that are trained separately. Then, it is used to update the policy. 2) The mixing weights are updated by the two-step separation method. 3) The original method (AD3) is integrated with the idea of unbiased DRL (Zhang and Huang, 2020). ""114\tThis paper discusses the problem of overestimation bias in deep reinforcement learning (DRL) and its potential solution, specifically in the context of discrete control tasks. The paper\u524d\u5148\u4ecb\u7ecd\u4e86DRL\u7684\u4e3b\u8981\u4efb\u52a1\u662f\u5b66\u4e60\u597d\u7684\u4ee3\u7406\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u7d2f\u79ef\u7684\u5956\u52b1\u3002\u5c3d\u7ba1\u5728\u591a\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u5df2\u7ecf\u901a\u8fc7DRL\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u51fd\u6570approximation\u9519\u8bef\u5bfc\u81f4\u7684\u9519\u8bef biases\u3002\u8fd9\u4e9b\u9519\u8bef biases\u6700\u521d\u7531 Mannor (2007) \u7814\u7a76\u3002DRL\u7684\u7b97\u6cd5\uff0c\u4f8b\u5982\u6df1\u5ea6 Q \u7f51\u7edc(DQN)\uff0c\u901a\u5e38\u7528\u4e8e\u79bb\u6563\u63a7\u5236\u4efb\u52a1\uff0c\u4f46\u7531\u4e8eDQN\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u8fd1\u4f3c\u7d2f\u79ef\u7684\u672a\u6765\u5956\u52b1\uff0c\u56e0\u6b64\u4e0d\u53ef\u907f\u514d\u7684\u4f1a\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u51fd\u6570approximation\u566a\u58f0\u4e00\u8d77\u5b58\u5728\u3002\u6b64\u5916\uff0cDQN\u7684\u66f4\u65b0\u7b56\u7565\u662f\u901a\u8fc7\u9009\u62e9\uff0c\u800c\u4e0d\u662f\u6700\u5927\u5316\u8bef\u5dee\u3002\u56e0\u6b64\uff0cDRL\u7b97\u6cd5\u5728\u5904\u7406\u79bb\u6563\u63a7\u5236\u4efb\u52a1\u65f6\u53ef\u80fd\u4f1a\u5b58\u5728overestimation bias\u3002\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3overestimation bias\u7684\u65b9\u6cd5\u3002\tThe paper presents an approach to mitigate the overestimation issue, which is quite common in RL algorithms whenever computing boostrap target is needed. The key idea is to introduce a weight parameter between two Q values and adopt a dual problem formulation to learn this weight and the policy parameters. The authors propose a two-step method to estimate these parameters and present some experiments to show the proposed algorithm's performance.""115\tThe paper presents the Introduction to Inverse Reinforcement Learning (IRL) which addresses the problem of determining a reward function over a Markov decision process (MDP) if the model dynamics are known. The recovered reward function provides a concise, robust, and transferable definition of the learning task and completely determines the optimal policy. However, the IRL problem is ill-posed, meaning that the policy may be optimal for many reward functions and expert demonstrations may be optimal for many policies. To overcome these limitations, two classes of probabilistic approaches are proposed: Bayesian inverse reinforcement learning (BIRL) and maximum equilibria learning (MLE). BIRL uses Bayesian MAP estimation to determine the reward function and MLE uses maximum equilibria theory to determine the optimal policy. The paper\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u65b9\u5411\u3002\tThe authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). In my understanding, the knowledge of the environment dynamics is assumed. The authors tried to validate the proposed idea on objectworld (Levine et al., 2011)""116\tThe paper presents the IRL problem, which is the inverse problem of determining a reward function over a Markov decision process (MDP) if the model dynamics are known. The optimal policy is determined by the recovered reward function. However, the IRL problem is ill-posed, as the policy may be optimal for many reward functions and expert demonstrations may be optimal for many policies. To overcome these limitations, two classes of probabilistic approaches are proposed: Bayesian inverse reinforcement learning (BIRL) and maximum e-max (ME)BIRL. BIRL uses Bayesian MAP estimation to determine the reward function, while MEBIRL uses maximum e-max estimation to determine the reward function. These approaches are shown to be effective in solving the IRL problem in various settings and domains.\tThe authors propose an approach to model-based inverse reinforcement learning which estimates a Gaussian mixture model over reward-function parameters. The method uses MCEM and samples reward functions from a current estimate of the GMM, updates them via a gradient-descent based maximum likelihood approach and then updates the GMM to fit the updated parameters. The authors evaluate the approach on objectworld.""117\tThe paper presents the IRL problem, which is the inverse problem of determining a reward function over a Markov decision process (MDP) if the model dynamics are known. The recovered reward function provides a concise, robust, and transferable definition of the learning task and completely determines the optimal policy. However, the IRL problem is ill-posed, meaning that the policy may be optimal for many reward functions and expert demonstrations may be optimal for many policies. To overcome these limitations, two classes of probabilistic approaches are proposed: Bayesian inverse reinforcement learning (BIRL) and maximum echelon learning (ML). BIRL uses Bayesian MAP estimation to determine the reward function, while ML uses maximum a posteriori estimation to determine the policy. These approaches have been shown to be effective in solving the IRL problem in various domains.\tThe paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. Prior work has either learned a point-estimate, notably maximum entropy IRL, or used Bayesian methods to learn a probability distribution over reward functions. Maximum entropy IRL has scaled to complex environments with unknown dynamics and non-linear rewards (with methods such as AIRL), but do not learn a probability distribution. By contrast, Bayesian IRL is more theoretically principled, but has not scaled to complex environments or non-linear rewards. This paper performs maximum likelihood estimation of a parameter for a *generative model* over probability distributions, using a Monte-Carlo expectation-maximization (MCEM) method. It therefore still outputs a probability distribution like Bayesian IRL, but is able to learn non-linear rewards unlike prior Bayesian methods.""118\tThis paper discusses the use of unlabeled data in deep learning and the recent advances in this area. It explains the importance of leveraging unlabeled data in deep learning and discusses recent research in semi-supervised learning and unsupervised domain adaptation, which use unlabeled data to improve their performance. It also discusses the challenges and limitations of using unlabeled data in deep learning, such as the need for additional data, accuracy, and the lack of a gold standard for labels. Finally, the paper concludes by suggesting future directions and potential applications of unlabeled data in deep learning.\tof the paper: The paper gives a theoretical justification of self-training. It proposes a new notion of \"expansion\" - the amount of data distribution in the neighbor of an example. Here the neighbor means adding perturbations to the example, or augmentations of the example. When the label distribution satisfies nice expansion properties and that classes are properly separated according to the neighbors, the paper proves distributional guarantees of self-training. Combining with generalization bounds of DNNs, the paper also derives finite sample bounds for DNNs. The paper also verifies the expansion assumption via experiments using a GAN.""119\tThis paper provides an overview of recent advances in leveraging unlabeled data in deep learning. It discusses the importance of unlabeled data, the various types of unlabeled data that can be used, the recent work in semi-supervised learning and unsupervised domain adaptation, and the recent progress in representation learning. The paper also highlights the challenges and future directions in leveraging unlabeled data in deep learning.\tThis work provides a unified framework to analyze the self-training, semi-supervised algorithms. The key assumptions are 1) the \u201cexpansion\u201d assumption which characterizes the low-probability data subset must expand to a neighborhood with large probability; and 2) the neighborhoods of samples from different classes have small overlap.  Then the authors established the upper bound of the prediction error on the population when minimizing the self-training and input-consistency based loss on the population. They also extend their results to a finite-sample setting and semi-supervised setting as well.  ""120\tThis paper discusses the use of unlabeled data in deep learning and the recent advances in this area. It emphasizes the importance of leveraging both labeled and unlabeled data in improving the performance of deep learning models. The paper also discusses the recent advances in unsupervised learning and representation learning, which can be used to learn high-quality features from unlabeled data. Finally, the paper provides an overview of the current state-of-the-art in this field and\u5c55\u671b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411.\tThis paper provides a theoretical analysis of self-training for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. The authors propose a novel assumption that they dub _expansion_ to effect this analysis. The expansion assumption requires that the neighborhood of small sets have a class conditional distribution that is large. Under this assumption, the authors show population results for an algorithm that performs self-training under the objective that enforces input consistency. ""121\tThis paper discusses video prediction, a technique that generates future frames based on the current videoclip. Video prediction is important for many applications, including improving planning for model-based reinforcement learning, forecasting future events, and activity. To be able to truly benefit from video prediction, it is necessary to be able to predict the long-term future of the visual sequence. Many previous approaches have formulation video prediction as a conditional generation task by recursively synthesizing future frames conditioned on the previous frames. Despite their success in short-term forecasting, none of these approaches has been able to predict the long-term future. This is because most of them rely on a simple model that only considers the current frame and does not take into account the future information. This paper presents a new approach that can predict the long-term future of the video sequence by using a complex model that considers both the current and future frames. The paper also discusses the challenges and limitations of predicting the long-term future of the video sequence and suggests future directions for the field.\tThis paper proposes a VAE based hierarchical model for video prediction. The model employs recurrent model to predict intermediate representations (in the form of label maps) and these representations are mapped to pixel level information, i.e., videos. The paper presents an interesting idea of using representations that do not use any domain knowledge. The authors demonstrate the value of modeling temporal evolution of these representations which enables long term video prediction.""122\tThis paper provides an overview of video prediction, a problem that is often used in computer vision, where it aims to predict future frames of a short video based on the current frame. The paper\u8ba8\u8bba\u4e86video prediction\u7684\u80cc\u666f\uff0c\u76ee\u6807\uff0c\u548c\u4e00\u4e9b\u5df2\u6709\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5728 Short-term forecasting \u548c Long-term forecasting \u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u96be\u9898\u3002\tThe paper extends video-to-video translation model of (Wang\u201918) to video prediction by first generating a sequence of segmentation masks and then translating them into videos. Variational video prediction is used to generate a sequence of segmentation masks. The model produces impressive high-resolution and long-horizon results, and is extensively evaluated on Kitti, Cityscapes, and dancing data, outperforming some previously proposed methods.""123\tThis paper provides an overview of video prediction, a problem that is particularly relevant in the context of reinforcement learning and other AI applications. It discusses the importance of forecasting the future of a visual sequence and the challenges that arise when trying to achieve this goal using traditional machine learning techniques. This paper proposes a new approach to video prediction that addresses these challenges by using a conditional generation task and a deep learning architecture. The proposed method consists of a recursively synthesized future frame generation network that is trained to predict the future of the video sequence. The paper also discusses the performance of the proposed method and the limitations of previous approaches.\tThis paper proposes a hierarchical framework for long-term video prediction. The structure is firstly predicted in the form of semantic map. It lies in a categorical structure space which is easier to predict. Then the authors translate the predicted semantic map to a real video sequence in a frame-by-frame manner. The proposed model is \"surprisingly successful\" for long-term video prediction, as claimed by the authors (thousands frames). ""124\tThis paper presents a review of the latest GNN (Graph Neural Network) architectures and their applications. The paper starts by introducing the GNN concept and highlighting its effectiveness in processing graph structured data. It then discusses the various architectures that have been developed over the years, including GCN, GraphSAGE, GAT, DGI, GIN, GCNII and GEN. The paper also highlights the importance of considering directed message passing between nodes in improving the performance of GNN. Finally, the paper ends with a summary of the key findings and future directions of GNN research.\tThis work proposes a new graph neural network architecture with modified rules for message passing, Iterated Graph Neural Network System (IGNNS). The paper then provides a theoretical analysis of the proposed architecture by connecting it with Iterated Function System (IFS), an important research field in fractal geometry. This paper further demonstrates empirically that the proposed architecture outperforms related models on citation network datasets.""125\tThis paper provides an overview of recent advances in graph neural networks (GNNs) and their application in various fields. It explores the history of GNNs, the various architectures that have been developed, and the benefits of including directed message passing in GNNs. The paper also discusses the challenges and limitations of GNNs, such as the size of the data and the computational complexity. Finally, the paper concludes by highlighting the potential of GNNs for future research and applications.\tThis paper proposes a new framework of GNN which can deal with undirected and directed graphs in a unified way. The authors argue that the size of the symbol space for a message passing path with length n is 2^n, while previous architectures only have constant size. Motivated by this observation, the authors borrow ideas from Iterated Function System to augment the symbol space. ""126\tThis paper provides an overview of the latest GNN architectures and their advantages over previous ones. The paper also discusses the challenges and future directions of GNNs.\n\nThe paper starts by introducing GNN ( Scarselli et al. , 2009 ), which is a machine learning technique that processing graph structured data. It has been widely used in various fields, including natural language processing, computer vision, data mining, social network and biochemistry.\n\nThe paper then presents a variety of GNN architectures, such as GCN ( Kipf & Welling , 2017 ), GraphSAGE ( Hamilton et al. , 2017 ), GAT ( Veli\u010dkovi\u0107 et al. , 2018 ), DGI ( Veli\u010dkovi\u0107 et al. , 2019 ), GIN ( Xu et al. , 2019 ), GCNII ( Ming Chen et al. , 2020 ), and GEN ( Li et al. , 2020 ). These architectures have a common feature, which is that they update the representation of each node using messages from its neighbors without distinguish the direction of message passing between two nodes.\n\nHowever, recent studies have shown that considering directed message passing between nodes can improve the performance of GNNs and achieve success in related fields. For example, DimeNet ( Klicpera et al. , 2020 ) considers the spacy word sense disambiguation task by considering the direction of message passing between words.\n\nThe paper also discusses the challenges and future directions of GNNs. One of the challenges is the scalability of GNNs, as they\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u624d\u80fd\u5904\u7406\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002Another challenge is the issue of overfitting, which occurs when the GNN is too complex to capture the training data and fails to generalize to new data.\n\nThe paper also suggests some future research directions, such as developing more efficient GNN algorithms, improving the representation of graph structured data, and developing new applications of GNNs in various fields.\tThe paper proposes a new definition of GNNs designed to cope with bi-directional message-passing processes.  To do so, a new symbols space, different from the one adopted by Bidirectional GCN,  is considered, together with an iterated function system. These lead to an architecture composed of 4 steps: an input layer that acts as a classic FC layer; an IFS layer that applies the iterated function system considering the adjacency matrix; a layer to concatenate or sum the expected values of each iteration; and an output layer that combines the results using the functions of the IFS and a new learnable weight matrix.""127\tThis paper discusses the problem of modeling distributions over graphs and the challenges that arise when trying to do so. It reviews the state of the art in graph generation and highlights the importance of properties such as isomorphism consistency in ensuring that the generated graphs are similar to their original ones. The paper also discusses the potential applications of deep graph generative models in a wide range of fields.\tThe paper investigates graph generation using adversarial technics. They introduce an algorithm named GG-GAN, based on Wassertain GAN, in order to accurately generates new graphs in hopefully the same distribution as a given dataset. GG-GAN generates points in an euclidian space that is then turned into a graph using a similarity function on the space. This approach is justified by Theorem 1. The authors show that their method successfully generate graphs within the same scope as the input dataset, and show that GG-GAN generates much more new graphs that current state of the art approach.""128\tThis paper studies the problem of training deep graph generative models and discusses the properties that need to be possess by these models in order to be useful for a wide range of application domains. The paper starts by introducing the fundamental problem of learning distributions from empirical observations and the challenges that arise when modeling graphs, which are discrete objects with complex relational structure. It then discusses the four properties that a graph generator should possess to achieve consistency in assignments of graphs. The paper ends by providing an overview of the current state of the art in training graph generative models and highlighting the potential applications of these models.\tIn general, this paper deals with an interesting and essential problem to generate geometric graphs under several standards. The whole algorithm seems easy to implement or reproduce. It seems with minor modifications to traditional autoregressor based generative graph models, the proposed framework can effectively model isomorphism as well as delivers certain novelty. The idea of the paper is with novelty and some theorems can support the observations.""129\tThis paper discusses the problem of modeling distribution over graphs, a challenging task due to the complex relational structure of these objects. The paper presents some recent advances in the field of deep graph generative models, which can be used to generate novel graph patterns with high accuracy. The paper also discusses the four properties that a graph generator should possess to ensure consistency in its output: isomorphism consistency, degree distribution, edge existence and edge probability. The paper provides a review of the existing literature and presents a future direction for the field.\tThe work proposes to use WGAN architecture to learn latent space for generating new graphs with similar properties to the original ones. The authors show that their model is capable to control a probability of each new generated graph. Moreover it\u2019s equivariant function which ensures that isomorphic graphs have the same probability to be generated. These properties are desirable  if we want to generate efficiently new graphs with properties similar to the graphs in the training set.""130\tThis paper introduces a new method forExplaining Neural Networks that uses rules X \u2192 Y to represent the interactions between neurons in a neural network. The method works by considering the activations of neurons in the network over a given dataset and proposing to characterized these in terms of rules. This allows for a more comprehensive understanding of the network's behavior and can be particularly useful for understanding how the network performs its tasks and perceives the world. The method is motivated by the fact that most existing methods for explaining neural networks focus on the mapping between input and output or only  characterize individual neurons within the network.\tThe paper proposes an approach to explainable supervised learning by extracting sets of rules for two individual layers within a neural network. The authors build their work on recent published work for patttern-based rule mining [0] to efficently find so-called robust rules. The authors evaluate the approach for image processing tasks with convolutional neural networks on MNIST, ImageNet and Oxford Flower by comparing generated rules against activation maps and prototypes.""131\tThis paper introduces a new method for understanding how neural networks operate, particularly how they learn to perform specific tasks and how their neurons work together. The method involves analyzing the activations of neurons in a neural network over a given dataset, using rules X \u2192 Y that represent the relationships between sets of neurons in different layers. This allows for a more comprehensive understanding of the network's behavior, and allows for the discovery of succinct and non-rarity rules. The method is also robust to noise and allows for the exploration of different network architectures.\tThis paper proposes to extract interpretable rules from a learned neural network. The authors claim that they are the first to propose rules connecting 1) multiple neurons together, and 2) do this at a dataset level. Their approach relies on using minimum description length and well known principles from the data mining community (e.g., downward closure lemma of apriori algorithm). The authors claim that experiments conducted on image data shows that their approach leads to more faithful, interpretable rules than other approaches such as prototyping or model distillation.""132\tThis paper proposes a new method for explaining how neural networks operate by considering the activations of neurons in the network over a given dataset in terms of rules X \u2192 Y, where X and Y are sets of neurons in different layers of the network. The rules represent that neurons Y are typically active when neurons X are, allowing for a more comprehensive understanding of the network's behavior. The method is inspired by model distillation and prototyping, but allows for more fine-grained characterization of the network's interactions. The paper also explores the impact of noise on the discovery of rules and the ability of the method to discover succinct and non-regrettable rules. The method is shown to be effective in explaining the performance of neural networks on a range of tasks and datasets.\tThe authors propose a method to explore how neurons interact within a neural network and derive rules of interactions that can help interpret the inner workings of the neural network and open up the black box. The algorithm, EXPLAINN, identifies rules between successive layers where each rule represents a set of neurons that are activate simultaneously and conditionally based on the previous layer. Minimum Description Length principle is used to derive an objective that minimizes the number of bits used to encode the rules. The rule sets are identified using a greedy heuristic and improved until convergence of the objective. The algorithm is then evaluated to demonstrate the interpretation of images with MNIST, GoogLeNet and VGG-S. ""133\tThis paper studies the worst-case guarantees of fixed-dataset policy optimization algorithms. It provides a unified conceptual and mathematical framework for the study of these algorithms and identifies a difficult-to-satisfy requirement for ensuring near-optimal policy selection. The paper shows that optimistic approaches may overstate the value of the dataset and lead to overfitting. To avoid this, algorithms can follow the pessimism principle, which states that they should choose the policy which acts optimally in the worst possible world. The paper provides a theoretical analysis and families of algorithms that follow this principle, and validate their performance using experimental results on a tabular gridworld and deep learning experiments on four MinAtar environments.\tThe paper proposes a theoretical framework for analyzing the error of reinforcement learning algorithms in a fixed dataset policy optimization (FDPO) setting.  In such settings, data has been collected by a single policy that may not be optimal and the learner puts together a model or value function that will have explicit or implicit uncertainty in areas where the data is not dense enough.  The authors provide bounds connecting the uncertainty to the loss.  They then show that explicitly pessimistic algorithms that fill in the uncertainty with the worst case can minimize the worst case error.  Similarly, proximal algorithms that attempt to adhere to the collection policy (as often the case in model-free batch RL) have improved error compared to a naive approach but not as good as an explicitly pessimistic approach.""134\tThis paper studies the worst-case guarantee of fixed-dataset policy optimization algorithms, specifically looking at the requirement to ensure that the selected policy is near-optimal. The paper provides a unified conceptual and mathematical framework for the study of these algorithms, highlighting the importance of\u907f\u514dvalue overestimation, which can lead to difficult-to-satisfy requirements. The paper also shows why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derivations of families of algorithms that follow the pessimism principle. The theoretical findings are validated by experimental results on a tabular gridworld and deep learning experiments on four MinAtar environments.\tThe message of this paper is that naive policy evaluations common in current (deep) RL algorithms, can lead to a dangerous overestimation of the value function. This overestimation of the value function can then lead to policy improvements with poor theoretical guarantees. To combat overestimation, the authors propose to penalize state-action pairs that are rarely visited. As an easier to implement alternative, and closer to existing algorithms in the literature, the authors also study another penalty term that penalizes deviation from the data generating policy. The authors show on a numerical example that the more principled penalty term that depends on visitation counts is better performing, and that the proximal penalty term only yields minor improvements over imitation learning (i.e. returning the data generating policy).""135\tThis paper studies the expected return of fixed-dataset policy optimization algorithms and provides a unified conceptual and mathematical framework for the analysis. The paper shows that for optimistic approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. The paper shows why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. The theoretical findings are validated by experiments on a tabular gridworld and deep learning experiments on four MinAtar environments.\tThis paper attempts to unify prior work on fixed-dataset (aka \"batch\" or \"offline\") reinforcement learning. Specifically, it emphasizes the importance of pessimism to account for faulty over-estimation from finite datasets. The paper shows that naive algorithms (with no pessimism) can recover the optimal policy with enough data, but do so more efficiently. The pessimistic algorithms are divided into \"uncertainty-aware\" and \"proximal\" algorithms where the uncertainty-aware algorithms are shown to be more principled, but most prior work falls into the computationally easier proximal family of algorithms that is closer to imitation learning. These insights are proven both theoretically and with some small experiments.""136\tThis paper proposes a new method for numerical integration of neural ordinary differential equations ( Neural ODEs). The method is based on the asynchronous leapfrog (ALF) solver and the Memory-efficient ALF Integrator (MALI). MALI has a constant memory cost w.r.t. the number of solver steps in integration and ensures accuracy in reverse-time trajectory, which is important for gradient estimation. The method was validated on image recognition tasks and outperforms a well-tuned ResNet. Overall, MALI enables feasible training of Neural ODEs on ImageNet and improves the performance of existing methods.\tThis paper presents a memory-efficient asynchronous leapfrog integrator for numerically solving neural ODEs, referred to as MALI. The method comes with a constant memory guarantee (like the adjoint method) and also guarantees reverse-time accuracy (like the adaptive checkpoint adjoint (ACA) method). The authors also give a rigorous theoretical analysis of MALI, and also discuss a \"damped\" version with an increased stability region. The method is evaluated on a variety of tasks which includes classification, dynamical modelling and generative modelling.""137\tThis paper proposes a new method for numerical integration of Neural ODEs, called the Memory-efficient ALF Integrator (MALI). ODEs are a type of differential equations that describe the dynamics of systems of interest, and Neural ODEs are a new family of deeplearning models with continuous depth. However, the numerical estimation of the gradient in the continuous case is not well solved, as existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory. MALI is based on the asynchronous leapfrog (ALF)solver and has a constant memory cost w.r.t. the number of solver steps in integration, similar to the adjoint method, and it guarantees accuracy in reverse-time trajectory (\u4ece\u800c\u63d0\u9ad8\u4e86\u6570\u503c\u8ba1\u7b97\u7684\u51c6\u786e\u6027)\u3002 The paper also validated MALI on various tasks, including image recognition, and it outperforms a well-tuned ResNet on ImageNet\u3002\tThere are typically two methods for estimating the gradients with respect to the loss for neural ODEs. The naive method directly backpropagates through the steps of the ODE solver leading to accurate gradients but very large memory cost. The adjoint method in contrast does not store the entire trajectory in memory, but has reverse trajectory errors (i.e. the numerical solution in the reverse direction will not be the inverse of the numerical solution in the forward direction). In this paper, the authors propose a method that is both reverse accurate and has low memory cost.""138\tThis paper proposes a new method for numerical integration of Neural ODEs, the Memory-efficient ALF Integrator (MALI). ODEs are a type of differential equations that describe the behavior of systems of particles or gases. Neural ODEs are a new family of deep learning models with continuous depth that can be used for tasks such as image recognition. However, the numerical estimation of the gradient in the continuous case is not well solved, as existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory. MALI uses the asynchronous leapfrog (ALF)solver to guarantee accuracy in reverse-time trajectory and has a constant memory cost w.r.t. the number of solver steps in integration, similar to the adjoint method. MALI is validated on various tasks, including image recognition, and outperforms existing methods.\tThis paper proposes a new algorithm for solving neural ODEs. Each numerical solver step of the neural ODE is implemented as an invertible neural network via a variant of the asynchronous leafprog integrator. While still computing an accurate gradient, this allows memory savings by discarding intermediate data from the numerical integration steps since it can be reconstructed using the inverse. A theoretical stability analysis is provided. The experimental results show that the algorithm achieves similar performance to previous methods (e.g. ACA) while using less memory. ""139\tThis paper discusses the recent advances in generative models, including Generative Adversarial Networks (GANs), and their applications in image generation and scene understanding. GANs have achieved remarkable results in single class conditional image generation and have led to the development of more challenging tasks such as complex scene conditional generation. The paper also highlights the importance of training GANs with large amounts of data and incorporating prior knowledge into the model.\tThe paper provides a set of comparisons among different scene generation methods. It assesses ability of the models to fit the training set (seen conditionings), generalize to unseen conditionings of seen object combinations, and generalize to unseen conditionings composed of unseen object combinations. It finds that these models fit the training distribution with a moderate success, display decent generalization to unseen fine-grained conditionings, and have significant space for improvement when it comes to generating images from unseen coarse""140\tThis paper provides an overview of the recent advances in generative models and their applications in various fields. It discusses the significant advances made in the area of generative models such as Generative Adversarial Networks (GANs) and their applications in single class and complex scene conditional image generation. The paper also highlights the challenges and future directions in this field.\tThis paper studies the problem of scene conditional image generation with a focus on the evaluation of existing works towards unseen complex scene generation on the COCO-Stuff dataset. Specifically, it evaluates the model performances from three aspects, namely, image generation from seen conditionings, unseen fine-grained conditionings, and unseen coarse conditionings. For each evaluation, it computes the precision, recall, conditional consistency, F1-score, object accuracy, FID and diversity score for both object-wise and scene-wise measures. ""141\tThis paper provides an overview of the recent advances in generative models and their application in image generation. The paper discusses the most promising approaches to generate high-quality images in various application domains, including single class conditional image generation and complex scene conditional generation. The paper also highlights the significant progress made in using generative models in real-world applications.\tProblem: There has been a plethora of work on image synthesis from a given layout of objects or label maps. However, it is not clear what has led to those results because there are no fixed backbone, optimization, training data, and evaluation protocol in each of them. This paper introduces a methodology to study three approaches (G2im, LostGAN, OC-GAN) that input a layout of objects to synthesize a new image.""142\tThis paper discusses the advantages and limitations of multi-layer Graph Neural Networks (GNNs), and how they can be simplified through various techniques. The paper also examines how well these simplified GNNs perform in various applications. The paper starts by introducing GNNs and highlighting their popularity in various fields. It then discusses the advantages of GNNs, including the ability to combine information across nodes, and the ability to learn complex relationships between variables. However, the paper also notes that GNNs can suffer from issues such as over-smoothing, exploding or vanishing gradients, and bottleneck effects, which can make them difficult to train. To address these issues, the paper suggests various techniques for augmenting the node featurization, such as augmenting the number of nodes in the graph, or adding features to the nodes. The paper also discusses how these simplified GNNs can be used in various applications, such as computer vision and\u81ea\u7136\u8bed\u8a00\u5904\u7406.\tThe paper presents a theoretical analysis to compare expressive power of Graph-Neural Networks (GNNs) w.r.t a class of simpler graph modles called  Graph-Augmented MLPs (GA-MLPs).   GNNs, especially deeper ones can be more difficult to train, and GA-MLPs have a simpler structure, significantly easier to train, and have been shown to have competitive performance on a number of tasks.  The paper dives deep into several problems (graph-isomorphism, node-classification, and community detection) and through innovative analysis shows that GNNs at least theoretically can have significant advantages for some of the problems. ""143\tThis paper discusses the advantages and limitations of Graph Neural Networks (GNNs), a type of neural network used for tasks such as\u805a\u7c7b\uff0c\u5206\u7c7b\uff0c and\u56fe\u7ed3\u6784\u8868\u793a. The paper begins by introducing GNNs and highlighting their popularity in various fields. It then discusses the challenges of training GNNs, including over-smoothing, exploding/vanishing gradient issues, and bottleneck effects. The paper\u968f\u540e\u4ecb\u7ecd\u4e86\u51e0\u79cd\u51cf\u8f7b\u8fd9\u4e9b\u9650\u5236\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u589e\u52a0\u8282\u70b9\u7279\u5f81\u7ef4\u5ea6\u3001\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7b49\u3002 The paper ends by discussing the potential future directions of GNNs and the limitations of current models.\tThe paper compares graph neural networks (GNNs) with graph-augmented multi-layer perceptrons (GA-MLPs) where GA-MLPs are MLPs over nodes with additional node features computed over the graph. The paper contains theoretical results and experimental results for graph isomorphism testing and for node level functions. In the overflow of papers studying the expressivity of GNNs, the originality comes from the study of GA-MLPs and from the comparison for node level functions.""144\tThis paper discusses the advantages and limitations of multi-layer Graph Neural Networks (GNNs), as well as recent research efforts to simplify or\u53ef\u4ee5\u51cf\u8f7b\u5176\u8bad\u7ec3\u548c\u74f6\u9888\u95ee\u9898\u3002\n\nGNNs \u57fa\u4e8e\u90bb\u57df\u805a\u5408\u6765\u7ec4\u5408\u4e0d\u540c\u8282\u70b9\u7684\u4fe1\u606f\uff0c\u5e76\u589e\u52a0\u6df1\u5ea6\u53ef\u4ee5\u589e\u52a0\u63a5\u6536\u5668\u8303\u56f4\u3002\u7136\u800c\uff0cGNNs \u5728\u8bad\u7ec3\u548c\u8868\u73b0\u4e0a\u53ef\u80fd\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u5982\u8fc7\u62df\u5408\u3001\u7206\u70b8\u68af\u5ea6\u548c\u74f6\u9888\u6548\u5e94\u3002\n\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\uff0c\u5982\u9996\u5148\u589e\u52a0\u8282\u70b9\u7279\u5f81\uff0c\u4ee5\u589e\u52a0\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u65e8\u5728\u4fdd\u7559GNNs \u7ec4\u5408\u4fe1\u606f\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u51cf\u8f7b\u5176\u8bad\u7ec3\u548c\u74f6\u9888\u95ee\u9898\u3002\tThe paper studies a variant of Graph Neural Networks (GNNs) namely, Graph Augmented MLPs (GA-MLPs). Unlike in GNNs where nodes send messages to neighbors, and aggregate received messages via non-linear MLPs,  GA-MLPs rely on a single augmented embedding computed once and then applying an MLP to the new embeddings. The augmented embeddings can be obtained by applying linear transformations of the form A, A^2, \u2026, A^k to the input representations, thereby capturing larger neighborhoods. The main goal of the paper is to demonstrate a fundamental weakness when using GA-MLPs for solving graph problems as compared to GNNs. Along these line the paper the main results can be characterized as follows:""145\tThe paper discusses the challenges of reinforcement learning in actor-latency-constrained settings, where the acting policy must be low-latency and have a limited model size. These constraints prevent the use of high-capacity models, such as model compression or off-policy reinforcement learning, because they require a high computational complexity during learning. The paper proposes solutions, such as using a single inference step within a fixed budget of time, Tactor, to overcome these constraints. This setting is ubiquitous in real-world application areas, such as learning policies for robotic platforms.\tThe paper proposes an original idea to use distillation to speed up modern distributed RL settings, when data collection is done on CPUs with the learning happening on accelerated hardware, e.g. GPU. More specifically, the authors propose to use a transformer for the learner and distil the policy into LSTM actors. With this, they achieve much faster wallclock time compared to the Transformer for Actors setup, however, losing in sample-efficiency.""146\tThis paper discusses the challenges in reinforcement learning when the acting policy has maximum latency constraints, which limit the model size and computational complexity. The paper\u8ba8\u8bba\u4e86\u4e00\u4e9b\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6a21\u578b\u538b\u7f29\u548c off-policy reinforcement learning\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728 latency  constraints \u9762\u524d\u90fd\u9762\u4e34\u7740\u4e00\u4e9b\u6311\u6218\u3002\u8be5\u6587\u8fd8\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u95ee\u9898\u5728 real-world \u5e94\u7528 areas \u4e2d\u7684\u666e\u904d\u60c5\u51b5\uff0c\u4f8b\u5982\u5728\u673a\u5668\u4eba\u63a7\u5236\u9886\u57df\uff0c\u8fd9\u4e2a\u95ee\u9898\u5bf9\u4e8e\u5b66\u4e60 policies \u6765\u8bf4\u662f\u4e00\u4e2a\u666e\u904d\u5b58\u5728\u7684\u95ee\u9898\u3002\tThe paper proposes a method for \"actor-latency constrained\" settings: Recently, transformers have been shown to be powerful models in RL which, in particular, exhibited better sample complexity in settings in which long-term credit assignment in partial observability was required (e.g. the T-maze). However, they are computationally expensive. Consequently, the authors propose to train transformers on the learner, supported by hardware acceleration, but also train a smaller LSTM agent which can be efficiently executed on the actors. ""147\tThis paper discusses the challenges and limitations of reinforcement learning in actor-latency-constrained settings, where the acting policy must be performed within a fixed budget of time (Tactor). In these settings, the model size is limited by latency constraints, which prevent using high capacity models or solution methods that reduce computational cost. The paper highlights the need for specific solutions that can be tailored to the specific requirements of each application area.\tThe paper proposes a solution to actor-latency constrained settings in RL by using policy distillation to compress a large \u201clearner model\u201d towards a more tractable \u201cactor model\u201d. In particular, it proposes to exploit the superior sample efficiency of transformer models while utilising an LSTM-based actor during execution. The proposed procedure, called Actor-Learner Distillation (ALD), provides comparable performance to transformers in terms of sample efficiency, yet produces a wall-clock run-time that's on par with LSTM agents.""148\tThis paper discusses the problem of multi-domain few-shot classification, which is a challenging machine learning task where a small number of examples is required to classify multiple different objects across multiple domains. The paper proposes a method that uses transfer learning to improve the performance of few-shot learning systems by leveraging the collective data of multiple tasks. The paper also discusses the challenges and limitations of using transfer learning in this setting, and suggests some potential future directions for research.\tThe paper addresses the problem of multi-domain few-shot image classification (where unseen classes and examples come from diverse data sources), and proposes a Universal Representation Transformer (URT) layer, which learns to transform a universal representation into task-adapted representations. The method proposed builds on top of SUR [Dvornik et al 2020], where a universal representation is extracted from the outputs of a collection of pre-trained and domain-specific backbones and a selection procedure infers how to weight each backbone for a given task at hand. While SUR inferred those weights by optimising a loss on the support set (the few examples provided in a task), the authors in this paper introduce an attention-based layer (inspired by Vaswani et al Transformer) that learns to weight the appropriate backbones for each task. This layer has the main advantage that it can be learned across few-shot tasks from many domains so it can support transfer across these tasks.""149\tThis paper discusses the problem of multi-domain few-shot classification, which is the task of classify objects into multiple categories, but each category has limited training data. The paper presents a review of the current state of the art in transfer learning and few-shot learning, and discusses the challenges and opportunities that arise when working with multi-domain data. The paper argues that leveraging data across multiple different tasks and domains can improve the performance of few-shot learning systems, even if each individual task has limited training data. The paper also presents several recent advances in the field, including methods for\u805a\u7c7b\u548c\u9884\u5904\u7406\u591a\u4efb\u52a1\u6570\u636e\uff0c and the use of neural networks for transfer learning.\tFew-shot learning on meta-dataset is challenging due to the domain gap between train and validation. In order to bridge this gap, the authors present a model that learns to combine domain-specific representations to generalize to new domains. This combination is done with a transformer model that pays attention to the features extracted from domain-specific backbones. The authors demonstrate empirically that their model attains comparable performance to previous state-of-the-art at higher efficiency and include ablation results to test their model components.""150\tThe paper discusses the problem of multi-domain few-shot classification, which is a challenge faced by machine learning systems when learning from small data. The paper presents a promising direction to address this challenge by developing methods that can perform transfer learning across the collective data of many tasks. The paper argues that few-shot learning systems should benefit from leveraging data across many different tasks and domains, even if each individual task has limited training data available. The paper proposes a variety of methods for performing transfer learning, including model-based and data-based approaches. The paper also discusses the challenges and opportunities associated with developing successful methods for this problem.\tThe paper presents a method for tackling multi-domain few-shot image classification problem where it obtains a task-adapted representation by weighing representations from pretrained domain-specific backbones according to the support set at hand. The desirable property of this framework is that the model can leverage information from other domains to make predictions. The effectiveness of Universal Representations have been discussed in the past work - SUR [1], and this work builds on top of it and introduces a learnable component (self-attention), and showed the improvement both quantitatively and qualitatively.""151\tThis paper discusses the Unsupervised Progressive Learning (UPL) problem, which is a challenging task in machine learning where an agent is presented with a sequence of unlabeled data vectors and is\u7684\u4efb\u52a1 is to learn representations that can be used for classification tasks. The agent is given a small number of labeled examples of one or more classes to associate with the learned representations. The goal is to learn in an online manner, with the set of output classes gradually increasing over time. The paper presents an overview of the problem\uff0c\u63a2\u8ba8\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u5982\u5b66\u4e60\u7387\u3001\u8d85\u53c2\u6570\u7b49\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u7b97\u6cd5\u548c\u6280\u672f\u3002\u6700\u540e\uff0c the paper provides an example of how the UPL problem can be solved using a state-of-the-art machine learning algorithm.\tThe paper introduces a non-parametric approach, STAM, for unsupervised progressive learning (UPL), a variant of continual unsupervised learning with a single-stream requirement. STAM is developed for visual tasks. It comprises several components: (1) online clustering of hierarchical visual features (2) novelty detection (3) dual-memory for prototypical features. Experiments show STAM performs better than GEM, MAS in specific scenarios. ""152\tThis paper presents a challenging problem, known as Unsupervised Progressive Learning (UPL), which is the ability of an algorithm to learn representations of data while unsupervised and gradually expand its knowledge of the classes. The paper explores the possible algorithms and their capabilities for UPL, including support vector machines (SVMs), neural networks, and decision trees. The paper also discusses the challenges and limitations of UPL, such as the limited amount of labeled data and the need for a correct classification algorithm to be chosen. Finally, the paper provides an example of how UPL can be used in a real-world application, where an algorithm is trained on a sequence of unlabeled data and then used to classify new examples.\tThe authors propose an approach (architecture + algorithms) to unsupervised progressive learning in a non-stationary environment (the number of classes grows gradually) by keeping centroids at several hierarchies, using a combination of techniques from online clustering, via computing and updating centroids, with novelty detection, and dropping (forgetting those deemed outliers).  A variety of experiments are performed on several image datasets (MNIST, EMNIST, SVHN, CIFAR-10) with comparisons to other adapted methods. They evaluate performance in a supervised setting where they describe how they learn centroid to label(s) mappings.""153\tThis paper presents an overview of an unsupervised progressive learning (UPL) problem, which is a challenging task in machine learning. The UPL problem involves learning representations of unlabeled data vectors by Progressively learning about the class labels that are hidden from the agent. The goal is to learn in an online manner, where the agent is provided with new examples of different classes as they are generated by the environment. The paper discusses the challenges and limitations of the UPL problem, and presents some recent advances in the field of UPL. The paper also includes an example of how a UPL algorithm can be used to solve a specific problem, such as image classification.\tThis paper presents an \"Unsupervised progressive learning\" (UPL) problem, where a model is exposed to data in an non-iid manner, and each training example is presented once. Simple to continual learning, but a little more explicit in the connections to the way biological agents learn. They present a model that uses clustering and long-term memory (buffered) and compare on a few UPL tasks with additional supervision signal (classification) or unsupervised (clustering).""154\tThe paper discusses the challenges faced by highly over-parametrized deep neural networks and the recent developments in distributed training algorithms for efficient machine learning. It discusses the use of data-parallel large mini-batch SGD and the communication primitive All-Reduce in SOTA training systems, but also highlights the sensitivity of exact averaging with All-Reduce to the communication hardware, which causes a bottleneck in efficient deep learning training. The paper also discusses the need for decentralized training as an indispensable training paradigm for efficient large scale training in the data-center.\tThe authors consider the decentralized optimization problem and explain the generalization gap using the consensus distance. They show that when the consensus distance does not grow too large, the performance of centralized training can be reached and sometimes surpassed. The conducted experiments are extensive and the delivered message is pretty clear -- Critical consensus distance exists in the initial training phase and ensures good optimization and generalization, while a non-negligible consensus distance at middle phases can improve generalization over centralized training.""155\tThis paper discusses the challenges and challenges overcome in developing distributed training algorithms for highly over-parametrized deep neural networks. The paper highlights the importance of using data-parallel large mini-batch SGD and the communication primitive All-Reduce in order to perform efficient and effective training. The paper also discusses the challenges of using exact averaging with All-Reduce in communication hardware-sensitive training systems. Finally, the paper proposes decentralized training as an alternative to distributed training algorithms in order to achieve efficient large scale training in the data-center.\tThis paper studies decentralized gradient methods for training deep networks. It focuses on the so-called \"critical consensus distance\" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny-ImageNet.""156\tThe paper presents a review of the current state of distributed training algorithms for highly over-parametrized deep neural networks. The paper highlights the challenges and opportunities associated with using these algorithms in a data-center environment. The paper discusses the use of data-parallel large mini-batch SGD, the All-Reduce communication primitive, and decentralized training, and highlights the benefits and limitations of each approach. The paper also examines the future directions and challenges of distributed training for deep neural networks.\tThis paper studies the problem of decentralized training where several computing units are used simultaneously to process the data, and computing units are assumed to be connected over a network. The main focus is to better understand the role of consensus, or lack there of, into the generalization abilities of decentralized training. The authors describe an upper bound for dissimilarity of local variables that guarantees the performance of decentralized training is as good as centralized one. Moreover, some heuristic guidelines are proposed to control consensus during training process. Some numerical evidence is also provided.""157\tThis paper discusses the importance of metric learning and its applications in machine learning. It also presents a review of the current state of the art in metric learning, including the use of neural networks for sequence metric learning. The paper also discusses the challenges of designing specific sequence metric learning algorithms and suggests some easy ways to adapt existing approaches toSequential data.\tThis work concerns the metric learning between sequences using RNNs. The paper notices the similarity between a dynamical system and an RNN. Then it demonstrates that learning a pair of siamese RNNs is similar to learning synchronization between two subsystems of a dynamical system. Finally, the paper proposes to introduce coupling between the two RNNs in order to improve synchronization.""158\tThis paper discusses the concept of metric learning and its applications in machine learning. It focuses on how to learn a similarity between sequence examples in a weakly supervised setting, where only equivalence constraints are known. The paper also highlights the importance of using neural networks for sequence metric learning, despite the simplicity of the siamese architecture. It also discusses some of the recent applications of metric learning in various fields, such as person re-identification, object tracking, and gesture recognition.\tDrawing inspiration from dynamic systems, the paper proposes a novel architecture that couple sequences. Such a system has easiness to bring two instances arbitrarily close and authors have shown the superiority of the approach over an action recognition dataset; but the results seem to far from state of the art on the dataset (see questions section). The authors also recognize that currently such systems need to calculate each pairs (can't be cached due to coupling) at inference time, which is slow. ""159\tThis paper discusses the importance of metric learning for machine learning and its applications in various fields. It also explores the challenges of designing specific sequence metric learning algorithms and suggests several ways to adapt existing approaches toSequential data. Metric learning aims to learn a similarity between two or more sequences, which can be used in a variety of applications, including person re-identification, object tracking, gesture recognition, and sentence similarity computation. The paper highlights the benefits of using metric learning in weakly supervised settings where only equivalence constraints between samples are known. It also discusses the challenges of designing specific sequence metric learning algorithms, such as the complexity of the siamese architecture and the need for efficient training algorithms. The paper suggests several ways to adapt existing approaches toSequential data, including learning representations through sequence-to-sequence models.\tI liked the formulation and motivation of the paper, explaining the sequence metric learning problem  and drawing parallel between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by a siamese style recurrent neural network. The authors propose modification the siamese recurrent network setting called classical Gated Recurrent Unit architecture (CGRU). The premise being two identical sub-networks, two identical dynamical systems which can theoretically achieve complete synchronization if a coupling is introduced between them. The authors describe how this model is able to simultaneously learn a similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way with the coupling demonstrating performance of the siamese Gated Recurrent Unit (SGRU) architecture on UCI activity recognition dataset (mobile data).""160\tThis paper discusses the recent improvements in the performance of machine learning models and the most common approaches to scaling up training, including data parallelism and synchronous first-order optimization. It explains how data parallelism is used to increase the number of workers needed for training and how synchronous first-order optimization can be used to ensure that all replicas of a model are processing the same mini-batch at the same time. The paper also discusses the challenges and limitations of both data parallelism and synchronous first-order optimization, and provides future directions for these approaches.\tThis work analyzes the effect of co-distillation for distributed training under moderate batch sizes. Using distillation-like techniques to improve synchronous SGD training is an interesting direction. And the paper carefully analyzed this setting while using the same amount of compute, which is not done by prior work to my knowledge. In addition, the writing is good and easy to follow.""161\tThis paper discusses the recent improvements in the performance of machine learning models and the most common approaches to scaling up training. It emphasizes the use of data parallelism, which involves using multiple workers to compute gradients on different training samples in parallel, as a key factor in improving model performance. The paper also discusses the most popular optimizers used in data-parallel training, including Stochastic Gradient Descent ( SGD ) and Adam. Finally, the paper provides an overview of the challenges and limitations of data-parallel training, including the need for efficient data storage and distributed\u8ba1\u7b97\uff0c as well as the need for proper planning and configuration to ensure successful training.\tThe paper studies the concept of codistillation in data parallel distributed training. In this setting, the standard minibatch SGD algorithm requires exchange of models in every update of every node. Recent work in distributed training has studied \"local SGD\", where models are exchanged at frequent (usually periodic) intervals after a bunch of local updates. This paper studies an alternative, called \"codistillation\". The idea is that at a given node, say node $i$, the local model updates are regularized by the most recent models at nodes $\\{j, j \\neq i\\}$ through an appropriately modified loss term. Specifically, the loss term bias the model at node $i$ towards having similar classification outcomes on the training data as the (most recent) local estimate of the model at nodes $\\{j, j \\neq i\\}.""162\tThis paper provides an overview of recent improvements in the performance of machine learning models and the most common approaches to scaling up training. It highlights the importance of using some form of data parallelism, particularly synchronous first-order optimization, to improve training efficiency. The paper also discusses the challenges and limitations of data parallelism, such as computational resources and the need for careful planning and scheduling. Finally, the paper provides an overview of the latest advances in data-parallel training, including the use of distributed neural networks and parallelized gradient descent.\tThis paper aims to have a closer look at the role of codistillation for distributed training. Authors provided an answer with their empirical observations. That is, codistillation acts as a regularizer, since the distance between the learned model and the initialization is smaller than sync SGD without codistillation. Then, the authors claim that the codistillation may over-regularize and study how to modify the training configurations to avoid it. There are further discussions on the overfitting and robustness to hyper-parameters in sec 4 and sec 5. ""163\tThis paper discusses the learning problem in neural networks and how it can be expressed as an instance of the population risk minimizing problem in statistics. The paper also discusses an approach to solving this problem using empirical risk minimizing strategies, which involve optimizing a cost function based on a training dataset with i.i.d. observations. The cost function may be non-convex in the parameters of the neural network, which makes the problem difficult to solve. The paper provides an overview of the challenges and approaches to solving this problem, and\u8ba8\u8bba\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u601d\u8def\u548c\u89e3\u51b3\u65b9\u6848.\tThis paper studies the relations between the heavy tail phenomenon of SGD and the \u2018flatness\u2019 of the local minimum found by SGD and the ratio of the step size $\\eta$ to the batch size $b$ for the quadratic and convex problem. They show that depending on the curvature, the step size, and the batch size, the iterates can converge to a heavy-tailed random variable.  They conduct experiments on both synthetic data and fully connected neural networks, and illustrate that the results would also apply to more general settings and hence provide new insights about the behavior of SGD in deep learning. ""164\tThis paper discusses the population risk minimizer problem in neural networks, which is a natural generalization of the training problem in neural networks. The problem is to minimize the expected risk, defined as the sum of the individual risks over all data points, subject to the condition that the individual risks are non-convex in the parameters of the neural network. The paper proposes an empirical risk minimizer strategy, which is able to solve this problem by solving an optimization problem that is much easier to solve than the original problem. The paper also discusses the limitations of the proposed strategy and the challenges of solving the population risk minimizer problem in neural networks.\tThe main theme of this work is to study conditions under which SGD iterations result in random variables with heavy-tail random distributions. Specifically they focus on the step size, batch size and problem dimension. First they show theoretical results showing how the tail-index of the distribution generated by SGD depends on the chosen step size, batch size and problem dimension.""165\tThis paper discusses the population risk minimizing problem in statistics, which is a common problem in machine learning, particularly in the context of neural networks. The paper presents an example of how to attack this problem using a training dataset, known as a empirical risk minimizer. The paper also discusses the limitations of this strategy and how other optimization methods can be used in place of it.\tThis paper gives a theoretical study of the tail behavior of the SGD in a quadratic optimization problem and explores its relationship with the curvature, step size and batch size. To prove their results, the authors approximate the SGD recursion by a linear stochastic recursion and analyze the statistical properties by the tools from implicit renew theory. Under this setting, they show that the law of the SGD iterates converge to a heavy-tailed stationary distribution depending on the Hessian structure of the loss function at the minimum and choices of the step size and batch size. They take a further step to clarify the relationship and study the moment bounds and convergence rate. ""166\tThis paper provides an overview of Graph Convolutional Networks (GCNs), a powerful machine learning technique for community detection. It discusses the history of GCNs, their inspiration from graph neural networks (GNNs), and their performance in community detection. It also highlights some of the challenges in using GCNs for community detection, such as over-smoothing and dilute classification information, and proposes some solutions to these issues. Finally, the paper concludes by highlighting future directions and potential applications of GCNs in community detection.\t The article analyzes GCNs from spectral viewpoint, and discusses the performance of GCNs with respect to spectral filtering. The paper shows by experimentation, that the performance of GCNs mainly depend on low frequencies (lower end of the spectrum/eigen-pairs). It then shows that an MLP with low frequency information (Eigen-pairs) performs very well in graph tasks. Aspects such as smoothness and high frequency ablations are also studied.""167\tThis paper discusses the state of the art in community detection using Graph Convolutional Networks (GCNs) and suggests that GCNs may over-smooth their representation, which can lead to poor performance. It also suggests that finding a way to exploit high-frequencies of the graph Laplacian, rather than just filtering through it, may be a good way to improve GCN performance. This is a new perspective that could have important implications for other graph-based tasks.\tThis paper aims to study how GCN will behave under spectral perturbations/manipulations. The empirical numerical analysis on three benchmark datasets (cora, citeseer, pubmed) show that most of the necessary information is contained in the low-frequency domain. Based on that, the author propose to expand the node feature matrix with the eigenvectors corresponding to low-frequency domain and apply MLP on this new feature matrix. Experimental results show that the proposed method outperforms vanilla GCN and achieve comparable results on pubmed with other baselines.""168\tThis paper discusses Graph Convolutional Networks (GCNs), a popular algorithm for community detection in graphs. It reviews the history of GCNs and the state of the art in community detection, and explains how GCNs are able to obtain good performance. It also discusses some potential issues with GCNs, such as over-smoothing and the use of the graph Laplacian for information processing. The paper also proposes new ideas for improving the performance of GCNs.\tThe work presents an interesting analysis of GCN models under spectral manipulations and relates the performance of GCNs through bandpass filtering. The authors demonstrate that GCNs mainly rely more on low-frequencies rather than high-frequencies which is contrary to what is observed in signal processing. For this, the authors use band-pass filters which allow only a portion of the spectrum to be utilized by the GCN model. The major findings are as follows:""169\tThis paper discusses the applicability of graph neural networks (GNNs) to semi-supervised classification problems where a graph structure is not readily available. The paper\u5148\u4ecb\u7ecd graph representation learning and GNNs\uff0c\u7136\u540e\u8ba8\u8bba\u5728\u7f3a\u4e4f graph structure\u7684\u60c5\u51b5\u4e0b GNNs\u7684\u9002\u7528\u6027\u3002\u5b83\u63d0\u51fa\u4e86\u4e00\u4e9b\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec fix a similarity graph between the nodes \u548c\u540c\u65f6\u5b66\u4e60 GNN \u548c graph structure\u7684\u65b9\u6cd5\u3002\u5728\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\uff0c\u4e00\u4e2a\u4e3b\u8981\u7684\u76ee\u6807\u662f\u6784\u5efa\u6216\u5b66\u4e60\u5177\u6709\u9ad8\u7a0b\u5ea6\u6807\u7b7e\u76f8\u4f3c\u5ea6\u7684 graph structure\uff0c\u4ee5\u5e2e\u52a9 GNN \u5206\u7c7b\u3002\u540e\u6587\u5c06\u8ba8\u8bba\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002\tGraph neural networks (GNNs) have become de facto methods for integrating the input graph structure and node features to learn effective node representations. However, in some domains (such as brain signals, particle reconstruction, etc.), there is access to only node features (but not the underlying graph structure). Motivated by the fact that GNNs tend to perform poorly in the absence of the graph structure, the paper""170\tThis paper studies the applicability of graph neural networks (GNNs) to semi-supervised classification problems where a graph structure is not readily available. The focus is on finding a graph structure that exhibits a high degree of homophily with respect to the labels, which can aid in the classification process. existing approaches for this problem either fix a similarity graph between the nodes or learn the GNN parameters and a graph structure simultaneously. The paper\u8ba8\u8bba\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u540c\u65f6\u5b66\u4e60 graph structure\u548c GNN parameters\uff0c\u4ee5\u89e3\u51b3\u8be5\u95ee\u9898\u3002 This new approach is described in detail and shown to be effective in practice. The paper also discusses related work and\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\tThis paper proposes to tackle jointly learning graph structures and GNN parameters without accessing the original graph structure. Specifically, the proposed method adopts a self-supervised auxiliary task, i.e., parallel training using the supervision of node labels and a self-supervised task using de-noised auto-encoding. The latent graph structure is generated through a fully-parameterized adjacency matrix or a KNN construction subsequent to passing node features to an MLP. Experimental results and corresponding analyses demonstrate the effectiveness of the proposed model.""171\tThis paper discusses the applicability of graph neural networks (GNNs) to semi-supervised classification problems where a graph structure is not readily available. The paper presents a review of the existing literature on graph representation learning and GNNs, and discusses the challenges and opportunities in this problem space. The paper also discusses two approaches to solving this problem: (1) fixed similarity graphs and (2) simultaneously learning the GNN parameters and a graph structure. The paper concludes by highlighting the potential of GNNs in this problem space and suggests future directions for research in this area.\tThis paper considers the problem of nodes classification with few labeled data and missing graph structures. The proposed solution is expected to infer unobserved graph structure as well as the parameters of the classification model. The main contribution of this paper is proposing adding a denoise autoencoder layer which provides more supervision to the learning. The model compares favorably with other states of art models in several benchmark graph data sets.""172\tThis paper discusses the challenges of continually learning systems, including the ability to mitigate catastrophic forgetting and the need for supervised and unsupervised learning techniques. The paper also discusses a new unsupervised class-incremental learning problem motivated by the desire to simulate how children's play behaviors support their ability to learn object models. It also provides an overview of the current state of the art in continually learning systems and future directions for research.\tThe authors propose a novelty detection module to help unsupervised class-incremental learning. The novelty detection relies on the percentage of accuracy drop during a model update when treating incoming data as a new class. If the model maintains high accuracy, then the module treats the incoming data as familiar, thereby choosing one of the existing classes as the correct label. The paper investigates the effectiveness of the proposed method on MNIST, SVHN, CIFAR-10, and CIFAR-100.""173\tThe paper \"class-incremental learning for unsupervised learning in\u5e8f\u5217 tasks\" by Stojanov et al. (2019) discusses a new approach to unsupervised class-incremental learning for\u5e8f\u5217 tasks, where the goal is to learn a model of objects in a dataset while also ensuring that the model can perform well on old tasks. The paper\u4ecb\u7ecd\u4e86 an approach to the problem of unsupervised class-incremental learning using a novel class-incremental learning problem motivated by the desire to simulate how children's play behaviors support their ability to learn object models. The approach uses an approach that involves training the model on a small set of images and then adjusting the model's parameters to improve its performance on the remaining images in the dataset. The paper also discusses the challenges and potential applications of this approach.\tThis article proposes a method for predicting whether a batch of data is of the same class as one of the classes already seen by a classifier or whether it contains data from another class. The idea is to then be able to incorporate this batch to the previous training set, in an unsupervised learning context. It is assumed that each batch contains data from only one class. Experiments are there to show the interest of this method for anomaly detection or incremental learning.""174\tThis paper discusses the development of continually learning systems, which are systems that learn new tasks while maintaining the ability to perform old ones, and mitigate catastrophic forgetting. The paper also discusses the challenges of developing such systems and\u4ecb\u7ecd\u4e86 a new unsupervised class-incremental learning problem motivated by the desire to simulate how children's play behaviors support their ability to learn object models. The paper also discusses the potential applications of such systems.\tThis paper proposes to tackle the problem of unsupervised class-incremental learning, where the training data is composed of a sequence of \"exposures\". Each exposure is comprised of a set of images that pertains to a single class, where the class label is unknown while the boundaries between exposures are known. The key difficulty in such unsupervised class-incremental learning is to determine whether an arriving exposure belongs to what the classification model $L$ has learnt previously or is a novel one, thus relating to the problem of novelty detection. The proposed method address the novelty detection by an interesting idea: they always treat the current exposure as a novel class and use it to train the copy of classification model $\\hat{L}$ together with the training exemplars of previously-learnt classes, if the current exposure actually belongs to one of the previous-learnt classes, the confusion occurs to make the classification accuracy significantly decrease (over a threshold) on that specific class, where the accuracy is computed based on the validation exemplars. Moreover, a technique of introducing class-imbalance into such confusion-based novelty detection is proposed and helps to boost the robustness of novelty detection. ""175\tThis paper discusses the challenges and limitations of using reinforcement learning (RL) in real-world scenarios. It explains that while RL has shown promise in simulated domains, it is difficult to deploy in real-world scenarios due to the need to ensure safety of the agent and its surroundings. The paper discusses two main limitations of Safe RL algorithms, which are the need for equal contribution and the requirement for user-input. It also provides an overview of the current state-of-the-art in Safe RL and suggests possible future directions.\tThe paper addresses how to learn policies for tasks in which constraints are specified in natural language. Towards this, the paper proposes a model that encodes the different types of natural language constraints into intermediate representations that model both spatial and temporal information between states. Then, they use this as input along with the observation to produce an action at each time step for a safe trajectory. ""176\tThis paper discusses the challenges of deploying reinforcement learning (RL) in real-world scenarios and proposes safety-based algorithms for handling these challenges. The paper starts by\u4ecb\u7ecdRL\u5728\u6a21\u62df domains\u4e2d\u7684\u8868\u73b0\u4ee5\u53ca\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u786e\u4fdd\u5b89\u5168\u6027\u662f deployment RL \u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u9700\u8981\u6ee1\u8db3\u7684\u91cd\u8981\u6761\u4ef6\u3002Next, the paper presents theSafe RL algorithms, which maximize rewards while minimizing constraint violations during exploration, during analysis and during training. These algorithms do not require equal contribution from all authors, which is a major limitation of previous safety-based RL algorithms. Finally, the paper concludes by discussing the potential applications of Safe RL in real-world scenarios and\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e2a\u8bdd\u9898\u7684\u91cd\u8981\u6027\u3002\tThe paper proposed an algorithm to learn a policy when provided with natural language constraints. The paper defined a navigation task called Hazard World, in which an agent navigation on the map to collect items. The authors defined three types of constraints to restrict agents to visit certain states: 1. budgetary constraints, 2. relational constraints and 3. sequential constraints. The three constraints are described in natural language. The authors proposed a two-step solution. In step one, the algorithm learns a mapping between a natural language constraint to an intermediate representation. In step two the algorithm takes the intermediate representation to learn a policy that satisfy the constraints.""177\tThis paper discusses the challenges and limitations of using reinforcement learning (RL) in real-world scenarios, specifically in domains where safety is a concern. The paper presents an overview of the various Safe RL algorithms that have been developed to address this issue, along with their limitations. It also discusses the challenges of implementing and using these algorithms in real-world applications. The paper\u6700\u540e suggests possible solutions to these challenges and provides an overview of the potential applications of Safe RL in various fields.\tThis paper presents a new test environment, Hazard World, for learning the safe reinforcement learning agents with given natural language constraints. In this problem, the goal of the agent is to find an optimal policy that maximizes the cumulative rewards while satisfying the constraints given in natural language. The authors introduce the model that contains the following two separate components; constraint interpreter for encoding the language constraints and policy network for learning the RL agent. Finally, they report the results of their proposed algorithm and compare it with the baselines.""178\tThis paper provides an overview of semantic edge detection, a method for identifying pixels that belong to boundaries of predefined categories. Semantic edge detection is an important technique for improving the performance of various applications, including semantic segmentation, object reconstruction, image generation, and medical imaging. The paper discusses the history of semantic edge detection, early algorithms that exploited hand-crafted features and local information, and recent advances in deep learning. It also explains the importance of category-aware semantic edge detection, which can improve the accuracy of the method by considering the specific categories that the object belongs to. The paper\u6700\u540e presents the current state-of-the-art in semantic edge detection and discusses future directions.\tThis paper proposes the few-shot edge detection task, which is similar to few-shot segmentation but for the dual task of detecting semantic edges. For the task, the authors construct datasets and experimental settings constructed from existing edge detection dataset (BSD) and a few shot segmentation dataset (FSS). For the proposed method, the authors:""179\tThis paper provides an overview of semantic edge detection, a technique that aims to identify pixels that belong to boundaries of predefined categories. The paper highlights the importance of boundary information for various applications, including semantic segmentation, object reconstruction, image generation, and medical imaging. It also discusses the early algorithms for semantic edge detection, including hand-crafted features and local information, and the significant improvements made using deep learning. Finally, the paper provides an overview of category-aware semantic edge detection, which allows for the recognition of different categories of objects.\tThis paper introduces a novel problem of \"few-shot semantic edge detection\" where semantic boundaries are to be learned/detected with a few labeled samples. In order to remedy the issue of label sparsity within the few-shot scenario, the authors have leveraged the use of the segmentation process which provides the semantic information to the edge detector. They have also incorporated a meta-learning approach, namely Multi-Split Matching Regularization (MSMR), to avoid the overfitting when high-dimensional embeddings are used for feature matching.""180\tThis paper presents a review of semantic edge detection, a technique that aims to identify pixels that belong to boundaries of predefined categories. The paper explores the history of the technology, the current state-of-the-art algorithms, and their applications in various fields. The paper also discusses the challenges and limitations of the technique, including the need for large datasets and the impact of deep learning on the performance of the algorithm.\tThis paper works on few-shot semantic edge detection. Instead of dealing with the problem in a single stage, the authors decompose the problem into two stages.  First, a few-shot segmentation stage, where the foreground and the background probability are estimated via attention with the foreground and the background prototype (averaged feature vector on the foreground and the background region). Second, the feature maps from the encoder are masked by the attention map and sent to the decoder to generate the final edge-map.""181\tThis paper provides an overview of the recent advances in graph neural networks (GNNs) and their application in various domains. It discusses the history of GNNs, the representation learning approach used to incorporate graph structure with node and edge features, the feature attribution technique used to provide post-hoc explanations, and the current state-of-the-art in GNN performance and explainability. The paper also highlights the challenges and future directions in GNNs and their application.\tThis work proposes to explain graph neural networks from a causal effect view. The proposed method, Causal Screening, iteratively adds edges into the explanatory subgraph. The goal is to maximize the $do(\\cdot)$ calculus, which tries to select an edge such that the prediction probability when feeding into GNNs is close to the original prediction. Experimental results show that the proposed method can outperform other comparing methods.""182\tThis paper provides an overview of graph neural networks (GNNs), a powerful machine learning technique that has shown impressive performance in a wide range of tasks, including image and speech recognition, natural language processing, and recommendation systems. It discusses the history of GNNs, the representation learning approach used to incorporate graph structure with node and edge features, the feature attribution technique used to construct explanations for GNN models, and the current state-of-the-art in GNN explainability. The paper also discusses potential future directions in GNNs and the importance of explainability.\tThe paper proposes a procedure for identifying a subgraph $\\mathcal{G}_K$ of a given size $K$ (measured by the number of edges) whose output through the GNN function $f$ is as close as possible to that of the full graph $\\mathcal{G}$. The proposed method is a greedy approach which starts from an empty graph and gradually adds the next edge by minimizing the difference between the outputs using mutual information. Furthermore, to reduce the computational complexity, a node clustering is done on the graph and the attribution is applied first on the edges between $C$ identified clusters and then transferred to all edges.""183\tThis paper provides an overview of graph neural networks (GNNs), a powerful machine learning technique that has shown impressive performance in a wide range of tasks, including social media analysis and\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP). It discusses the representation learning aspect of GNNs, which incorporates graph structure with node and edge features in an end-to-end fashion, and the importance of feature attribution in providing post-hoc explanations for GNN models. The paper also discusses current work in understanding and explaining GNN models, which falls into two categories: (1) decomposing the outcome prediction to graph structures and (2) offering explainable GNN models.\tThe paper introduces a novel method called Causal screening which takes a graph and the prediction made by a GNN, and returns an explanatory subgraph. The method aims to explain GNN models. To be precise, it starts from an empty set as the explanatory subgraph, and incrementally adds the edges, testing them for the individual causal effect.""184\tThis paper discusses the gap between learning-theoretic generalization bounds for highly overparameterized neural networks and their empirical generalization performance, a fundamental mystery in the field. The paper provides an overview of the current understanding of this gap, including the various theoretical and empirical attempts to identify generalization measures. The paper also discusses the importance of improving our understanding of how neural networks perform on unseen data for safety-critical use cases. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c including the development of more efficient and performant network architectures and training methods based on better understanding of generalization.\tThe paper proposes a novel generalisation measure, i.e., measurement that indicates how well the network generalises, based on pruning. The idea is to measure the fraction of the weights that can be pruned (either randomly, or based on the norms) without hurting the training loss of the model. The paper provides thorough discussion of the related methods and motivates the measure in multiple ways. Further, the authors show empirical evidence for the correlation of the pruning robustness to the generalisation ability of networks, based on the paper by Jiang et al., 2019 and dataset (updated with additional models) provided in the paper.""185\tThe paper discusses the gap between learning-theoretic generalization bounds and empirical generalization performance of highly overparameterized neural networks, which is a fundamental mystery to the field. The paper also discusses theoretical and empirical attempts to identify generalization measures that can distinguish models that generalize well from those that do not. The paper highlights the importance of improving our understanding of how neural networks perform on unseen data for safety-critical use cases.\tIn order to understand why deep networks generalize well, this paper proposes \"prunability\" as an empirical measure that can be predictive of the generalization. Prunability is roughly the smallest _fraction_ (i.e., $\\in [0,1]$) of parameters that can be retained, while zeroing out everything else, without increasing the model's training loss by too much. The authors experimentally demonstrate the predictive ability of this measure in three ways.""186\tThis paper discusses the gap between learning-theoretic generalization bounds for highly overparameterized neural networks and their empirical generalization performance. The paper highlights the importance of improving our understanding of how neural networks perform on unseen data for safety-critical use cases. It also suggests a number of generalization measures that can be used to distinguish models that generalize well from those that do not. The paper provides an overview of the existing literature on this topic and suggests further research in this area.\tIn the present work, the authors tackle the highly debated (and sometimes confusing) problem of finding a good simplicity/complexity measure able to predict generalization performance of deep networks. A novel measure called 'prunability' is introduced and compared with some of the many alternatives in the literature. This property measures how networks are able to retain low training loss when a fraction of the weights is set to zero, and is clearly related to common training practices (e.g. dropout) that seems to yield better generalization performance in practice. The experimental settings and the evaluation methods for this new metric are inspired by recent extensive studies on deep networks performance. The authors are able to show that prunability is in fact associated with good generalization and seems able to capture some non-trivial phenomena (double-descent), but they also find it to be inferior to pre-existing (margin based) measures. Moreover, the close relationship to perturbation robustness and flatness measures is investigated, but the results are not fully conclusive.""187\tThis paper explores the potential of learning discrete representations for high-level abstract planning for robots in unstructured environments. It highlights the need for pre-specified abstract states, task representations and transition models in existing methods, as well as the potential of using machine learning to learn these representations. The paper proposes a method for learning these representations using a combination of deep learning and reinforcement learning. The method is then evaluated on a test set of goal-directed tasks and compared to existing methods. The results show that the proposed method is able to learn and perform better than existing methods in both environments and tasks.\tThis paper tackles the problem of long horizon visual planning, with the aim of of being able to plan actions to reach distant goals. This is a well studied problem, and like prior work this method considers the setting where the agent is given an offline dataset of interaction, which it learns from to be able to reach new goals (specified by a goal image). The method first does unsupervised representation learning, where it learns a discrete representation of images (using contrastive predictive coding (CPC) with a discrete latent variable using Gumbel Softmax). Using a set of discrete latent variable, it then builds a set of graphs which connects these discrete states based on the collected experience, and derives a planning procedure to find a path in the graph which reaches the goal. ""188\tThis paper discusses the problem of learning discrete representations for high-level abstract planning in order to solve long-horizon goal-directed tasks in robots operating in unstructured environments. The paper highlights the importance of pre-specified abstract states, task representations, and transition models in these methods, and proposes a new approach to learn these representations using deep learning. The proposed method involves training a neural network on a large dataset of planning examples, using a combination of reinforcement learning and generative adversarial networks. The paper also discusses the potential applications of the proposed method in the field of robot navigation and motion planning.\tThis paper presents a method that combines learning discrete representations together with planning using graph search to solve long horizon tasks from vision. The approach works by generating data via random exploration and trains a representation encoder on this data. This network extracts objects from the observations and passes them through a CNN and shared encoder to generate one-hot encodings; these encodings are concatenated to generate the discrete representation. The encoder is trained via a contrastive learning objective with a similarity matrix that encourages nearby states to share similar encodings, thereby encouraging spatial and temporal abstractions. Next, these representations are combined together with an abstract planner to generate a sequence of waypoints to the goal. This is done by creating a graph of transitions from the collected exploratory data and search within this graph \u2014 this is executed for each encoding at a time with the assumption that the task can be solved by moving each object independently of the others. Finally, a low-level controller is used for reaching the waypoints and final goal, this is done via MPC on an action-conditional predictive model that generates future observations given the current state and goal. The approach is tested on two simple planar planning tasks where the agent has to solve k-object arrangement and open a room with a key respectively.""189\tThis paper discusses the potential of learning discrete representations for high-level abstract planning in order to solve long-horizon goal-directed tasks in robots. It highlights the importance of pre-specified abstract states, task representations and transition models in traditional task and motion planning methods, and suggests that learning these representations may be a promising approach to solving this problem. The paper also discusses some potential challenges and limitations of this approach, such as the need for large amounts of data and the need for complex model architecture.\tThis work presents Discrete Object-factorized Representation Planning (DORP), which learns a discrete representation from videos with an enforced temporal consistency. This representation can then be planned over through a sequence of small alterations to the discrete embedding, which are then executed via MPC. DORB is demonstrated to solve long-horizon tasks and learn representations that consider objects and their properties. ""190\tThis paper discusses the various techniques for reducing the memory requirements of deep neural network models while still achieving state-of-the-art performance. The paper also discusses the challenges and limitations of these techniques, including the computational cost and accuracy loss associated with certain methods. The paper concludes by discussing the future directions and potential applications of these techniques in various fields.\tThis paper basically proposed to learn the quantization bits (precision) in each layer. Specially, weights are constructed with binary representation as $W_s = \\[W_s^1,...,W_s^b\\]$. During training, $W_s^i$ is relaxed to $ \\in \\[0, 2\\]$. And a group sparsity is imposed to all $W_s^i$ for all weights in a layer, leading to certain $W_s^i \\to 0$, thus cancelling the bit allocation in $i$-th. Experimental results is promising.""191\tThis paper discusses the challenges and recent techniques for compressed deep neural network models. The paper starts by introducing the basic concept of deep neural networks and their applications. It then discusses some of the challenges in deploying and processing models on embedded and edge devices. The paper also presents a variety of model compression techniques such as pruning, factorization, and fixed-point quantization, and analyzes their performance and limitations. Finally, the paper concludes by discussing the future directions and potential applications of these techniques.\tThis paper introduces a new method to quantize neural networks in a differentiable manner. Proposed method applies the group lasso on the bit-planes of the weight parameters to let certain LSBs in each layer to be zero-ed out. STE is used to train the binary representation of each bit-plane and the sign of weights during the training. Results demonstrate that the proposed method can achieve higher accuracy and compression ratio compared to previous studies.""192\tThis paper studies various techniques for compression of deep neural network models, including pruning, factorization, and fixed-point quantization. The paper explores the effects of these techniques on the memory and computational requirements of the models and their applications in real-world problems. The paper also examines the trade-offs between compression ratio, performance, and the hardware resources required for implementation. The paper\u6700\u540e discusses the future directions and challenges of these techniques.\tQuantization of weights in DNNs is a very effective way to reduce the computational and storage costs which can enable deployment of deep learning at the edge. However, determining suitable layer-wise bit-widths while training is a difficult task due to the discrete nature of the optimization problem. This paper proposes to utilize bit-level sparsity as a proxy for bit-width and employ regularization techniques to formulate the problem so that precision can be reduced while training the model.""193\tThis paper discusses the issue of robustness of neural networks against gradient based adversarial attacks. It starts by discussing the importance of neural networks and their applications in real-time systems. Then it explains the concept of neural networkquantization, which reduces the capacity of the network while still allowing for real-time applications and inference on resource-limited devices. The paper\u8ba8\u8bba\u4e86\u4e24\u79cd\u5e38\u89c1\u7684Quantization\u65b9\u6cd5\uff1a\u53c2\u6570Quantization\u548c activationQuantization\u3002It also discusses the claim that parameter quantized networks have better robustness against gradient based adversarial attacks, but the paper shows that activation onlyQuantization methods are vulnerable. The paper then analyzes the robustness of both parameter and activation quantized models against adversarial attacks. It shows that the models suffer from issues such as overfitting and underfitting, and that the choice of the quantization method crucially affects the robustness of the model. The paper also provides insights into how to improve the robustness of quantized networks against adversarial attacks.\tThe paper identifies the gradient vanishing issue in the robustness of binary quantized networks. Therefore, it proposes to use temperature scaling approach in the attack generation. It has two methods for the temperature scale: (1) singular values of the input-output Jacobian and (2) maximizing the norm of the Hessian of the loss.""194\tThis paper studies the robustness properties of neural network models that are Quantized in terms of their parameters and activations against gradient-based adversarial attacks. It focuses on the binary neural network (BNN) case, which is an extreme case where both the parameters and the activations areQuantized. The paper finds that both parameter and activation quantized models are vulnerable to gradient-based adversarial attacks, while parameter quantized networks have better robustness against this type of attack. The paper also discusses the limitations of the existing literature on the subject and\u63d0\u51fa\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411.\t**Update**: Thanks to the authors for addressing my comments. As it was pointed out by the authors, temperature rescaling is mostly applicable to non-linear loss functions. For linear loss functions, temperature scaling only linear rescales the gradients. The difference between the proposed PGD++ attack and PGD with linear DLR loss is small (see the author's response to AR4). The improvements are most significant for FGSM but FGSM is not recommended for the robustness evaluation. Given the limited technical novelty and small improvements for linear loss functions, my score remains unchanged.""195\tThis paper discusses the issue of how Neural Networks (NNs)Quantization and its impact on the robustness of the models against adversarial attacks. The paper starts by introducing the concept of NNQuantization, which reduces the precision of the parameters and activations of an NN while still allowing for real-time applications and inference on resource-limited devices. The paper then discusses the recent claims of improved robustness of parameter quantized networks against gradient based adversarial attacks, but also shows that activation only quantized methods are vulnerable. The paper concludes by highlighting the importance of understanding the impact of NNQuantization on the robustness of the models and suggests future research directions.\tThis work starts by questioning the apparent robustness of quantized networks and demonstrates that such robustness is more so a failure of the attack algorithm in picking up the gradient signal. The authors address this by tuning a scalar multiplier applied to the network logits, which doesn\u2019t modify the model\u2019s decision boundary. Through analyzing the Jacobian, two approaches are proposed to determine the scalar $\\beta$ without tuning it by performing the attack. This approach is quite effective on quantized networks and even provides significant improvement on floating-point networks combining with existing attacks like FGSM and PGD. The proposed modification might seem trivial at first, but it constitutes an important factor the community hasn\u2019t taken notice of, to the best of my knowledge.""196\tThis paper discusses the problem of interpretability in neural networks, specifically in the context of RNNs. It highlights the challenges that arise when using the memory mechanism in RNNs to store contextual information, and proposes various solutions, such as using attention-based methods and visualization of the hidden states. The paper also discusses the limitations of these solutions and the need for further research in this area.\tThe authors propose ProtoryNet,  a prototype-based model for paragraph classification that associates each sentence in the paragraph with a relevant prototypical sentence from the training data. The idea is interesting and the ability to decompose sentiment scores over each sentence + find prototypes for each helps to build user understanding of the model prediction. Thank you to the authors for the submission.""197\tThis paper presents a review of the literature on interpretability in deep learning. The paper discusses the challenges of using RNNs in natural language processing and how recent research has focused on improving the interpretability of these models. The paper also discusses the use of attention-based methods, which allow for visualization of theRNN's attention mechanism, as a way to improve the interpretability of RNN models. Finally, the paper highlights some of the recent advances in this field and discusses the future directions for improving the interpretability of deep learning models.\tthis paper presents an RNN sequence classifying model that generates a prototype for each sentence in a paragraph. The generated prototypes help explain the model's prediction. The method embeds each sentence, matches to prototypes, then runs through an LSTM before making a prediction. Experiments found improved accuracy compared to a previous model that generates only one prototype for a paragraph. A user evaluation also found improvement in interpretability.""198\tThis paper discusses the challenges and recent research works in achieving interpretability inRNNs. It highlights the importance of utilizing an attention mechanism in DNNs and proposes several approaches to visualize the decision trees of RNNs. The paper also examines the limitations of using only\u5386\u53f2\u6570\u636e( History data) to train RNNs and suggests that there is a need for additional data, such as human labeled data, to improve the interpretability of RNN models. The paper concludes by suggesting future directions for research in this area.\tThis paper presents ProtoryNet, a framework for text data that classifies and explains the prototypes' results.   The key concept, that is the novelty of the work, is that this framework is based on sentence prototypes, called prototype trajectory in the paper. In particular, instead of working at the entity of the text level, the text is split into sentences and each sentence is analyzed by itself.  The structure of the framework is composed of a layer that encodes a text's sequences, followed by a prototype layer in which is computed the similarity among each sentence and the prototype trajectories. At this point, the sentences are represented in one-hot encoding: for each sentence, there is a bunch of zero and then a one for the most similar sentence prototype. This representation is used for the classification of the sentence, done using an LSTM structure. In this setting, the interpretation is given by exploiting the prototypes matched for the text under analysis. ""199\tThis paper provides an overview of how Hidden Markov models (HMMs) are used for modeling disease progression. HMMs allow researchers to conceptualize complex clinical measurements as originating from a smaller set of latent health states, which can explicitly account for uncertainty or measurement error. The paper discusses the parameters of an HMM, the emission distribution, and how HMMs can be integrated with other models to achieve more complex disease progression predictions. The paper also highlights some of the recent applications of HMMs in the field of disease progression modeling, including HIV patients, Alzheimer's disease, breast cancer screening decisions, and patient response to blood anticoagulants.\tAuthors demonstrated that one can encode the data likelihood function of an HMM using a specialized RNN architecture. Unlike previous work where neurons from different layers were multiplied together, the new encoding strictly followed the classical architecture restrictions of a neural network , i.e. each layer was a weighted sum of the previous layer. Empirically, author showed that the parameter learned by applying gradient descent on the likelihood function is similar to the one that is obtained using the EM algorithm. In addition, authors demonstrated that such formulation enables an application in studying Alzheimer's disease Symptom Progression.""200\tThis paper provides an overview of how hidden Markov models (HMMs) are used for modeling disease progression. HMMs allow researchers to conceptualize complex clinical measurements as originating from a smaller set of latent health states, which can explicitly account for uncertainty or measurement error. The paper discusses the benefits of using HMMs, including their intuitive parameter interpretations and flexibility, and how they have been used to model various diseases and their progression. The paper also provides an example of how an HMM can be integrated with other models to generate more accurate and informative predictions.\tThis paper introduces a novel architecture of recurrent neural network that mimics the working of a standard HMM. In particular, the proposed HMRNN learns model-parameters, which are statistically similar solutions to those of a standard HMM, within a general neural network learning framework. While it is shown the proposed network similarly to the HMM, there are many issues that should be considered critically.   ""201\tThis paper discusses the use of hidden Markov models (HMMs) for modeling disease progression. HMMs allow researchers to conceptualize complex clinical measurements as originating from a smaller set of latent health states, which can account for uncertainty or measurement error. The paper provides an overview of the history of HMMs, the advantages of using them, and some examples of how they have been used for disease progression modeling. It also discusses the challenges and limitations of using HMMs for this purpose, such as the need for large data sets and the need for parameter estimates that are accurate and consistent across different studies. Finally, the paper concludes by discussing the potential future directions of HMMs in disease progression modeling and how they could be integrated with other tools and technologies to provide more comprehensive and effective models.\tThe authors describe an RNN architecture, the HMRNN, which models the log-likelihood in an HMM.  The authors provide theoretical results showing that the HMRNN objective function does, indeed, correspond to the log-likelihood of an HMM.  Synthetic results are presented which compare the learned parameters between the HMRNN and an HMM trained using the Baum-Welch algorithm.  Finally, HMRNN training is augmented and compared to an HMM trained using the Baum-Welch algorithm for the task of Alzheimer's progression prediction.""202\tThe paper discusses the interpretation of single-cell RNA sequencing data in the context of neuronal phenotypes. The authors propose a supervised learning approach called factorized linear discriminant analysis (FLDA) to partition the gene expression data into components corresponding to individual phenotypic characteristics and their interactions. FLDA seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors then apply their approach to a single-cell RNA-Seq dataset of Drosophila T4/T5 neurons, and they show that FLDA can effectively extract relevant information from the data and help to identify specific phenotypic features. The approach has the potential to be used for the analysis of other single-cell RNA-Seq datasets and for the development of new models of neuronal behavior.\tThis manuscript presents a novel dimensionality reduction method, called Factorized Linear Discriminant Analysis. The method starts from a real problem in neurobiology, and tries to link expression levels of neural genes to phenotypes. In particular, the main goal of the proposed technique is to find linear projections of the genes expression which vary maximally with one phenotypical aspect and minimally with the others. The approach is evaluated using a synthetic example and a real case study involving Drosophila T4/T5 cells.""203\tThis paper presents a new method for analyzing single-cell RNA sequencing data in the context of neuronal phenotypes, called factorized linear discriminant analysis (FLDA). FLDA seeks to factorize the gene expression data into components corresponding to individual phenotypic characteristics and their interactions, which can help to identify key features of neuronal phenotypes and make more informed decisions about the study of neurons. The method uses a sparsity-based regularization algorithm to select a few genes that are important to a specific phenotypic feature or feature combination. FLDA was applied to a single-cell RNA-Seq dataset of Drosophila T4/T5 neurons, and the results showed that the method can be used to identify key features of neuronal phenotypes and help to improve the interpretability of single-cell data.\tThis manuscript describes a generalization of ANOVA that is intended to be used in the interpretation of single-cell RNA-seq data. The method requires specification of an orthogonal, discrete categorization of cells, nominally by phenotype.  The method then linearly factorizes the observed gene expression values into features and their interactions, relative to the phenotypic categories.  The factors can then be used to help interpret the categories, especially in conjunction with a regularizer to reduce the number of genes involved in the factors.""204\tThe paper discusses the use of supervised learning methods to interpret gene expression data in the context of neuronal phenotypes. The authors propose the use of factorized linear discriminant analysis (FLDA) to factorize the gene expression data into components corresponding to individual phenotypic characteristics and their interactions. FLDA seeks a linear transformation of gene expressions that varies highly with only one phenotypic factor and minimally with the others. The authors also use a sparsity-based regularization algorithm to select a few genes important to a specific phenotypic feature or feature combination. They apply this approach to a single-cell RNA-Seq dataset of Drosophila T4/T5 neurons and show that FLDA can successfully interpret the data in the context of neuronal phenotypes.\tThe paper studies a very important problem in gene data analysis. The proposed method is technically sound. The method is intuitive in its idea and easy to implement. The results are interpretable. And according to the experimental evaluations, the proposed method is consistent to existing biological observations and could further identify unknown genetic targets. Therefore, it, potentially, has insightful scientific implications. ""205\tThis paper discusses the importance of explainable deep learning and the problems it faces in training models. It also discusses the use of saliency maps, a technique used to represent the importance of each feature in a model's decision-making, in order to improve the interpretability of deep learning models. The paper also highlights the need for researchers and developers to work together to achieve a better understanding of deep learning and how to address the issues caused by explainable models.\tThe paper presents a new saliency map interpretability method for the task of image classification. It considers the saliency map as a random variable and computes the posterior distribution over it. The likelihood measures the predictions of the classifier for an image and its perturbed counterpart. The prior encodes positive correlation among adjacent pixels. Variational approximation is used to approximate the posterior.  ""206\tThis paper discusses the importance of explainable deep learning models and how they can help mitigate problems with training deep neural networks, such as fairness and unintended features. The paper also discusses the use of saliency maps, which represent the importance of each feature in a given data point, in order to obtain an explanation of a deep learning model's decision-making. The paper suggests that models that have gained users' trust through explanation are preferred in practical applications. Additionally, the paper provides an overview of the various approaches to obtaining an explanation of a deep learning model, including backpropagation based methods.\tThis paper proposed a method for generating saliency maps for image classifiers that are stochastic (instead of deterministic). The probabilistic model assumes a saliency map random variable that generates the data with a classifier. The inference is done by variational methods. The paper presents several qualitative examples and a comparison to previous work using the pixel perturbation benchmark.""207\tThis paper explores the importance of explainable deep learning models in practical applications. It explains the increasing importance of Fine Explanations in the context of Deep Learning, and highlights the need for models to be able to classify models based on unintended features, fairness problems, and how saliency maps can be used for interpretability in deep learning. The paper also discusses several approaches to obtain a saliency map, including backpropagation-based methods. The paper concludes by highlighting the benefits of explainable deep learning models in practical applications and suggesting ways to improve the quality of these models.\tThis paper proposes a new interpretability method for image classification networks. It considers a saliency map as a random variable and aims to calculate the posterior distribution over the saliency map.  The likelihood function and the prior distribution are then designed to make the posterior distribution over the saliency map explain the behavior of the classifier\u2019s prediction. Quantitative evaluation on the perturbation benchmark as well as qualitative result show the effectiveness of the proposed method over baselines. ""208\tThis paper provides an overview of the current state-of-the-art in text encoders, including the largescale neural sentence encoders pretrained on massive text corpora, such as Infersent (Conneau et al., 2017), ELMo ( Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2018). The paper also discusses the fairness of these pretrained models, including the issues of overfitting, underfitting, and\u4e0d\u516c\u5e73\u6027 (Li et al., 2019). The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u6539\u5584\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u548c\u6280\u672f.\tThis paper introduces a novel technique for debiasing pretrained contextual embedding models. Their approach trains a 2 layer fully-connected neural network which takes as input the output from the pretrained model and outputs a new, \"debiased\" representation. This model is trained by minimizing the InfoNCE between the representation produced of original sentence and the representation of that same sentence with some tokens replaced with differently-biased tokens (e.g. \"his\" -> \"hers\"). This paper also introduces a regularizer which minimizes the CLUB between the generated representation and a word embedding for a biased token. ""209\tThis paper provides an overview of the recent progress in the field of text encoders, particularly in the context of largescale neural sentence encoders pretrained on massive text corpora. The paper\u8ba8\u8bba\u4e86Text encoders\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u57fa\u672c\u5de5\u5177\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u51e0\u79cd\u4e3b\u6d41\u7684Text encoder\u6a21\u578b\uff0c\u5982Infersent\u3001ELMo\u3001BERT\u548cGPT\u7b49\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4ece\u591a\u4e2a\u65b9\u9762\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3001\u6548\u7387\u3001\u9c81\u68d2\u6027\u7b49\uff0c\u4f46\u516c\u5e73\u6027\u4e00\u76f4\u662f\u7814\u7a76\u7684\u70ed\u70b9\u4e4b\u4e00\u3002\u672c\u6587\u5c06\u8ba8\u8bba\u8fd9\u4e2a\u8bdd\u9898\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e9b\u76f8\u5173\u7684\u5efa\u8bae\u3002\tThis paper studied a debiasing method to remove social bias in pretrained NLP models. The authors proposed to train a neural network which takes the sentence representations of a pretrained NLP model as input and outputs the unbiased representations. The neural network is trained by maximizing the mutual information between a sentence and its \u201ccounterpart sentence\u201d, which is automatically generated by replacing sensitive words by other values (e.g. replacing \u201che\u201d with \u201cshe\u201d). Moreover, the network can be further trained by minimizing the mutual information between the sentence representation and its sensitive word representation. The experiments show that the proposed method can effectively reduce bias while achieving better downstream task performance of the pretrained model.""210\tThis paper focuses on discussing the fairness of pretrained text encoders in natural language processing (NLP). Pretrained text encoders, which are largescale neural sentence encoders pretrained on massive text corpora, have become the mainstream in extracting sentence-level text representations and have shown desirable performance on many NLP downstream tasks. However, the fairness of these models has not been fully studied. This paper proposes a novel approach to evaluating the fairness of pretrained text encoders and provides some useful insights.\tThe paper builds on recent working attempts to debais sentence encoders by considering modified sentences. Sentences are selected that, for example, contain\u00a0gender cueing words, and then swap those words with a predetermined 'opposite' (i.e. man<->woman). The core novelty in the work is to train a lightweight modification the encoding of the sentence and its swap to (a) reduce the distance between the two embeddings, using a contrastive learning objective and (b) reduce the mutual information between cueing words and the new embeddings. Evaluated on a WHEAT style task, modified for sentences, the method performs significantly\u00a0better than existing recent work.\u00a0""211\tThis paper discusses the vulnerability of neural networks to adversarial examples and the efforts to develop robust neural networks with theoretical guarantees. It explains the importance of this issue in safety-critical applications and the various approaches that have been taken to address it. The paper also analyzes the existing literature on defense algorithms and their limitations. It highlights the need for more research in this area to find better robust solutions that can meet the needs of real-world safety-critical applications.\tThis paper proposes an improved sample-wise randomized smoothing technique, where the noise level is tuned for different samples, for certification of robustness. Further, it also proposes a pretrain-to-finetune methodology for training networks which are then certified via sample-wise randomized smoothing. The authors show in experiments on CIFAR and MNIST that combining their training methodology and certification methodology can sometimes improve the average certified when compared to state-of-the-art randomized smoothing techniques Smooth-Adv (Salman et. al, 2019).""212\tThis paper provides an overview of the literature on training robust neural networks and certifying network robustness with theoretical guarantees. The vulnerability of neural networks to adversarial examples has been noticed in safety-critical scenarios, and many heuristic defense algorithms have been developed to address this issue. However, many of these algorithms lack theoretical guarantees of robustness and have been broken in recent studies. The paper discusses the challenges of training robust neural networks and proposes a number of new algorithms with theoretical guarantees of robustness.\tThis paper suggests an extension of randomized smoothing, wherein the degree of smoothing is optimized both at training and test-time on each individual sample. At training time, the model is first \"pre-trained\" using a range of smoothing parameters (variance of the Gaussian perturbations), and then \"fine-tuned\" by selecting the variance on each sample which maximizes the verified radius. At test time, we can again select the smoothing parameter to maximize robustness.""213\tThis paper discusses the vulnerability of neural networks to adversarial examples and the efforts to develop robust neural networks with theoretical guarantees. It provides an overview of the existing literature on this topic, including the development of heuristic defense algorithms and empirical approaches to\u4fdd\u62a4\u81ea\u5df1\u7684\u795e\u7ecf\u5143\u514d\u53d7 adversarial \u653b\u51fb\u3002 The paper also discusses the limitations of existing algorithms and the need for new ones that provide theoretical guarantees of robustness\u3002 The paper ends by highlighting the importance of\u7ee7\u7eed\u63a2\u7d22\u8fd9\u4e2a\u9886\u57df\uff0c\u4ee5\u627e\u5230\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u795e\u7ecf\u5143\u514d\u53d7 adversarial \u653b\u51fb\u3002\tThis paper considers the problem of provably defense to adversarial perturbations using randomized smoothing. The authors propose sample-wise randomized smoothing -- assigning different noise levels to different samples. They also propose to first pretrain a model and then adjust the noise for higher performance based on the model\u2019s outputs. Experiments show that proposed approach improves the performance of randomized smoothing with same noise level for small perturbations. ""214\tThis paper presents a novel approach to automated data cleaning that involves usingVariational techniques to fill in missing values, correct noisy samples, and perform other tasks commonly performed during the pre-processing phase of data modeling. The paper argues that traditional data cleaning methods are practical only in small to medium-sized datasets and that the increase in data collection and storage rates in recent years has made it increasingly challenging to use traditional supervised algorithms in large-scale applications. The paper proposes a novel variational framework that uses machine learning techniques to automate the data cleaning process and achieve improved accuracy and performance. The framework is designed to work well with a wide range of data sources and can be easily integrated with existing machine learning models.\tThis paper proposes a Tomographic auto-encoder (TAE) for unsupervised recovery of corrupted data. More specifically, TAE takes a Bayesian approach to recover the posterior distribution of a clean image conditioned on an observed corrupted image and thus effectively modeling uncertainty in data recovery. The paper argues that a naive application of VAE is not effective due to the latent variable collapse, and proposes an alternative model where hierarchical latent variable models are used for both prior and variational posterior. Some tricks are introduced to facilitate the stochastic gradient variational inference. ""215\tThis paper presents a novel approach to automated data cleaning, called the Variational Data cleaning (VDB) framework. The VDB framework involves usingVariational inference to fill in missing values, correct noisy samples, and perform other tasks in a data pre-processing stage. It does not require any supervised learning algorithm or human supervision, making it practical for large-scale data analysis. The VDB framework is based on the idea of usingVariational inference to learn a flexible model that can capture the underlying structure of the data, rather than rely on a fixed model fit through supervised learning. This allows the VDB framework to handle complex relationships and missing data in the data, while still being able to make accurate predictions. The VDB framework is demonstrated on a real-world dataset and shown to be effective in cleaning and analyzing large data sets.\tPractical datasets often come with corruptions, such as missing items or noisy observations, thus needs models enable to recover the corrupted data automatically. This paper presents the tomographic auto-encoder (TVAE), which conducts inference over the data space $x$. Because the prior regularization acts over the data space, TVAE is enforced to generate diverse samples from the corrupted observations. Empirically, the paper demonstrates that TVAE can indeed generate diverse samples and can achieve superior test ELBO compared to the previous baselines.""216\tThis paper discusses the challenges of data pre-processing and the need for automation in the analysis of large data sets. It explains the importance of expert human supervision and manual adjustments in the pre-processing of data, and how filling missing entries, correcting noisy samples, filtering collection artefacts and other similar tasks are some of the most costly and timeconsuming stages in the data modeling process. The paper also presents a novel variational framework for automated data cleaning that overcome these challenges. The framework uses a combination of machine learning algorithms and natural language processing to identify and correct issues with the data, without the need for human expertise. The paper also discusses the potential applications of this framework, including the use of data cleaning algorithms in machine learning models and the use of the variational framework in data-driven decision making.\tThis paper proposes a novel approach to handle the recovery of dirty data in fully unsupervised scenarios. The corrupted data considers both missing data and noisy samples. They derive a VAE model with a novel reduced entropy condition inference method that results in richer posteriors. This is a very challenging problem, since the model cannot use clean examples as part of their training procedure.""217\tThis paper discusses the development of Graph Attention Networks (GAT) and related models, which provide a self-attention mechanism for Graph Neural Networks (GNNs) to improve the performance of graph processing tasks. GAT allows for the computation of attention scores between nodes connected by edges, allowing the model to attend to messages of node's direct neighbors according to their attention scores. However, this means that a node can only attend to its immediate neighbors to compute its next layer representation, which limits the receptive field of a single GNN layer to one-hop network neighborhoods. The paper also discusses the challenges and limitations of using self-attention in GNNs, and proposes various ways to overcome these challenges.\tThis paper proposes MAGNA, a multi-hop self-attention mechanism for attention based graph neural networks. The proposed method increases the receptive field at each layer, requiring less layers to achieve a large receptive field. Also, with the proposed method the attention coefficient between two nodes is not just a function of the two nodes but also of their neighbourhood. The proposed MAGNA method is an extension of GAT networks that introduces a diffusion step on the computed attention coefficients, following a similar approach (Diffusion-GCNs) that has been used for GCNs.""218\tThis paper discusses the use of self-attention mechanism in Graph Neural Networks (GNNs) for graph presentation learning and other applications. The self-attention mechanism is a powerful tool that has enabled state-of-the-art performance in many domains, including graph presentation learning. Graph Attention Network (GAT) and related models, such as Veli\u010dkovi\u0107 et al. (2018) and Li et al. (2018), have developed a self-attention mechanism for GNNs, which allows for the attention of nodes to be computed between edges. However, this means that a node can only attend to its immediate neighbors to compute its next layer representation. This implies that the receptive field of a single GNN layer is limited to one-hop network neighborhoods. The paper also discusses the challenges and limitations of using self-attention mechanism in GNNs, such as the need for additional parameters and the issue of overfitting.\tThe authors propose a novel attention-based GNN called MAGNA. The main contribution consists in considerably increasing the receptive field by considering a multi-hop neighborhood instead of the standard one hop. The technical challenge consists in obtaining attention scores for all relevant nodes in an efficient way. MAGNA solves this by using a diffusion-based technique combined with a geometric distribution. The authors show that the latter further allows for approximations, and also give interesting theoretical insights (e.g., show a relation to page rank). ""219\tThis paper discusses the use of self-attention mechanism in Graph Neural Networks (GNNs) for graph presentation learning and other domains. The self-attention mechanism is a powerful tool that allows the network to focus on the important nodes and edges in the graph. The paper explains how Graph Attention Network (GAT) and related models implement this mechanism in GNNs, and how it enables the network to learn a more compact and efficient representation of the graph. However, the self-attention mechanism is also limited by the fact that it can only attend to its direct neighbors, which implies that the receptive field of a single GNN layer is limited to one-hop network neighborhoods. The paper also discusses the challenges and limitations of using GNNs for graph presentation learning and other applications, and\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848 for these issues.\t     Conventional Graph Neural Networks (GNNs) learn node representations that encode information from multiple hops away by iteratively aggregating information through their immediate neighbors. Self-Attention modules have been adopted to GNNs to selectively aggregate information coming through the immediate neighbors at different propagation stages. However, current self-attention mechanisms are limited to only attend over the nodes' immediate neighbors and not directly over their neighbors that are multiple hops away. Here in this work, the authors intend to address this issue and propose a means to obtain attention scores over indirectly connected neighbors. ""220\tThe paper discusses the issue of how well NLP models are doing on recent benchmarks while still being unable to achieve human-level performance in overall language understanding. The paper points out that the performance of these models on certain tasks is still far below human levels, which suggests a disconnect between the capabilities of these models and their actual performance. The paper proposes the General Language Understanding Evaluation benchmark (GLUE) and the SuperGLUE benchmark with more difficult tasks as ways to evaluate the performance of NLP models and address this issue. The paper also suggests that an array of commonsense benchmarks should be used to measure basic reasoning abilities in language understanding.\tThis paper describes a dataset consisting of ~14k multiple-choice questions drawn from many different fields across the humanities and science as well as professional disciplines such as law and medicine. It presents results for GPT-3 models (LMs trained on text corpora with document context) of different scales, as well as for the UnifiedQA model (seq2seq model trained on various QA datasets). Performance of these models is well below their performance on other benchmarks: not above chance for the smaller GPT-3 models, and under 50% average accuracy for the best models.""221\tThe paper discusses the issue of how well NLP models are performing on recent benchmarks compared to human-level performance for overall language understanding. The paper suggests that there is a disconnect between the performance of these models and the benchmarks used to evaluate them. The General Language Understanding Evaluation benchmark (GLUE) was introduced in 2018 and top models achieved superhuman performance within a year. To address the shortcomings of GLUE, the SuperGLUE benchmark with more difficult tasks was designed in 2019. Since the release of SuperGLUE, performance has again been essentially human-level. The paper also proposes that there is a need for more commonsense benchmarks to measure basic reasoning abilities.\tThis paper focuses on coming up with 57 different tasks and measure the performance of these large scale transformer models such as GPT3 on these different tasks. The main claims of this paper are to demonstrate these large-scale models still struggle to use the knowledge it has learned during the pretraining phase and these models struggle to on calculation-intensive tasks. Further one of the more important contributions of this work includes the massive multi-task dataset that comprises 57 different subjects.""222\tThe paper discusses the recent advances in NLP models and the problems with using traditional benchmarks to evaluate their capabilities. It proposes the SuperGLUE benchmark with more difficult tasks as a better measure of overall language understanding than just performance on a specific benchmark. It also suggests that the performance of NLP models on a benchmark does not necessarily reflect their capabilities for a particular task.\tThe paper proposes a benchmark for NLP models. The purpose of this test is to measure the model's knowledge in 57 topics covered by approx 15000 tasks in total, each formulated as a closed-form question in zero-shot and few-shot settings. Most of the tasks were taken from different human examination sets. Then, the paper provides results of experiments with the latest (GPT-3 and T5 based) models along with some quantitative and qualitative observations.""223\tThis paper discusses the importance of tabular data and the use of natural language interfaces for accessing and analyzing it. It also highlights the recent advances in the field of semanticParsing, a task that maps natural language queries over tabular data to formal programs. Pre-trained language models such as BERT and RoBERTa have achieved tremendous success on this task, shift the focus from building domain-specific semanticparsers to more general-purpose tools that can handle a wider range of tasks.\tThis work explores a pretraining strategy (similar to https://arxiv.org/abs/1606.03622) to the problem of table question answering. More specifically a synchronous context-free grammar (SCFG) is first learned from training data (with manual alignment of entities/phrases). Then the SCFG is used to generate more full supervision data for Roberta model pretraining. The training objective is a combination of two parts: SQL Semantic Precision (SSP) predicts elements in SQL given the question on the synthetic data, and masked-language modeling (MLM) on the natural (training) data.""224\tThis paper discusses the importance of tabular data and the need for a natural language interface to make it more accessible for non-technical users. It also discusses the recent advances in the field of semanticParsing, which involves mapping natural language queries over tabular data to formal programs. Pre-trained language models such as BERT and RoBERTa have achieved great success on a spectrum of natural language processing tasks, including semanticParsing. This has shift the focus from building domain-specific semantic parsers to more general-purpose tools that can process natural language queries over tabular data.\tThe paper provides a interesting direction in pre-training for table semantic parsing. In particular, it proposes to first collect a collection of pseudo question-SQL pairs in an automatic way, based on tables from WikiTables tables and tables and databases in the training sets  SPIDER and WIKISQL. After that, masked language modeling and a newly introduced task called SSP, which is, given a natural language sentence and table headers, to predict whether a column appears in the SQL query and what operation is triggered. Experiments on Spider and WikiSQL show that the model achieves new state-of-the-art.""225\tThis paper discusses the importance of tabular data and the need for a natural language interface to make it more accessible for non-technical users. It also discusses the recent advances in the field of semanticParsing, which involve using pre-trained language models (LMs) such as BERT and RoBERTa to perform the task of mapping natural language queries over tabular data to formal programs. The paper also highlights the challenges and future directions in this field.\tThis paper presents a general-purpose pre-training approach for jointly encoding utterances and relational tables in the task of table semantic parsing, where a natural language utterance is transduced into an executable query (e.g., SQL) over relational database tables. A core challenge in table semantic parsing is to understand the compositional semantics in utterances, and further ground salient entities and relations in the utterance onto the corresponding tabular schema (e.g., columns, cells). To improve understanding and grounding of compositional utterances, the authors propose fine-tuning a pre-trained masked language model (RoBERTa) using linearized table headers paired with synthetic utterances generated from a synchronous context free grammar, via an objective that encourages the model to discover the syntactic roles of columns mentioned in the input utterance. Experiments over four datasets demonstrated strong results when the this newly proposed pre-training objective is combined with classical masked language modeling objective.""226\tThis paper provides theoretical insights into multi-task and transfer learning methods by studying the tractable least-square support vector machine (LS-SVM) multi-task learning (MTL) method in the limit of large and numerous data. By applying a random matrix analysis to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge to a deterministic limit involving simple small-dimensional statistics of the data. The paper also provides a simple method to correct biases in the standard MTL LS-SVM algorithm and reveal the sufficient statistics at play in the method, which can be efficiently estimated even for quite small datasets. The findings are used to automatically optimize the performance of MTL methods.\tThis paper provides a theoretical analysis of the inner workings of multi-task learning methods, based on a random matrix analysis applied to Gaussian mixture data model. The analysis is based on MTL LS-SVM with data from a Gaussian mixture model, where the bias of MTL LS-SVM is shown and a simple method is proposed to correct it. Experiments are conducted on a synthetic dataset and image classification task, where superior performance is shown in addition to the theoretical guarantees.""227\tThis paper provides theoretical insights into the inner workings of multi-task and transfer learning methods by studying the tractable least-square support vector machine (LS-SVM) multi-task learning (MTL) method in the limit of large and numerous data. The paper shows that the performance of MTL LS-SVM convergences to a deterministic limit involving simple small-dimensional statistics of the data. The analysis also reveals that the sufficient statistics can be efficiently estimated even for quite small datasets. The paper uses this result to automatically optimize the performance of MTL LS-SVM. The paper provides insights into the trade-offs between task complexity, data size, and computational resources required for MTL methods.\tThe paper provides interesting theoretical insights in multi-task learning using common and specific parameters modeling framework and based on least-squares SVM. Especially, it is theoretically established that the standard MTL LS-SVM is biased. Thereon a method derived from the analysis is proposed to correct the bias and allows to achieve enhanced performances. Empirical evaluations highlight the effectiveness of the method.""228\tThis paper provides theoretical insights into the inner workings of multi-task and transfer learning methods by studying the tractable least-square support vector machine (LS-SVM) multi-task learning (MTL) method in the limit of large and numerous data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge to a deterministic limit involving simple small-dimensional statistics of the data. The paper also prove that the standard MTL LS-SVM algorithm is in general strongly biased and may fail, and reveal the sufficient statistics at play in the method. The latter result is exploited to automatically optimize the performance of the MTL method for small datasets. The paper also provides insights into the trade-offs between the complexity of the method and its performance, and discusses potential applications of the MTL method in various fields.\tThe paper considers the multitask least-square SVM problem. Such a problem consists of k SVM tasks, each being a binary classification problem. The normal vector of the separating hyperplane in each task is \u201cclose\u201d to each other, reflecting the commonality of the tasks. For an input data point, the problem asks to predict the classification of the input data point for a given task. This problem has a standard optimization formulation.""229\tThis paper discusses the role of data symmetries in deep neural networks and the applications of incorporating group symmetries into neural networks. It focuses on convolutional neural networks and how group symmetries can be incorporated into them to solve dynamic modeling problems and infer the action of the group. The paper also provides an example of how Hamiltonian dynamics can be solved using a convolutional neural network with assumed symmetries. Finally, the paper suggests that incorporating data structure symmetries into models as inductive bias can reduce the model complexity and improve performance.\tThis paper addresses an important symmetry in meta-learning.  Namely, the context data consists of a set of datapoints in arbitrary order.  The model should thus be permutation equivariant to their order.  At the same time, the data itself may have its own symmetries, e.g. rotation, which the network should likewise be equivariant to.  The authors follow a theory-driven approach, proving in Thm 2 that a function with these two types of symmetries may be factored and represented by a composition of functions reflecting each symmetry individually.  They then design a Neural Process (NP) model, EquivCNP, which reflects this result.  Other works have used permutation equivariance and translation equivariance in NPs, but this is the first to incorporate other symmetry groups. ""230\tThis paper discusses the role of data symmetries in deep neural networks and how they can be incorporated into neural networks to improve their performance. The paper starts by introducing the convolutional neural network and its translation equivariance, which allows it to preserve the symmetries of the translation group. It then discusses various studies that have aimed to incorporate group symmetries into neural networks, particularly convolutional operations. These studies have used various methods to incorporate the symmetries into the neural network, such as assuming the symmetries in the latent space inferred by the network or using inductive bias. The paper also discusses some applications of these methods, such as solving dynamics modeling problems and estimating the action of the group. Finally, the paper concludes by discussing the potential limitations and future directions of these methods.\tThe paper provides an extension of convolution conditional neural processes CNPs to more general Lie group equivariant CNPs. The development of the theory seems sufficiently clear to someone more familiar with the field. However, for newer readers, it seems important to be familiar with background concepts and prior work. This is not a penalizing point but rather just an observation.""231\tThis paper discusses the role of data symmetries in deep neural networks and how they can be incorporated into convolutional neural networks to improve their performance. The paper\u5148\u4ecb\u7ecd\u4e86\u6570\u636eSymmetry\u7684\u6982\u5ff5\uff0c\u5373\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5 preserve the symmetry of the translation group\uff0c\u5305\u62ecConvolutional Neural Network(CNN)\u3002\u7136\u540e\uff0c\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e9b studies \u65e8\u5728\u5c06\u5404\u79cdgroup symmetries \u6dfb\u52a0\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u7279\u522b\u662fConvolutional operation( Cohen et al. ,2019;Defferrard et al. ,2019;Finzi et al. ,2020)\u3002\u6587\u7ae0\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5e94\u7528\uff0c\u4f8b\u5982\u7528\u4e8e\u89e3\u51b3\u52a8\u529b\u5b66\u5efa\u6a21\u95ee\u9898\uff0c\u5e76\u4f7f\u7528Hamiltonian dynamics( Greydanus et al. ,2019;Toth et al. ,2019;Zhong et al. ,2019)\u3002\u6700\u540e\uff0c\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4f7f\u7528data structure( symmetries)\u4f5c\u4e3ainductive bias\uff0c\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u548c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\u3002\tThis paper presents EquivCNP which extends Conditional Neural Processes (CNP) to incorporate symmetries of the data, e.g. rotation and scaling. The approach utilizes a combination of LieConv (Finzi et al., 2020)  and DeepSet (Zaheer et al., 2017) to achieve the equivariance in the data space and permutation invariance across the samples in a dataset. They provide empirical results on a 1D regression task with synthetic and 2D image completion tasks using digital clock digits dataset which they constructed.""232\tThis paper discusses the problem of using autoregressive models to generate text, specifically how the training and evaluation objectives can differ greatly from one another, leading to undesired generation. The paper also notes the existence of an exposure bias problem, where the model conditions on the model-generated history at inference time, leading to lower output quality than would be expected based on the gold history/prefix. The paper proposes several solutions to these issues, including using more advanced decoding algorithms and changing the evaluation objective to better reflect the quality of the generated text.\tThis paper introduces a new optimization method for text generation that improves upon directly optimizing MLE. It frames text generation learning as an off-policy reinforcement learning (RL) problem using demonstrations (the training examples). The authors also discuss why off-policy learning is more suitable for text generation than on-policy learning. After simplifications and approximations, the proposed optimization objective comes down to a form that is similar to MLE, but upweighs training examples that are more likely under the learned policy $\\pi_\\theta$ (\"easy\" examples) and having higher estimated rewards (considering the future).""233\tThis paper discusses the limitations of using autoregressive models for text generation, specifically highlighting the existence of two discrepancies between the training and evaluation objectives, and the exposure bias problem. The paper begins by\u4ecb\u7ecdautoregressive models, which are used to generate text by modeling the history of previous words in a sequence. It then discusses the training loss and evaluation objective, which are used to guide the model's development and evaluation. The paper also highlights the problem of over-fitting when using maximum likelihood estimation (MLE) to learn an autoregressive model. Finally, the paper proposes several solutions to these issues, including carefully selecting the decoding algorithms, using other evaluation metrics for evaluating the quality of the generated text, and using additional data for training the model.\tThis paper proposes a method to train generative models of text using reinforcement learning from off-policy demonstrations. This helps solve the problems of exposure bias and mismatched objectives in standard learning schemes such as maximum likelihood estimation and policy gradient optimization on metrics like BLEU. In the proposed method (GOLD), the authors use policy gradient combined with importance weighting to train the model using just the off-policy demonstrations, i.e. human-written text. They experiment with three different reward formulations, and demonstrate improvements over MLE baselines on tasks like summarization and machine translation. ""234\tThis paper explores the limitations of using autoregressive models for text generation, specifically highlighting two main discrepancies between training and evaluation objectives: the negative log-likelihood training loss, and the exposure bias problem at inference time. The paper also discusses how to address these issues in practice. The paper concludes by suggesting alternative approaches to text generation that overcome these limitations.\tThis paper formalizes training of conditional text generation models as an off-policy RL objective, specifically, in the limit case where samples are only obtained from the training data. The motivation behind this is that MLE objective optimizes recall -i.e. increasing the prob of all correct sequences that could be generated by humans as an output to a certain input context. While for certain tasks such as MT or summarization it is often sufficient to focus training to generate 1 single correct Translation or summary (see cons: for comments on the effect of this on sample diversity).  Therefore explorations in traditional PG by generating examples from an auto-reg model is unnecessary in this context. Therefore, using importance sampling, PG objective \\E_{x \\sim \\pi_\\theta} is modified to \\E_{x \\sim \\pi_{data}} with the incorporation of the importance ratio and given a uniform sample probability of the training examples. As shown in eq(4), The gradient updates from this loss are identical to the standard MLE loss reweighted by the global reward and the current model probability of the training example, enforcing the model's current belief of the training data. This aligns with the previous intuition of enhancing precision on the expense of recall. Given this formulization this work experiments with three types of rewards: a constant reward and 2 MLE based rewards.    ""235\tThis paper discusses the use of end-to-end (E2E) back-propagation for training deep neural networks. E2E back-propagation involves\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5728\u6240\u6709\u7f51\u7edc\u5c42\u4e0a\uff0c\u7136\u540e\u9010\u5c42\u4f20\u64ad\u68af\u5ea6\u4ee5\u66f4\u65b0\u6743\u91cd\u3002\u5c3d\u7ba1\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f46\u5b83\u53ef\u80fd\u9762\u4e34 memory and computation inefficiencies\u7684\u95ee\u9898\u3002\u9996\u5148\uff0c\u9700\u8981\u5b58\u50a8\u6574\u4e2a\u8ba1\u7b97\u56fe\u4ee5\u53ca\u5927\u591a\u6570\u751a\u81f3\u6240\u6709\u5c42\u7684\u6fc0\u6d3b\u503c\uff0c\u5bfc\u81f4\u5185\u5b58\u5bc6\u96c6\u578b\u3002GPU \u5185\u5b58\u9650\u5236\u901a\u5e38\u662f\u4e00\u4e2a\u74f6\u9888\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u8db3\u591f\u5927\u7684\u6279\u91cf\u5927\u5c0f\u7684\u4f18\u79c0\u6a21\u578b\u3002\u8fd9\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u4f8b\u5982 2D/3D \u8bed\u4e49\u5206\u5272/\u7269\u4f53\u68c0\u6d4b\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u5b66\u5f71\u50cf\u548c remote sensing \u4e2d\u7684\u5bf9\u8c61\u8bc6\u522b\uff0c\u5bfc\u81f4\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002\tThe paper proposes a strategy for training feed-forward networks in a more memory-efficient manner by employing local as opposed to end-to-end supervision. End-to-end/global (E2E) supervision as the dominant paradigm in training deep networks considers a loss function at the very end of the network for backpropagation of the resulting gradients, whereas local supervision injects supervisory signals (such as the same E2E objective, e.g. classification) at intermediate layers in the network. The benefit of such intermediate supervision is the ability to train larger networks in smaller chunks piece by piece, where each individual training is more memory efficient due to reduced need to store activations (and weights and biases) in GPU memory. As a drawback, however, it had been shown earlier that such local training is less optimal than global training in terms of the achievable generalization performance. The authors propose a new training strategy that aims at combining the memory efficiency of local supervision and piecewise training with the error performance of global training. Considering a given intermediate layer, the paper motivates to maximize the mutual information between the activations in this layer and the input signal to retain relevant information, while minimizing the mutual information of the activations and a nuisance variable, where the nuisance is defined as having no mutual information with the target variable (e.g. the classification prediction). The authors argue that this local supervision allows to train the features at the intermediate layer such that they carry relevant information from the input to the target variable without resorting to direct supervision with the target variable. Direct computation of the nuisance variable is infeasible and the authors propose a bounded approximation. ""236\tThis paper discusses the standard paradigm for training deep neural networks, called end-to-end (E2E) back-propagation. It explains that E2E back-propagation is effective for training deep networks, but it may suffer from memory and computation inefficiencies. The paper also discusses the limitations of using the GPU memory as a bottleneck in training state-of-the-art models with high-resolution inputs and sufficient batch sizes, which is a common issue in real-world applications.\tThe paper analyzes the pitfalls of locally supervised learning from the point of view of information propagation and proposes a new auxiliary loss that can facilitate locally supervised learning. The proposed loss, \"infopro loss\", is then relaxed to a tractable upper bound, which is then used instead. To implement the loss, mutual information is approximated with a decoder, as well as a classifier. The authors further introduce now contrastive learning fits in the framework as a lower bound maximization process regarding mutual information. The experimental results on standard datasets demonstrate the efficacy of the proposed method.""237\tThis paper discusses the standard paradigm for training deep neural networks, known as end-to-end (E2E) back-propagation. The paper\u8ba8\u8bba\u4e86E2E back-propagation\u7684\u57fa\u672c\u6982\u5ff5\u548c\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5305\u62ec\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u3001\u4f20\u64ad\u68af\u5ea6\u3001\u4fdd\u5b58\u8ba1\u7b97\u56fe\u548c activations\u7b49\u6b65\u9aa4\uff0c\u4ee5\u53caGPU\u5185\u5b58\u7ea6\u675f\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\u3002E2E back-propagation\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u4f46\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\u5176\u4e2d\uff0c\u4fdd\u5b58\u6574\u4e2a\u8ba1\u7b97\u56fe\u548c\u5927\u591a\u6570\u5c42\u7684\u8ba1\u7b97\u7ed3\u679c\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u5bfc\u81f4GPU\u5185\u5b58\u4e0d\u8db3\u6210\u4e3a\u9650\u5236\u8bad\u7ec3\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u8db3\u591fbatch\u5927\u5c0f\u7684\u74f6\u9888\uff0c\u5e38\u89c1\u4e8e2D/3D semantic segmentation/object detection\u3001 tissue segmentation in medical imaging\u548cobject recognition from remote sensing\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u3002\u56e0\u6b64\uff0c\u89e3\u51b3\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u4ecd\u7136\u662f\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\tThis paper analyzed the reason why locally supervised training led to performance degradation. And based on the analysis, the author proposed the information propagation loss (can be understood as the combination of a classification loss and a reconstruction loss) aiming to prevent information collapse. Equipped with the proposed method, 40% memory footprint can be reduced demonstrated by their experiments, which is surprising. ""238\tThis paper provides an overview of graph neural networks (GNNs), their performance on graph-structured data, and the research problems related to their design and analysis. It also discusses the importance of improving the expressive power of GNNs and designing new models with higher expressive power. The paper starts by providing an introduction to graph neural networks and their history. Then, it discusses the performance of GNNs on a variety of graph-structured tasks, such as node classification and graph classification. It also highlights the benefits of using GNNs for these tasks and the challenges that arise when using them. The paper ends by discussing the research problems related to GNNs, such as improving the expressive power of existing models and designing new ones with higher expressive power.\tIn this paper, the authors propose to sample nodes of a given graph multiple times to form a set of K sub-graphs. GNNs are then applied on each sampled graph for learning node representations. For each node, all representations are combined for the downstream tasks. The idea of doing multiple sampling is to increase the chance that nodes with different neighborhoods can be more different in the set of sampled graphs, than only considering the original single graph. In contrast, nodes with same neighborhood will remain the same over all sampled graphs. This mechanism helps discriminate node representations better.""239\tThis paper presents an overview of graph neural networks (GNNs), a type of neural network that has been successfully used for graph representation learning and many other tasks, such as node classification and graph classification. The paper also discusses the importance of analyzing and improving the expressive power of GNNs and designing new GNNs with high expressive power. Xu et al. provide an analysis of the expressive power of GNNs and show that it depends on the neighborhood aggregation function. They develop a simple approach to improve the expressive power of GNNs by using more sophisticated aggregation functions. This work is an important step in the direction of improving the expressive power of GNNs and making them more powerful for various tasks.\tThe paper presents a method that should increase the expressive power of GNN. This method includes sampling subgraphs out of the input graph using a novel diverse sampling method and calculating the output of each node by using a shared GNN on each sampled graph and summing over the outputs. The empirical evidence presented shows that this method outperforms other GNN architectures such as GCN and GAT on several node classification tasks.""240\tThis paper provides an overview of graph neural networks (GNNs), a type of neural network that has been successfully used for graph representation learning and many predictive tasks on graph-structured data. The paper\u8ba8\u8bba\u4e86GNNs\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5305\u62ecneighborhood aggregation scheme\u3001 representation\u3001recurrent architecture\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u3002 The paper also discusses the expressive power of GNNs, which is believed to be responsible for their success in learning representation of nodes and graphs. Xu et al. provide an analysis of the expressive power of GNNs and develop a simple method for improving their performance. Finally, the paper presents future directions for GNN research and highlights the importance of studying the underlying architecture of GNNs, as well as the need to explore new data sources and task-specific architectures.\tThis paper claims that existing GNNs often suffers from the limited capability of the aggregation function. This paper proposes a new framework of a diverse sampling of the graph to solve this problem. Specifically, this paper first samples several different graphs and use GNN on each graph to generate features, and finally use a type of injective multi-set aggregation function to obtain the final representation. The experimental results show that adding this module to GCN and GAT can further boost node-based multi-class classification performance. ""241\tThis paper studies the robustness of machine learning (ML) models to bit-level network and file corruptions, which are non-adversarial examples that arise in the real world from network congestion. It provides an overview of the current state of the art in this area and discusses the challenges and opportunities that arise when trying to robustly train ML models against bit-level corruptions. The paper presents a number of works that have been done in this area, including some of the most recent research, and discusses the limitations and future directions of these works. Finally, the paper concludes by highlighting the importance of considering bit-level corruptions when training ML models and suggests some possible solutions to this problem.\tThe authors explored the robustness of video machine learning models to bit-level corruption. They investigated previous methods such as Out-Of-Distribution (OOD) detection and adversarial training and found that they are not effective enough to defense against the bit-level corruption.  Accordingly, this paper proposed a new framework, Bit-corruption Augmented Training (BAT), which utilizes the knowledge about corruption by bit-level data augmentation at the training stage. Also, the authors argue that the proposed method outperforms the previous methods in handling the bit-level corrupted dataset.""242\tThis paper studies the robustness of machine learning (ML) models to bit-level network and file corruptions in the real world. Video, as a datamodality, has become increasingly common, with applications in online conferencing, autonomous vehicles, action recognition, event detection, and more. However, videos are also susceptible to bit-level corruptions beyond the pixels, such as network and file corruptions. These bit-level corruptions are generally non-adversarial and arise from network congestion or other real-world issues. This paper discusses the effects of these bit-level corruptions on ML models, and\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848. The paper also highlights the need for robust ML models and techniques to handle real-world bit-level corruptions.\tThe authors evaluate the effect of bit-level corruption, including network packet losses and bit corruptions, on video models such as action recognition and multi-object tracking. They found that the model performances drop significantly under severe corruption levels. To overcome this issue, they propose a defense method named Bit-corruption Augmented Training (BAT) to enhance the robustness of the model by embedding corrupted video samples in the training process. Results show that BAT is able to improve the model robustness over other methods such as Out-Of-Distribution (OOD) detection and Adversarial Training (AT). ""243\tThis paper discusses the issue of bit-level corruption in videos, which can affect the performance of machine learning (ML) models. It explains the concept of bit-level corruption and discusses the sources of it, including network congestion and file\u4f20\u8f93 problems. The paper also presents a number of research papers that have studied the issue of bit-level corruption and how it can affect ML models. The paper concludes by discussing the potential solutions to this problem, including improving the quality of video\u4f20\u8f93 and using ML models with multiple inputs to address the issue of bit-level corruption.\tThis work investigates the problem of building robust video prediction models in the presence of signal corruption. The problem itself is not widely studied and experimental work like this one certainly opens some possibilities. The solution on the other hand is surprisingly simple and easy to implement. It serves the purpose of introducing the problem to a wider audience, and shed some light in different types of remedies. ""244\tThis paper discusses the use of node and graph embeddings as standard feature representations in many learning tasks and the challenges that arise when trying to resolve time-resolved networks. It also discusses the applications of these methods in natural and artificial systems. The paper begins by providing an overview of the types of networks that can be represented as networks of elementary structural entities coupled by relations between them. It then presents the concept of node embeddings and how they can be used to solve downstream tasks such as edge prediction, network reconstruction and node classification. The paper also discusses the challenges that arise when trying to resolve time-resolved networks. Finally, the paper provides an overview of the latest research in this field and discusses the future directions for this research.\tIn this paper, the authors propose learning node embeddings of time varying graphs. They extend the ideas from Skip Gram Negative Sampling (SGNS) to time varying graphs. They extend the relationship between SGNS and Matrix Factorization to a tensor setting. The key contribution seems to be learning a static embedding for each node and an embedding for a time step. These embeddings are combined to learn a time-aware node embedding. Experiments on multiple datasets show that the proposed method outperforms related benchmarks. ""245\tThis paper discusses the use of node and graph embeddings in machine learning tasks. It explains the importance of representation methods for natural and artificial systems, and the use of embeddings in solving downstream tasks such as edge prediction, network reconstruction, and node classification. It also discusses the challenges of real-world networks and how time resolved networks can support dynamic processes. Finally, the paper presents a review of the current state of the art in node and graph embeddings and discusses future directions.\tIn this paper, the authors studied the problem of time-varying graph embedding problems. The authors generalized skip-gram based graph embedding method to time-varying graphs. The authors show that the method can be used to factorize time-varying graphs as high-order tensors via negative sampling. The authors carried out experiments on several time-resolved proximity networks with comparison to several state-of-art baselines.""246\tThe paper discusses the importance of node embeddings in representation and analysis of complex networks, and how they have been used in various learning tasks. It also highlights the limitations of using node embeddings for dynamic networks. The paper also discusses the challenges of time resolved networks and how they can be used to improve the accuracy of node embeddings.\tThe paper proposes an implicit tensor factorization approach for learning time-varying node representations over dynamic networks.  The core method lifts the well-known skip gram based embedding approach from matrix to higher order tensors to support temporal dimensions. The authors claim that such tensor based treatment allows to disentangle the role of node and time. Negative sampling method ( similar to noise contrastive estimation) is extended the higher order tensor setting and incorporated in the cross entropy objective for training. In the experiments, the authors consider five variants of face-to-face proximity data that contains temporal interactions and focuses on tasks of node classification (predicting outcome of SIR epidemic process) and link prediction (in the form of event reconstruction). The proposed method has been compared against two discrete time graph representation learning model and a recently proposed tensor based method. The authors claim that the provided method shows comparable performance with requirement to train lesser number of parameters. Also, the authors provide qualitative analysis in terms of embedding visualizations and goodness of fit plots. ""247\tThis paper presents a study on the ability of self-supervised language models to perform logical reasoning on mathematical formulas. To evaluate the models' abilities, the paper proposes several downstream tasks such as inferring types, suggesting missing assumptions, and completing equalities. The paper also analyzes the models' ability to formulate new conjectures. The results show that models trained on a novel skip-tree task perform significantly better than models trained on standard skipsequence tasks, and outperform even human experts. The paper also suggests that self-supervised language modeling can be used for developing mathematical language models that can help with mathematical research and development.\tThe authors propose a self-supervised learning task to enhance the reasoning capabilities of machine learning models on mathematical formulas and to perform conjecturing in higher order logic. The task consists in masking out specific portions of mathematical statements and predict them from the surrounding parts. The task can (i) be used during training, to provide supervisory signal to the machine learning model and to increase the effective size of the otherwise small training dataset, and (ii) be used during testing, to evaluate the reasoning capabilities of the learnt models by masking out the mathematical statements at different level of granularities. The authors perform an extensive experimental analysis and provide evidence on the utility of using self-supervised learning in the context of theorem proving.""248\tThis paper discusses the use of self-supervised language modeling for mathematical reasoning. It proposes a novel task called skip-tree that is used to evaluate the logical reasoning abilities of language models and finds that models trained on the skip-tree task outperform models trained on standard skipsequence tasks. The paper also analyzes the models' ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proof.\tThis paper proposes a skip-tree training task. The authors show that self-supervised language models (the Transformer architecture to be exact) trained on the proposed skip-tree training task for mathematic theorem proving enable mathematical reasoning capabilities. Moreover, no fine-tuning is required to achieve the reported reasoning capabilities. They compare the mathematical reasoning abilities of the skip-tree training task with skip-sequence and show an impressive performance improvement. Another interesting result is studying whether any useful (novel) conjectures can be generated by the model.""249\tThis paper presents a study of the ability of self-supervised language models to perform logical reasoning when applied to mathematical formulas. To evaluate the models' abilities, the paper proposes several downstream tasks, such as inferring types, suggesting missing assumptions, and completing equalities. The paper also analyzes the models' ability to formulate new conjectures. The results show that models trained on a novel skip-tree task perform significantly better than models trained on standard skipsequence tasks, and show strong mathematical reasoning abilities.\tThis paper extends the idea of language-model style self-supervised learning approach to training logical reasoning models from unlabeled mathematical expressions. The main idea is to develop a skip-tree proxy task (self-supervision) for training the encoder-decoder architecture.  The skip-tree method masks out a complete sub-tree in the input and linearizes it into a sequence in the form of S-expression. The model is required to predict the masked subtree at the decoder end. The paper also proposes several new reasoning tasks for evaluating the model performance. Experimental results show that models learned from this task significantly outperform those trained on the skip-sequence task. Furthermore, the model also exhibits good conjecturing ability in generating quite reasonable amount of new theorems that are provable and useful, which is quite encouraging and impressive.""250\tThis paper studies the problem of how to evaluate the robustness of deep neural networks against adversarial examples. The paper begins by introducing the concept of adversarial examples, which are input instances crafted by adding small adversarial perturbations to natural examples. Deep neural networks are vulnerable to these examples, which can fool them into making false predictions with high confidence. The paper then discusses a number of defense methods that have been proposed to overcome this vulnerability, including gradient maskining and obfuscation. \n\nThe paper also discusses a common pitfall in evaluating the robustness of adversarial networks, which is the phenomenon of gradient masking or obfuscated gradients. This can lead to weak or unsuccessful attacks and false signals of robustness. The paper provides a summary of the main findings and\u63d0\u51fa\u4e86\u4e00\u4e9b future research directions in this field.\tThe paper introduces and analyses the possibility that the effectiveness of PGD-based adversarial attacks might be reduced by imbalanced gradients between the terms of the margin losses commonly used. As a remedy, it also proposed a new scheme for PGD attack, where for the first half of the iterations a single-term loss is optimized, before falling back on the the usual margin loss. The authors test the hypothesis of imbalanced gradients, introducing a new metric, GIR, and the newly proposed attack in two versions, MD and MDMT, on several defenses based on adversarial training.""251\tThis paper provides an overview of the state of the art in evaluating deep neural network robustness to adversarial examples. The paper highlights the challenges of evaluating adversarial robustness and the common pitfall of including gradientmasking or obfuscated gradients in the evaluation process. The paper also discusses some of the recent advances in defense mechanisms against adversarial examples, including the use of larger model sizes, faster training algorithms, and more advanced evaluation metrics. The paper\u6700\u540e concludes by highlighting the need for further research in the area of adversarial robustness evaluation and the development of more robust and effective defense mechanisms.\tThis paper explores constructing adversarial examples in classification, in order to create better robustness metrics for general classifiers. An attack is defined as an epsilon-perturbation of the learned parameters which create a model whose performance is much degraded. The premise of this paper is to use gradient imbalance as a way of creating perturbation targets, which are claimed (and shown numerically) to better fool networks that are trained to withstand more traditional attacks, and can be used to create more robust models in general.""252\tThis paper discusses the vulnerability of deep neural networks to adversarial examples and the efforts to overcome this vulnerability. The paper also highlights the problem of gradient masking or obfuscated gradients that can lead to false signals of robustness in adversarial robustness evaluation. The paper provides an overview of the literature on this topic and discusses some of the current challenges in evaluating adversarial robustness. It also suggests some potential future directions for research in this area.\tThis work highlights the existence of imbalanced gradients as a phenomenon that may hinder optimization of gradient-based adversarial attacks and, thus, give a false sense of robustness. Imbalanced gradients may occur as the attack objective consists of the difference of two terms (typically, the outputs of the network on two different classes). When the gradients of these two terms have opposite directions, the attack optimization may get easily stuck in a suboptimal local optimum, thus decreasing the attack effectiveness.""253\tThis paper presents a graph-to-sequence deep learning system for generating axiomatic proof of semantic equivalence between complex expressions represented as typed trees. The system is designed to handle expressions from a rich multi-type symbolic language for linear algebra, and includes 16 distinct operators combining scalars, vectors, and matrices with 147 distinct axioms of equivalence. The paper investigates the robustness of the system to generate increasing length proof, and achieves 93% average true positive coverage on 10,000 test cases while ensuring zero false positives by design.\tThis paper proposes a model for verifying semantic equivalence  between symbolic linear algebra expressions. Expressions are represented by trees and equivalence is proven by a sequence of axioms applied to the first expression. The proposed model encodes the expression/program trees as nodes on a graph connected by edges representing one of a set of axioms being applied to one of the elements in the first expression to yield a node in the second expression. The output of the model is a path, a sequence of edges, on this constructed graph that correspond to a sequence of axioms applied to the first expression to arrive at the second.""254\tThis paper discusses a problem in computer science called proof generation, which is the process of creating axiomatic proof(s) of a given statement or theorem from a set of input statements or expressions. The goal of proof generation is to create a proof that is both rigorous and verifying, while also being short and easy to understand.\n\nIn this paper, we propose a graph-to-sequence deep learning system that can generate axiomatic proof(s) of semantic equivalence between program pairs. We use a rich multi-type symbolic language for linear algebra to represent the input expressions and theaxioms of equivalence. Our system generates expressions that include scalars, vectors, and matrices, and 16 distinct operators combining them, with 147 distinct axioms of equivalence. We study the robustness of the system to generate proof(s) of increasing length, demonstrating how incremental graph-to-sequence networks can learn to represent complex and verifiable symbolic reasoning.\n\nOur system achieve 93% average true positive coverage on 10,000 test cases while ensuring zero false positives by design, which is a significant improvement over previous proof generation systems. We also investigate the performance of our system on a different task, prover generation, and find that it can generate high-quality proof(s) of a wide range of statements and theorems.\n\nOverall, our paper provides an overview of the problem of proof generation, and presents a novel and efficient system for generating axiomatic proof(s) of semantic equivalence between program pairs. This work has the potential to have a significant impact on the field of proof generation, and can serve as a reference for other researchers in the area.\tThe authors introduce a new synthetic dataset of equational proofs over the basic axioms of linear algebra.  The dataset consists of triples `(t1, t2, rewrites)` where `rewrites` is a sequence of rewrite instructions that can transform `t1` into `t2`, though some of their models emit one rewrite at a time and observe the result of applying the rewrite instruction to `t1`.  They develop a GNN for the task called pe-graph2axiom and show that it beats two baselines.  Finally, they show that their trained system can solve 15 problems from two Khan Academy modules that can be expressed in their fragment.  The appendix refers to supplementary material including code and data, but no supplementary material was submitted.""255\tThis paper presents a graph-to-sequence deep learning system for generating axiomatic proof of semantic equivalence between complex expressions represented as typed trees. The system is designed to handle expressions from a rich multi-type symbolic language for linear algebra and generate 147 distinct axioms of equivalence between program pairs. The paper demonstrates the system's ability to generate increasing length proof of equivalence, and achieve 93% average true positive coverage on 10,000 test cases while ensuring zero false positives by design. The system is also shown to be robust to changes in the input expression and can generate proof of axiomatic equivalence that is verifiable.\tThis paper proposes a synthetic dataset of algebraic expressions with various kinds of symbols (e.g. scalars, vectors, matrices), and applies graph-to-sequence networks (with attentions) for predicting a sequence of rewrite rules (i.e. axioms) as an equivalent proof between two expressions. The prediction can be validated by a simple checker so that any false positives can be eliminated.""256\tThis paper discusses the problem of learning multimodal representation from unlabeled videos and proposes a new approach to address this issue. The paper explains that existing approaches learn localized representations from short videos, which capture only short-term dependencies in data. However, learning representation that captures long-term dependencies is equally important for activity recognition and other applications. The paper proposes an approach that\u5982\u4f55\u5904\u7406long videos, which requires large computational resources, by using a global representation that captures both short-term and long-term dependencies. The proposed approach is shown to be effective in several experiments and can be applied to various applications in the field of multimodal learning.\tThe paper is a nice read. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. This typical trend is due to the memory requirements for training a multi-modal transformer end-to-end.""257\tThis paper provides an overview of the current state of the art in learning multimodal representation from unlabeled videos. It highlights the importance of capturing long-term dependencies in the representation learning process for various applications. The paper discusses existing approaches to this problem, including localizing representations from short videos that capture only short-term dependencies, and representing long videos using larger scale representation. The paper also highlights the need for further research in this area to explore more effective and efficient methods for multimodal representation learning.\tIn this work, the authors present a method for learning audiovisual (AV) representations from videos using a Transformer-based model architecture. Since both video processing and Transformer-based model are memory-intensive, a parameter-reducing scheme is proposed, which facililates training the model end-to-end. The AV representations are learned by training the network to solve two self-supervised pertaining tasks, and subsequently evaluated on various audio/visual downstream tasks. An ablation analysis is performed to demonstrate the efficacy of the various contributions.""258\tThis paper discusses the problem of learning multimodal representation from unlabeled videos. It proposes that existing approaches learn localized representations from short videos, capturing only short-term dependencies in data, which is useful for certain applications but is not sufficient for learning long-term dependencies. It also\u6307\u51fa that processing long videos requires large computational resources. Therefore, it is proposed to develop methods that can learn more general and effective representations that capture long-term dependencies from long videos.\tThis paper studies modeling and training choices when designing a single model based on ConvNets and transformers for audio-visual representation learning. It proposes ablations for which weights/layers to share across modalities, when/where to fuse/join both modalities, and other modeling details (that matter). It also completes pre-training with 3 InfoNCE (audio-audio, visual-visual, audio-visual) with a binary classification loss about if two pairs of audio-visual are from the same or different videos. As strategies for negative sampling in audio and videos are different, it proposes to sample negatives that are similar in the ConvNets' embeddings. The models are pretrained on Kinetics-700 and AudioSet and evaluated on UCF101, ESC-50, and Kinectics-Sounds.""259\tThis paper aims to bridge the gap between typical human and machine-learning environments by extend the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and models are evaluated online while learning novel classes. The paper proposes a new few-shot learning dataset based on large scale indoor imagery that mimicking the visual experience of an agent wandering within a world. The paper also proposes a new contextual prototypical memory model that can make use of the underlying context that changes throughout time. The approach is designed to improve the performance of few-shot learning models.\tThe paper proposes a new learning paradigm that combines both few-shot learning(FSL)  and continual learning (CL) to provide a more realistic learning environment rather than the traditional train-test-retrain approach in FSL. Two environments are proposed, along with a novel dataset. The evaluation seems to be thorough, with strong baselines (conventional approaches adapted to the proposed setting). A novel approach is proposed based on augmenting ProtoNets with contextual memory and is shown to have consistently strong performance compared to the baselines on both tasks.""260\tThis paper proposes a new framework for online, continual few-shot learning, which allows models to learn new classes while online and while constantly improving. The framework includes an extension of the standard few-shot learning framework to an online, continual setting, where episodes do not have separate training and testing phases. Instead, models are evaluated online while learning novel classes. The paper also proposes a new dataset of large scale indoor imagery that mimicking the visual experience of an agent wandering within a world. The approach is designed to be able to learn new skills and improve over time, similar to how humans learn. Additionally, the paper proposes a new contextual prototypical memory model that can make use of the changing context throughout time to improve performance.\tThe paper presented a new setting of online contextualized few shot learning to mimic human learning. This setting combines continual learning and few shot learning, and additionally considers context switch. Specifically, a learning method is presented with a sequence of samples that might come with labels. The method is then tasked to classify the current input into known categories, or recognize the input as belonging to a \u201cnew\u201d category, while at the same time updating the model for known and new categories. Two new datasets (hand-written characters and indoor images) were constructed to support the learning setting. An extension of Prototypical Network (Snell et al.) was explored for this new setting. The results were compared against several baselines and were quite promising. ""261\tThis paper proposes a new framework for online few-shot learning, which allows models to learn new classes while being evaluated online. The framework extends the standard framework of few-shot learning to an online, continual setting, where episodes do not have separate training and testing phases. The paper proposes a new dataset of large scale indoor imagery that mimicks the visual experience of an agent wandering within a world, and converts popular few-shot learning approaches into online versions. Additionally, the paper proposes a new contextual prototypical memory model that can make use of the changing context throughout time. This framework aims to bridge the gap between typical human and machine-learning environments by enabling models to learn new classes and retrieve learned skills in the past.\tThis work aims to make a realistic learning setting by combining few-shot learning and continual learning in the online setting. Similar to few-shot learning, the model needs to adapt to new classes with a few samples (at least in the beginning). Similar to continual learning, the model needs to learn new classes over time while being tested on the older classes as well. When encountering a new class, the model is expected to recognize that. Similar to the online setting, model evaluation happens on each trial, after which the model can be updated with that data (labeled or unlabeled). This new paradigm is called Online Contextualize Few-Shot Learning.""262\tThis paper discusses the training of Graph Neural Networks (GNNs) on temporal graphs, a type of graph that can be represented by a time series of vertices and edges. It presents recent work on combining GNNs with recurrent modules, vertex embeddings as a function of time, and sampling-based techniques to improve the scalability of GNNs. The paper also discusses some of the fundamental issues with temporal graphs, including the distribution shift caused by new vertices and edges appearing over time and the need for additional pre-processing or post-processing to address this issue.\tThis work empirically evaluates the sliding-window strategy for training GNNs with temporal graphs. One may cast the temporal nature of the graph data in an online setting, under which the change of the graph structure as well as the variation of the classes cause distribution shift. The authors conduct a series of experiments to show that the sliding-window strategy is as effective as using the entire historical data for training.""263\tThis paper provides an overview of the current state of research on training Graph Neural Networks (GNNs) on temporal graphs. It discusses recent works on combining GNNs with recurrent modules, vertex embeddings as a function of time, and sampling-based techniques to improve the scalability of GNNs. However, there are also fundamental issues with temporal graphs that are not properly answers yet, such as the distribution shift caused by the addition of new vertices and edges over time. The paper proposes future directions for research in this area, including the use of advanced graph processing techniques and the development of more efficient GNN models.\tThis paper proposes a paradigm which speeds up the training time of GNNs while not compromising too much performance. The method adopts a layerwise training procedure. In particular, the authors inject a loss function at each layer while storing and fixing the feed-forward values of its previous layer. The training is then carried out along all layers parallelly, which allows the updating of paradigms to be decoupled and is not applicable in previous works. A further improvement (lazy-update) by not updating the feed-forward values of each layer is used to reduce the training time.""264\tThis paper discusses the problem of training Graph Neural Networks (GNNs) on temporal graphs, which arise in many real-world applications such as social media analysis and natural language processing. It explores recent works on combining GNNs with recurrent modules, vertex embeddings as a function of time, and other approaches to improve the scalability of GNNs. The paper also highlights some of the fundamental issues with temporal graphs, including the distribution shift caused by new vertices and edges appearing over time, and the need for further research to address these issues.\tThis work studies the problem of online or incremental learning in temporal graphs (dynamic networks), and more precisely, whether past data can be discarded/ignored without losing predictive accuracy under the assumption that there is the presence of a distribution shift. This question has been essentially investigated over the years in various contexts, e.g., relational learning and classification in dynamic or time-evolving networks. It is also completely obvious that forgetting older data, especially under the assumption of a distribution shift, makes sense and is the correct thing to do. This is exactly what has been done in time-series forecasting for decades. The problem formulation is unclear and can be more precisely defined and motivated appropriately. This needs to be fixed. Are the class labels of a node changing over time, so if a node has label A at time t, then at time t+1 it could have label B, etc. This doesn\u2019t seem true, as it seems the class labels of the nodes are \u201cstatic\u201d, which is unrealistic in many cases. How are the graph snapshots created? How was the timespan selected? What does every time step represent (1 hour, 5 minutes, etc.)?  Also, are the node features changing over time? This doesn\u2019t seem true, but if this is the case, then it is unclear why this would be the case in practice (it would be great to provide some motivation for this, or an example application or problem where this may be true). There are many assumptions that make this problem unrealistic. Furthermore, there have even been works that study the dynamic node classification problem previously, see [1-2] below. ""265\tThis paper discusses the recent success of deep reinforcement learning (RL) in mastering video games and the game of GO, but also highlights the challenges that come with using this technology to achieve good generalization in new environments. The paper suggests that regularization methods such as dropout and data augmentation can help improve the performance of RL agents.\tThis paper proposes a new method for learning representations of images with the goal of improving generalization in RL. The key idea is to regularize the learned feature space by forcing embeddings of states followed by the same action (or sequence of actions) to be more similar than embeddings of states followed by different actions. The method is evaluated on the Procgen and achieves superior performance relative to Rainbow, a standard RL algorithm.""266\tThis paper discusses the recent success of deep reinforcement learning (RL) in mastering various games and the limitations of the approach. The paper highlights the importance of regularization methods such as dropout and data augmentation in improving the generalization of RL agents to new environments. Additionally, the paper discusses the challenges of training RL agents on a large, real-world environment with a large number of states and actions.\tThis paper tackles the problem of representation learning from visualized input. The paper presents \"cross-state self-constraint(CSSC)\", a technique for regularizing the representation feature space by favoring representation similarity (scalar product between representations) between representations when the agent behaves similarly. The approach is tested with deep RL on the OpenAI ProcGen benchmark.""267\tThis paper discusses the problem of memorization in deep reinforcement learning, which has been observed in recent research works. The paper highlights the importance of regularization methods like dropout and data augmentation in improving the generalization of deep reinforcement learning algorithms.\tThe authors modify the Rainbow loss through an additional loss based on a cross-state similarity. The notion of similarity is called in the paper \"cross-state self-constraint\" - CSSC. The CSSC loss looks very simple and can be applied with other RL algorithms. The loss is defined in terms of an embedding e of the visual input into a 1-dim space. Given three states x_p, x_q, and x_r, their similarity is defined as the scalar product of e(x_p) and e(x_q) minus the scalar product of e(x_q) and e(x_r). It would be useful if the authors include pseudocode how the loss is exactly computed for a given batch of samples. My understanding of the description is that for a given batch the authors generate triples with the same action set and then take as an auxiliary loss the weighted sum of the logarithms of sigmoids of similarities of these triples.""268\tThis paper discusses the surge of research interest in graph neural networks (GNNs), a family of deep learning models on graphs, and the increasing concerns about their robustness under adversarial attacks. The paper also examines the research on adversarial attacks on GNNs and their ability to help better understand the intrinsic properties of existing GNN models. The paper ends by providing some future directions for GNN research.\tThis paper studies the problem of designing adversarial attacks (on GNN models) that perturb the feature to maximize the misclassified instances. Assuming that the activations are activated independently at random, the paper shows that the attack design can be reduced to the influence maximization problem under the threshold model. The paper identifies several conditions on the threshold that can make the influence maximization problem submodular, thereby making it easy to optimize.  Experiments have been shown that the proposed attack method has higher performance compared to the existing ones.""269\tThis paper provides an overview of graph neural networks (GNNs), a family of deep learning models on graphs, and the recent research on their robustness and adversarial attacks. The paper highlights the surge of research interest in GNNs due to their applications in online Web services and superior performance on various tasks, such as traffic forecasting, social network analysis, and recommender systems. However, the paper also mentions the increasing concerns regarding the robustness of GNNs under adversarial attacks, especially in realistic scenarios. The research on adversarial attacks on GNNs helps us better understand the intrinsic properties of existing GNN models and has led to the development of new attack methods and GNN models that are more robust to adversarial attacks.\tPaper summary: The paper studies the problem of attacking GNNs in a restricted black-box setup, i.e., by perturbing the features of a small set of nodes, with no access to model parameters and model predictions. The authors draw a connection between the restricted attack problem and the influence maximization problem, and then propose several approximation techniques to solve the reformulated attack problem. Experimental results on attacking three GNN models demonstrate the effectiveness of the proposed attack. ""270\tThis paper focuses on the research on graph neural networks (GNNs) and their application in various domains. The paper first introduce the GNNs and their properties, then discuss the performance of GNNs on various tasks and their comparison with other deep learning models. The paper also investigate the robustness of GNNs under adversarial attacks and the different types of adversarial attacks. Finally, the paper concludes with the future directions of GNNs and their application in real-world problems.\tThis paper introduces a novel connection between adversarial attack on graph neural networks in a restricted black-box setup via node feature perturbation, on the one hand, and the influence maximization problem under the linear threshold model on the same graph, on the other hand. An analysis shows that the objective function of the corresponding IM problem is submodular under assumption, hence the problem admits greedy approximation algorithms as effective black-box attack strategies. Experiments show such attacks are effective compared to baselines in degrading the performance of GNNs in terms of mis-classification rate.""271\tThis paper discusses the problem of learning causal structures from purely observational data. It categorizes existing methods into two classes: constraint- and scorebased. The former use statistical tests to extract conditional independence constraints from the data, and seek to identify the class of causal structures compatible with those constraints. The\u540e\u8005\u5219 use scores to predict the effects of random variables based on the relationships between them. The paper\u8ba8\u8bba\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\tThis paper attempts to exploit the low-rankness of the adjacency matrix of the DAG in Bayesian network structure learning. The overall framework is similar to NOTEARS, except that the adjacency matrix W is decomposed into low rank components W = UV'. To justify the approach, the paper also includes lower and upper bounds of the rank of DAGs, albeit mostly theoretical and not applicable to real experiments. ""272\tThis paper provides an overview of the problem of learning causal structures from purely observational data. It discusses the importance of this task in various sciences, the existing methods for learning causal structures, and the challenges and opportunities in the field. The paper concludes by highlighting the need for further research in this area to improve the accuracy and effectiveness of learning causal structures from observational data.\tThe paper develops several useful lower and upper bounds on the rank of DAGs \u2014 specifically minimum and maximum rank of all weighted matrices that induce the same DAG \u2014 in terms of various graphical properties like head-tail vertex cover, number of non-root and non-leaf vertices. The paper also bounds the rank of DAG in terms of the rank of its skeleton and moral graph. The paper proposes learning low-rank linear or non-linear structural equation models (SEMs) by adding simple norm constraints or matrix factorization to existing SEM learning methods. Through experiments on synthetic and real world data the authors demonstrate that when the underlying SEM is low-rank, exploiting this low-rank assumption in the learning process can lead to better performance. The authors also demonstrate that the rank can be estimated using the obtained bounds from a validation set.""273\tThis paper discusses the problem of learning causal structures from purely observational data, which is often available in various domains. existing methods can be roughly categorized into two classes: constraint- and scorebased. The former use statistical tests to extract from data a number of constraints in the form of conditional (in) dependence and seek to identify the class of causal structures compatible with those constraints. The\u540e\u8005\u5219 use machine learning techniques to learn a DAG representation of the underlying causal structure from the data. The paper\u6df1\u5165\u63a2\u8ba8\u4e86\u4e24\u79cd\u73b0\u6709\u65b9\u6cd5\uff1a\u7ea6\u675f\u65b9\u6cd5\u548c\u5f97\u5206\u65b9\u6cd5\u3002\u524d\u8005\u4f7f\u7528\u7edf\u8ba1\u6d4b\u8bd5\u6765\u63d0\u53d6\u6570\u636e\u4e2d\u7684\u6761\u4ef6(in)\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5bfb\u627e\u7b26\u5408\u8fd9\u4e9b\u7ea6\u675f\u7684 causal\u7ed3\u6784\u7c7b\u522b\u3002\u540e\u8005\u5219\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u5b66\u4e60 DAG \u8868\u793a\u3002\u8be5\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7ea6\u675f\u65b9\u6cd5\u548c\u5f97\u5206\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\tThe paper provides a new approch for learning a (possibly densely-connced) low-rank DAG models in the high dimensional settings. In particular, this paper provides how to exploit the property of the low-rank for recovering a underlying causal structure. Futher shown is that under what circumstance the low-rank assumption holds and heuristically confirms that thgrough simulation settings. Lastly, the proposed approach is compared against the state-of-the-art DAG learning algorithms that requirs the assumption of a sparse graph.""274\tThe paper discusses the development of AutoML algorithms, which enable automatic training of deep models from data without human involvement. The algorithms include automated data augmentation, neural architecture search, and hyper-parameter optimization. However, the development of these algorithms has been\u9047\u5230\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c including low efficiency and sub-optimal results. The paper suggests a solution to achieve a full pipeline from data to model, which involves implementing all three components of AutoML in one stage.\tThis paper shows how the complex autoML pipeline for neural networks can be trained in an end-to-end manner by combining existing methods. By using backpropagatable discrete sampling methods (Gumbel softmax), input transformed by data augmentation is seamlessly embedded in full backpropagation flow. And a differentiable architecture search algorithm is used, which also incorporates architecture search in full backpropagation flow. On top of this differentiable procedure, an alternating optimization is introduced to train network parameters and hyperparameters.""275\tThis paper discusses the development of AutoML, a technique that enables automatic training of deep models from data without human involvement. The paper covers the history of AutoML, the proposed algorithms for automated data augmentation, neural architecture search, and hyper-parameter optimization, and the challenges and limitations of these algorithms. The paper also suggests a new approach to achieving full-pipeline from data to model, which involves combining multiple AutoML algorithms in a single pipeline.\tSome of the choices that have to be made when training a neural net based image model are: type of data augmentation, architecture of the neural network, and other hyperparameters such as regularization and optimization hyperparameters (e.g. learning rate). Optimizing all of these is a challenging problem, NAS deals with architecture but ignores the others. More general hyperparameter optimization techniques such as Bayesian Optimization struggle with the dimensionality of the architecture parameters. And optimizing them independently might lead to local minima, and/or be slow.""276\tThis paper discusses the challenges and opportunities in AutoML, a field that enables automatic training of deep models from data. The paper\u8ba8\u8bba\u4e86\u5728AutoML\u4e2d\u5b9e\u73b0\u5b8c\u6574 pipeline \u7684\u6311\u6218\u548c\u673a\u4f1a\uff0c\u5305\u62ec\u81ea\u52a8\u6570\u636e\u589e\u5f3a( DA )\uff0c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22(NAS )\uff0c\u548c\u8d85\u53c2\u6570\u4f18\u5316( HPO )\u3002\u7136\u800c\uff0c\u5206\u522b\u5b9e\u73b0\u8fd9\u4e9b AutoML \u7ec4\u4ef6\u4ee5\u7279\u5b9a\u4efb\u52a1\u4e0d\u4ec5 suffering from low efficiency\uff0c\u800c\u4e14\u4f1a\u5bfc\u81f4 sub-optimal results\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5728\u4e00\u4e2a\u6574\u4f53\u8fc7\u7a0b\u4e2d\u5b9e\u73b0 AutoML\uff0c\u4ee5\u4ece\u6570\u636e\u5230\u6a21\u578b\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u662f\u672c\u8bba\u6587\u8ba8\u8bba\u7684\u91cd\u70b9\u3002\tThis paper focuses on achieving automated \"from data to model\" including different components in modeling, namely data augmentation, Neural Architecture Search, Hyper Parameter Optimization. The proposed approach first use data augmentation to select the data argumentation transformation. It tries to select examples which incurs higher training loss for the model to address hard examples. Then use the DAG for neural architecture search. Given the data and architecture, it then alternatively update the model parameter and hyper parameter. The overall proposed framework is end-to-end. Experiment on ImageNet shows slight performance improvement over existing approaches. The authors also conduct ablation study to show the effectiveness of jointly modeling the three components (data augmentation, neural architecture search, hyper-parameter optimization).""277\tThis paper discusses the importance of uncertainty estimates in machine learning and the various methods that can be used to provide them. It also highlights the benefits of using ensemble methods, which provide robust uncertainty estimates and allow for the interpretation of their contributions. The paper begins by discussing the traditional approach to uncertainty in machine learning, which is to remove the uncertainty by\u5173\u95ed\u505c\u6b62\u8fd0\u884c\uff0c\u5220\u9664\u6587\u4ef6\uff0c\u624b\u52a8\u5173\u95ed\uff0c\u6216\u5173\u95ed\u8fd0\u884c\u3002 The paper then discusses ensemble methods, which combine multiple models to provide a more confident and robust estimate of the uncertainty. These methods can be used to provide interpretable measures of uncertainty, which can be useful for understanding the contribution of each model to the overall uncertainty. The paper then provides a summary of the various methods that can be used to provide uncertainty estimates in machine learning, including traditional approaches and ensemble methods. It also highlights the importance of considering the uncertainty in decision-making and the benefits of using uncertainty estimates in high-risk applications.\tThis paper extends Prior networks models, previously introduced for classification, to regression problems.  Prior networks are neural networks whose main target is to \"modelling uncertainty in classification tasks by emulating an ensemble using a single model\".  Standard Prior networks models output the parameters of a Dirichlet probability distribution. This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes. This hierarchical approach allows to better capture uncertainty. The presented approach extends this framework to regression tasks. So, instead of returning the parameters of a Dirichlet distribution, it returns the parameters of a Normal-Wishart distribution, which then defines a probability distribution over Normal distributions, and, in turn, each Normal distribution defines a probability distribution over the value of the target variable.  ""278\tThis paper provides an overview of machine learning and uncertainty in neural networks. It discusses the benefits of using neural networks for machine learning and the need for estimates of uncertainty in their predictions. The paper also discusses ensemble methods, which provide robust estimates of uncertainty and allow for interpretable measures of uncertainty to be derived. The paper\u6700\u540e presents an example of how ensemble methods can be used in a real-world context to improve the accuracy and safety of an AI system.\tPrior Networks (Malinin & Gales, 2018) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks. This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity. The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data, UCI datasets and monocular depth estimation. ""279\tThis paper presents a summary of a paper that discusses the use of neural networks in machine learning, specifically their ability to address a wide range of machine learning tasks. The paper also discusses the importance of providing estimates of uncertainty in models in order to improve the safety of AI systems and avoid costly mistakes in high-risk applications. Additionally, the paper discusses the use of ensemble methods, which provide both improved predictive performance and robust uncertainty estimates, and the mathematical consistency of probabilistic frameworks that can be used to decompose overall uncertainty into component uncertainty. The paper also provides a summary of the main findings of the paper.\tThis paper addresses interpretable uncertainty quantification for data driven models. In particular, the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only. The author contribution is thus clearly stated and positioned w.r.t. prior arts and tackle a non-trivial issue.""280\tThis paper discusses the limitations of popular gradient-based model-agnostic meta-learning algorithms (MAML) for few-shot classification problems and proposes a Bayesian online meta-learning framework that overcome these limitations. The framework incorporates MAML into a Bayesian online learning algorithm with Laplace approximation or variational inference, which enables few-shot classification on a range of sequentially arriving datasets with single meta-learned model and training onSequential few-shot tasks.\tThe paper develops a semi-online Bayesian approach to meta-learning, where tasks arrive sequentially and learning within any task is performed in batch mode (hence my terminology semi-online). It suggests a sequential between-task Bayesian update, eq. 5, and proposed three approximations to aid computation. The basic setup is motivated within the recently introduced MAML framework where the learning takes place by adapting a within-task parameter to effectively set up learning within each individual task, allowing the learner to transfer information between tasks, while remaining adaptive to a specific novel task. The authors phrase this idea in the Bayesian language of posterior distributions, that are updated both within and between tasks. The posterior formed after learning t tasks, serves as a prior for learning a new task. The authors suggest 3 approximation schemes, a Laplace approximation, a Hessian approximation, and a variational approximation. Finally, a set of experiments are presented comparing performance to 2 baselines, namely TOE (train of everything) and TFS (train from scratch). A particularly interesting application is to 5 standard sets of images, testing for catastrophic forgetting of previous tasks and the transferability of  information across tasks in the face of distributional shift.""281\tThis paper proposes a Bayesian online meta-learning framework to overcome the limitations of popular gradient-based model-agnostic meta-learning algorithms (MAML) for few-shot classification problems. The framework incorporates MAML into a Bayesian online learning algorithm with Laplace approximation or variational inference, which enables few-shot classification on a range of sequentially arriving datasets and training onSequential few-shot tasks with a single meta-learned model. The paper provides an analysis of the effectiveness of the proposed framework, and also presents results of a case study.\tThe paper proposes an Bayesian approach to online meta-learning. This is done by lifting (approximate) sequential Bayesian inference from the model parameters to the meta-parameters. Two approaches are proposed to do this: (i) Laplace Approximation (LA), thereby extending Ritter et al\u2019s method from online learning to online meta-learning; and (ii) VI, thereby extending Nguyen et al\u2019s Variational Continual Learning (VCL) method in a similar way. Experiments are performed by converting existing meta-learning benchmarks into online settings in two ways, which they call \u201csequential tasks\u201d and \u201csequential datasets\u201d respectively. The experimental results show resistance to catastrophic forgetting in both of these experimental settings. ""282\tThis paper discusses the limitations of popular gradient-based model-agnostic meta-learning algorithms (MAML) for few-shot classification problems and proposes a Bayesian online meta-learning framework that overcome these limitations. The framework incorporates MAML into a Bayesian online learning algorithm with Laplace approximation or variational inference, which enables few-shot classification on a range of sequentially arriving datasets with single meta-learned model and training onSequential few-shot tasks. The paper also discusses the potential applications of the framework.\tThis work proposes a Bayesian approach to meta-learning from sequential data. Two algorithms are proposed. The first is based on the Laplace approximation to the model posterior which is made tractable by using K-FAC approximation of the Hessian. The second approaches uses a variational approximation for the posterior, where meta-learning corresponds to learning the variational prior. The experiments present results of the proposed method in sequential Omniglot and a pentathlon task involving different datasets.""283\tThis paper discusses the development and application of Graph Neural Networks (GNNs) for graph representation learning. GNNs are a powerful machine learning technique that\u5df2\u7ecf\u88ab\u8bc1\u660e\u5728\u5904\u7406Graph\u65f6\u975e\u5e38\u6709\u7528\u3002\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u91cd\u8981\u7684GNN\u7c7b\u578b\uff0c\u5305\u62ecMessage Passing Graph Neural Networks (MPNNs)\uff0c\u5b83\u4eec\u901a\u8fc7\u8fed\u4ee3\u6d88\u606f\u4f20\u9012\u6765\u6784\u5efaGraph\u8868\u793a\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684 expressive power \u88ab\u8bc1\u660e\u662f\u6709\u9650\u7684\uff0c\u4f46\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6210\u529f\u7ecf\u9a8c\u5df2\u7ecf\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u53ef\u884c\u6027\u3002\u8be5\u8bba\u6587\u8fd8\u8ba8\u8bba\u4e86GNNs\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5206\u5b50\u6a21\u62df\u3001\u793e\u4ea4\u7f51\u7edc\u5206\u6790\u548c\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7b49\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e9b\u6539\u8fdbGNNs\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5b83\u4eec\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002\tThe paper has a fully theoretical flair while proposing a novel and seemingly efficient procedure to recursively compute the higher-order (i.e. more than 1-hop) neighbourhood of a node that are used for learning discriminative graph embedding. The paper contributes with the model above (RNP-GNN) and by providing a proof of its representational power and a general theorem supplying an information theoretic lower bound on the complexity of GNNs that can count induced substructures.""284\tThis paper provides an overview of graph neural networks (GNNs), a powerful tool for graph representation learning. It discusses the history of GNNs, their properties, and applications in various domains. It also discusses the limitations of GNNs, including their expressive power and the problems that arise when trying to train them on large-scale graphs. The paper\u6700\u540e presents a future direction for GNNs and suggests ways to improve their performance.\tThe goal of the paper is to show that GNN's (without exponential computational complexity) can be constructed with the ability to count subgraphs. To this effect, the authors propose a principled neighborhood pooling strategy and theoretically characterize their expressive power - with respect to other models proposed earlier. More specifically, the authors propose a recursive neighborhood pooling strategy which characterizes graphs based on the counts of subgraphs . Furthermore, they show that if the tuple of recursion parameters are chosen well, their proposed model can capture all induced subgraphs (universality) of sizes smaller than the first value in the tuple of recursion parameters plus 1 - and show a relationship to the reconstruction conjecture (Kelly et al. 1957). The authors also provide a bound on the number of iterations required to learn the expressive representations.""285\tThis paper provides an overview of graph neural networks (GNNs), a powerful tool for graph representation learning. It discusses the history of GNNs, the importance of graph structure in understanding complex systems, and the different types of GNNs, including Message Passing Graph Neural Networks (MPNNs) and Deep Convolutional Neural Networks (DCNNs). The paper also highlights some of the challenges and limitations of GNNs, such as the need for large training datasets and the problem of overfitting. Finally, the paper concludes by discussing the potential applications of GNNs in various fields, including social media analysis and medical imaging.\tThe proposed paper seeks a theoretical possibility of counting the subgraph by a graph neural network. To this end, the authors proposed a recursive neighborhood pooling graph neural network and proved the express power of the model. The universal approximation results on a subgraph have been shown as well. Analysis of computational complexity shows the algorithm is much efficient than the known class of models that can count substructures.""286\tThis paper discusses the importance of understanding how agents learn in competitive environments in Machine Learning (ML). The paper focuses on the evolutionary game theory (EGT) approach, which has been used for decades to study adaptive learning behaviors of agents in competitive environments from Economics, Biology, and Physics. The paper highlights the benefits of using EGT for understanding ML processes and the irregular behaviors that are commonly observed in games. EGT provides a dynamicalsystemic perspective that helps to understand ML processes and provides new insights into ML-related problems. This perspective is particularly helpful in studying games where irregular behaviors are common.\tThis paper studies Lyapunov chaos in learning algorithms for matrix games. It appears to extend earlier work by Cheung and Piliouras to more general-sum settings with the conclusion that in these more common settings the learning algorithms considered exhibit chaos. The paper also presents an interesting notion of matrix domination which is a necessary and sufficient condition for chaos, and also a linear programming approach for the purpose of identifying chaotic games.""287\tThis paper discusses the importance of understanding how agents learn in competitive environments, particularly in the context of Generative Adversarial Networks (GANs). It also discusses the evolutionary game theory (EGT) approach to understanding learning processes, which has been useful in studying irregular behaviors in competitive environments. EGT provides a dynamicalsystemic perspective on learning that is particularly helpful in studying games, where irregular behaviors are common. The paper concludes by highlighting the potential applications of EGT in machine learning.\tThis paper studies the chaos phenomena of learning in general normal-form games beyond zero-sum and coordination games. Building upon the previous works by Cheung & Piliouras, the authors apply the canonical decomposition of a general bimatrix game to a sum of a zero-sum game and a coordination game. The authors further devise two new techniques: matrix domination and linear program to help analyze the game dynamics.""288\tThis paper discusses the importance of understanding how agents learn in competitive environments in Machine Learning (ML). It highlights the use of Evolutionary Game Theory (EGT) as a perspective to study these processes and provides insights into some of the ML-related problems that EGT can help solve. EGT is a decades-old area dedicated to the study of adaptive learning behaviors of agents in competitive environments, and has been used to study irregular behaviors in games, which are a common type of environment in ML. The paper provides a summary of the main findings of the paper and suggests how EGT can be used to improve the performance of ML systems.\tThis paper provides tools for classifying the payoff dynamics in general-sum n-player games as Lyapunov chaotic assuming three common algorithms are used: multiplicative weights update (FTRL with entropy regularizer), optimistic MWU, and FTRL with L2 regularizer. Previous work (Cheung & Piliouras) showed that the existence of Lyapunov chaos in the dual space is indicated by the sign of a function C of the game. This work shows that this function can be decomposed into a sum: C of the zero-sum part + C of the coordination part. They also show how to use trivial matrices (which don't affect C) to further reduce parts of the game in a way that eases the analysis. As part of their analysis, they prove that the set of bi-matrix games exhibiting chaos has positive Lebesgue measure and discuss how the relative strength of the zero-sum and coordination parts determines the ultimate sign of C.""289\tThis paper discusses the problem of accelerate the convergence speed of optimization algorithms in machine learning. The paper\u4ecb\u7ecd\u4e86\u4e00\u4e9b popular optimization algorithms, such as stochastic gradient descent (SGD) and momentum, and their variations, including AdaGrad. The paper also discusses the issue of improving the performance of these algorithms in high-dimensional problems.\n\nThe paper starts by introducing the basic concepts of optimization and discussing the main issues that arise in machine learning. It also discusses the history of optimization algorithms and the impact of machine learning on the field.\n\nThe paper then presents the main algorithms discussed in the paper, including SGD, momentum, and AdaGrad. It also includes a discussion of the issues that arise when using these algorithms, such as the issue of overfitting and the need for data Augmentation.\n\nThe paper then discusses the possible ways to improve the performance of these algorithms, such as using faster algorithms, adding data Augmentation, or using other optimization techniques.\n\nThe paper\u6700\u540e\u603b\u7ed3\u4e86 the main findings of the paper and\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u65b9\u5411.\tThe paper provides a new family of adaptive optimization algorithms by designing the proximal function of the adaptive algorithms to minimize marginal regret bound. The paper shows that the regret bound is better than existing algorithms in a sense. The paper also presents simulation study on a variety of domains that compare the proposed algorithms with other commonly used algorithms.""290\tThis paper discusses the importance of accelerate the convergence speed of optimization algorithms in the machine learning community and proposes several popular algorithms, including momentum and AdaGrad, as well as their limitations. The paper also explores the use of various data structures to improve the performance of these algorithms. The main focus of the paper is on improving the convergence speed of optimization algorithms, particularly in high-dimensional problems such as deep learning. The authors of the paper propose that using a larger number of gradient updates or adding more data to the problem can be effective in\u52a0\u901f\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6. The paper also highlights the importance of considering the complexity of the algorithms and the design of their parameters, as well as the use of data structures to improve the performance of the algorithms.\tThis paper proposes a new class of adaptive algorithms inspired by finding an optimal proximal function of adaptive algorithms. They provide a theoretical analysis of the new method showing that it would potentially improve the regret bound of current algorithms. Finally, the proposed method is empirically matched with or superior to other popular algorithms on different tasks. ""291\tThis paper discusses two popular optimization algorithms in machine learning: stochastic gradient descent (SGD) and AdaGrad. It also discusses the issue of improving the convergence speed of these algorithms.\n\nSGD is a popular optimization algorithm used for training neural networks. It uses the gradient of the loss function as the guidance to update the parameters of the model. However, the convergence speed of SGD can be slow, especially in complex problems with large amounts of data.\n\nAdaGrad is a variant of SGD that uses a matrix to scale the gradient instead of directly moving parameters in the negative direction of the gradient. The matrix is designed to be the square root of the global average of the squared gradients. Duchi et al. (2011) showed that this algorithm can be faster than SGD when the gradients are sparse. However, when the gradients are dense, AdaGrad's performance deteriorates.\n\nTo address this issue, many researchers have proposed different techniques for improving the convergence speed of AdaGrad, such as using momentum, adding a learning rate term to the loss function, or using other optimization algorithms.\n\nOverall, the issue of improving the convergence speed of optimization algorithms is a significant concern in the machine learning community and there is ongoing research to find more efficient and faster algorithms that can train neural networks effectively.\tThis paper introduces a new online convex optimization algorithm that operates in via the reduction to online linear optimization in which the regret is bounded by $\\sum_{t=1}^T \\langle g_t, x_t - u\\rangle$ where $g_t$ is the gradient of the t^th loss at $x_t$. The algorithm is based on online mirror descent with non-decreasing quadratic regularizers $x^\\top H_t x$, using the update $x_{t+1} = argmin_x \\langle g_t, x \\rangle + (x-x_t)^\\top H_t(x-x_t)$ (or, equivalently using the terminology in the paper, $argmin \\langle g_t, x_t\\rangle /\\sqrt(t) + (x-x_t)^\\top H_t(x-x_t)$ where we replace $H_t$ by $H_t/\\sqrt{t}$. The analysis is restricted to diagonal $H_t$, for which we can break the regret into a sum of $d$ 1-dimensional problems, so it suffices to do the analysis in the scalar case. The idea is to break out the standard analysis of mirror descent regret as the sum over all t of $D^2(H_t - H_{t-1}) + g_t^2 H_t^{-1}$, where $D$ is the $\\ell_\\infty$ diameter of the domain, and then choose $H_t$ to minimize each of these terms greedily subject to the non-decreasing condition. A regret bound is provided for this algorithm that achieves worst-case $\\sqrt{T}$ regret, but in cases in which the gradients are small, the regret is much better. By employing this strategy on a per-coordinate basis one can obtain an adagrad-esque regret bound.""292\tThis paper provides an overview of reinforcement learning (RL) with Markov decision processes (MDPs), which is a type of dynamic decision-making problem. The paper focuses on the mean-variance trade-off, which is a key concept in RL that is used to balance the risk of poor performance with the potential for high reward. The paper\u4ecb\u7ecd\u4e86 various criteria that have been proposed to capture this risk, such as Value at Risk (VR) and variance. It also discusses some of the most popular methods for implementing these criteria in RL, including the policy gradient method, theActor-Critic method, and the multi-agent RL (MARL) framework. The paper concludes by providing an overview of the recent advances in this field and highlighting the potential applications of RL in various real-world problems.\tThe paper proposes an Expected Quadratic Utility Maximization (EQUM) framework for policy grandient Mean-Variance control. The authors claim that current state-of-the-art methods suffer either suffer from computational issues or cannot control risk at desiderable level, hence they propose their approach as a possible solution. They provide different interpretation for the EQUM: standard objective with a regularization, variance minimization with a constraint on expected return and a return targeting optimization. A policy gradient algorithm for EQUM is proposed together with its Actor-Critic extension.""293\tThis paper provides an overview of reinforcement learning (RL) with Markov decision processes (MDPs), a type of dynamic decision-making problem. The main objective of RL is to learn an optimal policy that maximizes the expected cumulative reward. However, in real-world applications, risk-aware decision-making has gained increasing attention, particularly in finance and robotics. The paper\u8ba8\u8bba\u4e86\u51e0\u79cd\u7528\u4e8e\u5904\u7406MDP\u4e2d\u98ce\u9669\u7684\u5e38\u89c1\u6807\u51c6\uff0c\u5305\u62ecValue at Risk (VAr)\u548cVariance\uff0c\u5e76\u4ecb\u7ecd\u4e86mean-variance trade-off(MVRL)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8bd5\u56fe\u540c\u65f6\u6700\u5927\u5316\u671f\u671b\u7d2f\u79ef\u5956\u52b1\u5e76\u6700\u5c0f\u5316\u65b9\u5dee\u3002\u5728\u8ba8\u8bba\u4e2d\uff0c\u4f5c\u8005\u4ecb\u7ecd\u4e86\u4f20\u7edf\u7684VAr\u65b9\u6cd5\u548cVariance\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u4f18\u70b9\u3002\u6700\u540e\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MVRL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236(ML-MPC)\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u98ce\u9669\u3002\tThe paper proposes a policy gradient style RL algorithm that optimizes an expected quadratic utility, a commonly used objective of risk management in finance and economics. The key idea here is based on the observation that when using the quadratic utility function, the use of mean-variance RL methods can be shown to optimize the utility of the agent. To this effect, the paper considers the use of expected quadratic utility maximization in the policy gradient. The quadratic utility can be naturally modeled using mean and variance. The paper implements two variations -- policy gradient and actor-critic with EQUM framework. ""294\tThis paper discusses the use of Reinforcement Learning (RL) to address the problem of Markov decision processes (MDPs) in the context of risk-aware decision-making. The paper highlights the importance of capturing the risk associated with the decision-making process in order to improve the performance of the policy. The paper also discusses various criteria used in the context of risk-aware decision-making, including Value at Risk (VR) andVariance. The paper ends with a summary of the main findings and a discussion of the potential implications of these methods for real-world applications.\tIn this paper the author proposed a new mean-variance algorithm whose policy gradient algorithm is more simpler than other SOTA methods and it has an unbiased gradient. Instead of formulating the problem as an traditional mean variance constrained problem, the authors utilized quadratic utility theory and formulate the problem as variance minimization problem with a mean reward equality constraint. Then by reformulating the problem with the penalized problem and opening up the variance formulation, they showed that this mean-variance formulation does indeed have an unbiased policy gradient, that does not require advanced techniques such as double sampling or frenchel duality. To demonstrate the effectiveness of this method on balancing risk and return, they also evaluate their methods on several risk-sensitive RL benchmarks (such as portfolio optimization) and compared with a wide range of risk-sensitive RL methods.""295\tTitle: Multiple Tasks for Deep Neural Networks: A Review of the Literature.\n\nAbstract: This paper provides a comprehensive review of the literature on the use of multiple tasks in deep neural networks. The focus of the review is on the impact of training multiple tasks on the performance of deep neural networks, especially in domains where there is limited data. The review\u53d1\u73b0\uff0c training with multiple tasks can significantly improve the performance of deep neural networks, especially in the low data regime. In some cases, it is easy to design beneficialauxiliary tasks and collect supervised data. The findings of this review suggest that training with multiple tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks.\n\nKeywords: multiple tasks, deep neural networks, performance, training, data.\n\nIntroduction: Deep neural networks have become a popular tool for solving complex problems in various domains, such as image and speech processing, natural language processing, and machine learning. However, training deep neural networks on a large number of tasks can be difficult, as the network may overfit to some tasks and not others. To address this issue, researchers have proposed the use of multiple tasks, which can help the network learn more meaningful representations and avoid overfitting to spurious correlations.\n\nTraining with multiple tasks adds an inductive bias that pushes learned models to capture meaningful representations and avoid overfitting to spurious correlations. This can improve the performance and generalization of deep neural networks. The performance of deep neural networks can be significantly improved by training the main task of interest with additionalauxiliary tasks (Goyal et al., 2019; Jaderberg et al., 2016; Mirowski, 2019). For example, learning to segment an image into objects can be more accurate when the model is simultaneously trained to predict other properties of the image like pixel depth or 3D structure (Standley et al., 2019). In the low data regime, models trained with the main task only are prone to overfit and generalize poorly to unseen data (Vinyals et al., 2016). In this case, the benefits of learning with multiple tasks are amplified (Zhang and Yang, 2017).\n\nTraining withauxiliary tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks. However, it is important to design beneficialauxiliary tasks and collect supervised data to ensure that the network is able to learn meaningful representations. In some domains, it may be easy to design beneficialauxiliary tasks and collect supervised data. For example, numerous tasks have been proposed for image and speech processing, such as object detection, image classification, and speech recognition (Table 1).\n\nThe literature on the use of multiple tasks in deep neural networks is vast and growing. However, the review\u53d1\u73b0\uff0c there is still a lack of clarity on the best practices for designing and collecting beneficialauxiliary tasks, as well as the best way to evaluate the performance of deep neural networks. The review suggests that there is a need for further research in this area to better understand the role of multiple tasks in deep neural networks and to develop effective strategies for training and evaluating these networks.\n\nTable 1: List of tasks proposed for image and speech processing.\n\n|  Task | Example tasks |\n| ---- | ---- |\n| Image classification |  detection of objects, image segmentation, image classification |\n| Image recognition |  speech recognition, object recognition, image classification |\n\nIntroduction: Convolutional neural networks (CNNs) have become the standard tool for solving tasks in image processing, such as object detection and image classification. However, these networks are not designed to perform multiple tasks, such as object detection and image segmentation. To address this issue, researchers have proposed the use of additional CNNs, or CNN-like networks, that can perform multiple tasks. These networks are typically designed to share some of the same components as a CNN, but with a different architecture and data\u7ed3\u6784\u7684\u8bbe\u7f6e\uff0c to perform multiple tasks.\n\n\u8bad\u7ec3with multiple tasks can significantly improve the performance of deep neural networks, especially in the low data regime. In some cases, it is easy to design beneficialauxiliary tasks and collect supervised data. The findings of this review suggest that training with multiple tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks.\n\nThe performance of deep neural networks can be significantly improved by training the main task of interest with additionalauxiliary tasks (Goyal et al., 2019; Jaderberg et al., 2016; Mirowski, 2019). For example, learning to segment an image into objects can be more accurate when the model is simultaneously trained to predict other properties of the image like pixel depth or 3D structure ( Standley et al., 2019). In the low data regime, models trained with the main task only are prone to overfit and generalize poorly to unseen data (Vinyals et al., 2016). In this case, the benefits of learning with multiple tasks are amplified (Zhang and Yang, 2017).\n\n\u8bad\u7ec3withauxiliary tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks. However, it is important to design beneficialauxiliary tasks and collect supervised data to ensure that the network is able to learn meaningful representations. In some domains, it may be easy to design beneficialauxiliary tasks and collect supervised data. For example, numerous tasks have been proposed for image and speech processing, such as object detection, image classification, and speech recognition (Table 1).\n\nThe literature on the use of multiple tasks in deep neural networks is vast and growing. However, the review\u53d1\u73b0\uff0c there is still a lack of clarity on the best practices for designing and collecting beneficialauxiliary tasks, as well as the best way to evaluate the performance of deep neural networks. The review suggests that there is a need for further research in this area to better understand the role of multiple tasks in deep neural networks and to develop effective strategies for training and evaluating these networks.\n\nThe performance of deep neural networks can be significantly improved by training the main task of interest with additionalauxiliary tasks (Goyal et al., 2019; Jaderberg et al., 2016; Mirowski, 2019). For example, learning to segment an image into objects can be more accurate when the model is simultaneously trained to predict other properties of the image like pixel depth or 3D structure ( Standley et al., 2019). In the low data regime, models trained with the main task only are prone to overfit and generalize poorly to unseen data (Vinyals et al., 2016). In this case, the benefits of learning with multiple tasks are amplified (Zhang and Yang, 2017).\n\n\u8bad\u7ec3withauxiliary tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks. However, it is important to design beneficialauxiliary tasks and collect supervised data to ensure that the network is able to learn meaningful representations. In some domains, it may be easy to design beneficialauxiliary tasks and collect supervised data. For example, numerous tasks have been proposed for image and speech processing, such as object detection, image classification, and speech recognition (Table 1).\n\nThe review suggests that there is a need for further research in this area to better understand the role of multiple tasks in deep neural networks and to develop effective strategies for training and evaluating these networks. This research may help to identify best practices for designing and collecting beneficialauxiliary tasks, as well as for evaluating the performance of deep neural networks.\n\nConclusion: This paper provides a comprehensive review of the literature on the use of multiple tasks in deep neural networks. The review\u53d1\u73b0\uff0c training with multiple tasks can significantly improve the performance of deep neural networks, especially in the low data regime. In some cases, it is easy to design beneficialauxiliary tasks and collect supervised data. The findings of this review suggest that training with multiple tasks can provide a powerful strategy for improving the performance and generalization of deep neural networks.\n\nThe performance of deep neural networks can be significantly improved by training the main task of interest with additionalauxiliary tasks (Goyal et al., 2019; Jaderberg et al., 2016; Mirowski, 2019). For example, learning to segment an image into objects can be more accurate when the model is\tThis paper pinpoints the key issues of Auxiliary Learning: (1). how to design useful auxiliary tasks, (2) how to combine auxiliary tasks into a single coherent loss. Motived by the issues, this paper proposes a novel Auxiliary Learning frame work, named AuxiLearn. The paper is globally well organized and clearly written. ""296\tTitle: Benefits of Training Neural Networks withauxiliary tasks: A Review of the literature.\n\nIntroduction: Neural networks have revolutionized machine learning and are widely used in a variety of fields. However, training neural networks to perform well on multiple tasks is still an area of ongoing research.\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u989d\u5916\u4efb\u52a1\u4ee5\u6539\u5584\u6027\u80fd\uff1a\u5f15\u8a00. \n\n\u65b9\u6cd5\uff1a \n\n1. \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e3a\u4e3b\u4efb\u52a1\u548c\u989d\u5916\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002 \n\n2. \u5728\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u8bad\u7ec3\u4e3b\u4efb\u52a1\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002 \n\n3. \u5728\u989d\u5916\u4efb\u52a1\u7684\u5e2e\u52a9\u4e0b\uff0c\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002 \n\n4. \u5728\u4e00\u4e9b\u9886\u57df\uff0c\u8bbe\u8ba1\u6709\u76ca\u989d\u5916\u4efb\u52a1\u548c\u6536\u96c6\u76d1\u7763\u6570\u636e\u53ef\u80fd\u5bb9\u6613\u3002 \n\n5. \u8fd9\u4e9b\u65b9\u6cd5\u7684\u7814\u7a76\u63ed\u793a\u4e86\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u989d\u5916\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \n\n\u7ed3\u679c\uff1a \n\n1. \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e3a\u4e3b\u4efb\u52a1\u548c\u989d\u5916\u4efb\u52a1\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002 \n\n2. \u5728\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u8bad\u7ec3\u4e3b\u4efb\u52a1\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002 \n\n3. \u8bad\u7ec3\u989d\u5916\u4efb\u52a1\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002 \n\n4. \u5728\u4e00\u4e9b\u9886\u57df\uff0c\u8bbe\u8ba1\u6709\u76ca\u989d\u5916\u4efb\u52a1\u548c\u6536\u96c6\u76d1\u7763\u6570\u636e\u53ef\u80fd\u5bb9\u6613\u3002 \n\n5. \u8fd9\u4e9b\u65b9\u6cd5\u7684\u7814\u7a76\u63ed\u793a\u4e86\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u989d\u5916\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \n\n\u8ba8\u8bba\uff1a \n\n\u672c\u7efc\u8ff0\u8ba8\u8bba\u4e86\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u989d\u5916\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u5df2\u6709\u7814\u7a76\u7684\u7ed3\u679c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e3a\u4e3b\u4efb\u52a1\u548c\u989d\u5916\u4efb\u52a1\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u3002\u8bad\u7ec3\u989d\u5916\u4efb\u52a1\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002\u5728\u4e00\u4e9b\u9886\u57df\uff0c\u8bbe\u8ba1\u6709\u76ca\u989d\u5916\u4efb\u52a1\u548c\u6536\u96c6\u76d1\u7763\u6570\u636e\u53ef\u80fd\u5bb9\u6613\u3002\u672c\u7efc\u8ff0\u8fd8\u8ba8\u8bba\u4e86\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u989d\u5916\u4efb\u52a1\u7684\u4e00\u4e9b\u5e38\u89c1\u65b9\u6cd5\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u4e2d\u3002\tThis paper studies a variant of multi-task learning, auxiliary learning, where one main task dominates, and other tasks are used to learn a good representation. To achieve this goal, the authors propose a learning-to-learn algorithm. In particular, the auxiliary losses are represented by a vector and then transformed to a new loss term via linear or nonlinear function $h$. They also made two more contributions. First, an approach of new auxiliary task generation is proposed. Second, an implicit differentiation based optimization method is proposed to find the solution. Both theoretical analysis and empirical studies demonstrate the superiority of their proposed model.""297\tThis paper discusses the use of additionalAuxiliary tasks in deep neural networks for improved performance. The paper provides an overview of the benefits of training withauxiliary tasks, including increased generalization and reduced overfitting, when compared to training with the main task only. The paper also discusses the challenges of training models with additional tasks, such as the need for high data availability and the potential for overfitting. The paper provides an example of how auxiliary tasks can be designed and implemented in a number of domains, and concludes by highlighting the potential for using multiple tasks in deep learning to improve the performance of complex systems.\tThe paper proposes AuxiLearn, a framework that can be used to combine losses from multiple auxiliary tasks (if present) into a single combined, loss function that does not require expensive grid search over possible linear combination. It uses an implicit differentiation-based approach to train a (deep) non-linear network that weighs the various auxiliary losses to optimize the generalization capabilities of the network. In the absence of such pre-defined tasks, a variation of the approach, using teacher-student networks, helps create relevant tasks to improve the performance of the network. Experiments across tasks such as classification (both few-shot and with limited labels) and segmentation show that the approach helps improve the performance of the model on the main task.""298\tThis paper discusses the replacement of statistical machine translation (SMT) with neural machine translation (NMT) in the field, along with the changes that have occurred and the\u5931\u53bb\u4e86 properties of the old SMT models. The paper also analyzes the performance gain brought by NMT and the importance of the desirable properties of the old SMT models. The paper also discusses some potential future directions for the field.\tThe paper proposes a technique for assessing the uncertainty of a Transformer-based NMT model on a given input $x$. The technique relies on computing a variance-like estimate over a collection of translation candidates for $x$, where these candidates are obtained by perturbing the decoding mechanism through the use of dropout at test time. Experiments compare this technique with other ways of measuring the \"epistemic uncertainty\" of the NMT model. In limited training data conditions, the proposed measure is better aligned with the actual performance of the model than competing measures, and in particular is better able to detect Out-of-Domain translation requests.""299\tThis paper presents an overview of the statistical machine translation (SMT) and neural machine translation (NMT) fields, as well as their recent developments and challenges. SMT has been based on probabilistic models such as the IBM alignment models (Vogel et al., 1996; Brown et al., 1993; Gal & Blunsom, 2013) and has been widely used for machine translation. NMT, on the other hand, has emerged in recent years and has brought with it huge performance gains (Sennrich et al., 2016). However, SMT has lost many of its desirable properties, such as the ability to guess at random on inputs it has never seen before ( Ghahramani, 2015). The paper discusses how this loss has affected the development of SMT and NMT, and how new approaches can be used to address these challenges.\tThe paper proposed a Baysian method for detecting out of distribution (OOD) in machine translation. To this end, the paper introduces BLEU variance (BLEUVar) that is computed based on a number of samples from Transformer with MC Dropout. The advantage of BLEUVar is that it doesn\u2019t require reference, instead it\u2019s computed based on pairwise comparison of the decoded sentences.""300\tThis paper discusses the recent rise in the use of Neural Machine Translation (NMT) methods, which have replaced statistical machine translation (SMT) in many applications. The paper highlights the benefits of NMT, such as improved performance and the ability to handle uncertainty, but also notes that many of the properties of SMT are now lost in NMT. The paper also discusses the challenges of using NMT in practice, including the need for large amounts of training data and the need for proper handling of uncertainty.\tThis paper describes a method for estimating a neural machine translation (NMT) system's uncertainty about its translation of a sentence that has two parts: (1) use MC Dropout as a proxy for integrating out parameters; (2) two uncertainty metrics (probability of translation summing over randomly-sampled parameters and variance in BLEU using randomly-sampled parameters). The baseline method is just to use the probability of the 1-best translation under the MLE parameters. The method is evaluated by measuring the BLEU score of a test set retaining only the most-certain fraction of the sentences.""301\tThis paper discusses the possibility of pruning neural networks during training to reduce computational demands. The paper starts by discussing the history of neural network training and the ability to eliminate a significant number of parameters without affecting accuracy at inference time. It also discusses the recent development in hardware designed to exploit sparsity and the ability to conduct neural network pruning during training. The paper also discusses the challenges of early network pruning and the potential benefits of doing so.\tGenerating a pruned network falls into two broad categories: 1) spend some extra time and effort to train or fine-tune the pruned model after first training a dense version, or 2) cut out that extra time and effort by generating a sparse network \"from scratch.\"  While approach (1) has historically given the best accuracy, recent advances (such as the lottery ticket hypothesis) suggest that there are sparse networks hidden in the initialization that don't need to first be trained, if only we could divine the structure of those models.  Approach (2) seeks to do just this: determine the connectivity as close to initialization possible.  However, even the best results taking this second path fall short when compared to the accuracy of the former path - why is this?  The submission pokes at three recent techniques to pull out some commonalities that are *not* shared with (1), suggesting possible issues that need to be overcome to improve accuracy, and proposes a set of experiments and comparisons that should be part of any new technique that claims to discover a good sparse mask at initialization.""302\tThis paper discusses the possibility of pruning neural networks early in training or before training to reduce computational demands. The paper discusses the history of neural network pruning, the potential benefits of pruning, and the current research in this area. The paper also mentions the use of hardware and libraries to facilitate pruning and the recent findings of pruned neural networks in tasks such as image recognition and machine learning.\tThe paper provides an extensive empirical analysis of Pruning-at-Initialization (PaI) techniques and compares it against two pruning methods after (or during) training. This comparison sheds some light on why pruning at initialization is inherently hard. Furthermore, the comparison among PaI methods with various ablations shows some inherent properties that are common to PaI methods and the benefits/drawbacks of certain methods. With these experiments, certain conclusions are reached among them an important one is that PaI methods only determine what is the fraction of weights to be pruned in each layer rather than which weights to prune.""303\tThis paper explores the idea of pruning neural networks in order to reduce computational demands while still maintaining accuracy. It notes that neural network pruning has been known for over 30 years, and that it is possible to eliminate a significant number of parameters without affecting accuracy at inference time. The paper\u8ba8\u8bbas the effects of pruning on the computational demands of inference and how it can be conducted on hardware or with libraries and hardware designed to exploit sparsity. It also discusses the challenges of early or even before training pruning, and how researchers are exploring the possibility of reducing the computational demands of training by pruning networks early in training or even before training.\tA recent trend of 'pruning at initialization' in neural network pruning has left me baffled. It's counter-intuitive that neural networks can be pruned at initialisation, improving results for the training done thereafter. Nitpicking semantics, one could hardly even call this a pruning technique, since there is no a-priori knowledge of the dataset distilled in the network. Perhaps it's more aptly referred to as a method of sparse initialisation methods.""304\tThis paper provides an overview ofFederated Learning (FL), a new distributed learning paradigm that preserve users' privacy when all data is held locally by each user. The paper\u63a2\u8ba8\u4e86FL\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86 secure aggregation technique \u6765\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u63a2\u8ba8\u4e86\u5728FL\u7cfb\u7edf\u4e2d\u5982\u4f55\u8bc6\u522b\u6076\u610f\u5ba2\u6237\u7aef\u5e76\u63d0\u51fa\u4e86SGD\u4f5c\u4e3aFL\u7b97\u6cd5\u7684\u65b0\u6807\u51c6\u3002\u6700\u540e\uff0c\u8be5\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86FL\u5728\u9690\u79c1\u654f\u611f\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982Google GBoard\u3001 healthcare \u670d\u52a1\u548c self-driving cars \u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136FL\u5047\u8bbe\u6240\u6709\u7528\u6237\u90fd\u8bda\u5b9e\u5730\u4e0a\u4f20\u81ea\u5df1\u7684\u672c\u5730\u66f4\u65b0\uff0c\u4f46\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5b58\u5728\u6076\u610f\u5ba2\u6237\u7aef\u3002\u56e0\u6b64\uff0c\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u673a\u5236\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684FL\u7b97\u6cd5\u6807\u51c6\u3002\tThe paper claims to be the first paper that simultaneously handles Byzantine threats while ensuring privacy in a federated learning setup. One of their main claims is that this is the first algorithm that provides dimension independent robustness guarantees against byzantine threats (I have some concerns regarding this claim). The algorithm first divides all the machines into shards. Within each shard there is secure aggregation. Finally, the outputs of each shard is robustly aggregated such that the error isn't dimension dependent.""305\tThis paper discusses the topic of Federated Learning (FL), a new distributed learning paradigm that allows users to collaborate with a centralized server while preserving their privacy. The paper presents a threat model for FL systems and discusses how to enhance the privacy guarantee of the users. It also explains the benefits of FL, such as its ability to be used in a variety of sensitive applications, and the graceful balance between utility and privacy that it promotes. Finally, the paper concludes by discussing some potential challenges and future directions for FL.\tThe paper considers robustness to poisoning and backdoor attacks in the context of federated learning. It proposes a defence based  on splitting the clients into shards, averaging their updates via secure aggregation and then using a robust mean estimation on top to ensure robustness. The authors point out that controlling the number of shards is a way to trade-off privacy vs robustness, thus potentially dealing with both malicious clients and an honest, but curious server. The paper provides some theoretical justification for the algorithm, as well as an experimental evaluation where its performance is tested against multiple attacks and compared to other existing methods.""306\tThis paper discusses the importance of privacy in distributed learning, specificallyFederated Learning (FL), which is a new paradigm for training models in a distributed manner using data held locally by each user. The paper highlights the benefits of FL, including improved efficiency and privacy preserving practices. It also discusses the potential threats to privacy in a FL system, including the possibility of malicious clients and the need for secure aggregation techniques to enhance the privacy of the users. The paper also discusses the latest developments in FL, such as the use of deep learning algorithms and the development of secure FL systems.\tThe authors consider federated learning setting and how to defend the overall learning task against malicious clients and a semi-honest centralized server. Though there are known ways to prevent attacks, they suffer from a large error in the estimator and also do not preserve privacy of updates since the server sees them in the clear in order to adjust for error. This paper proposes a sharding technique and use of the estimator method whose error does not depend on the number of dimensions as previous work.""307\tThe paper discusses the problem of automatically synthesizing designs that maximize a desired objective function in various scientific and engineering domains. The objective function may be unknown, and the values for a novel design can only be evaluated by running computer simulations or physical experiments. The process of optimizing an unknown function is known as black-box optimization and is typically solved in an online iterative manner. The paper discusses recent advances in using machine learning methods to address this problem, including applications to protein design, superconducting material discovery, and other domains.\tThis paper focuses on model-based black box optimization problems in the offline setting. These are settings where access to ground truth is expensive, and instead the optimizer has access only to a trained model of the ground truth based on limited data. While optimizing on this surrogate space, a good optimizer often needs to account for model uncertainty and accuracy degradation. The main aim of the paper is to provide a test bed for algorithms that try to solve this challenge. ""308\tThe paper discusses the use of machine learning methods to automatically synthesize designs that maximize a desired objective function in various scientific and engineering domains. The problems involved in this task are typically characterized by the fact that the exact form of the objective function is unknown, and the objective values for a novel design can only be evaluated through computer simulations or physical experiments. The paper provides an overview of the different approaches that have been used in the field, including traditional optimization methods and machine learning-based methods, and discusses the advantages and limitations of each. The paper also highlights the need for further research in this area to advance the state-of-the-art and make it more practical for real-world applications.\tThis paper studies the evaluation of offline black-box optimization algorithms. The community currently lacks a standardized benchmark to compare the performance of methods. This paper presents a new suite of offline model-based optimization tasks and standardized evaluation procedures for the community. The evaluation criterion for the quality of a benchmark is the realism and diversity of the tasks, with special consideration for high-dimensional design space and the objective function's sensitivity. The paper then evaluates several algorithms on the benchmark. ""309\tThe paper discusses the problem of automatically synthesizing designs that maximize a desired objective function in various scientific and engineering domains. The exact form of the objective function may be unknown, and the objective values for a novel design can only be evaluated by running computer simulations or physical experiments. The process of optimizing an unknown function is known as black-box optimization and is typically solved in an online iterative manner.\u7814\u7a76\u4eba\u5458 have made significant progress in applying machine learning methods to such optimization problems.\tThis paper proposes a benchmark suite of offline model-based optimization problems. This benchmark includes diverse and realistic tasks derived from real-world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from""310\tThis paper discusses the availability of multiple data types and their potential for learning representations that generalize well across multiple modalities. It also discusses the added self-supervision that occurs when multiple data types are present, which allows for efficient learning from multiple data types in a self-supervised fashion. The paper also explores the challenges and research questions related to understanding the interplay between different data types and how to effectively train models that learn from multiple data types in a self-supervised fashion. Finally, the paper presents some potential solutions to these challenges and discusses the future directions for studying the interplay between different data types and machine learning.\tThis paper focuses on providing a more generalize multimodal ELBO to encompass previous PoE and MoE as special cases and combines their benefits. To this end, the authors first define the new ELBO L_{MoPoE} which is an interesting extension of PoE and MoE. Different from PoE (product of experts) and MoE (mixture of experts), MoPoE (mixture of product of experts) explores a more general way by mixing more experts where each expert is the product of a subset of all modalities\u2019 posterior. In this way, as illustrated by the author, PoE and MoE can be seen as specific cases of MoPoE easily. The proposed model achieves competitive results compared with PoE and MoE.""311\tThis paper discusses the availability of multiple data types, their promise for learning representations that generalize well across multiple modalities, and the value of additional self-supervision in the form of shared information connecting the different data types. It also discusses the challenges of learning from multiple data types, including the labeling of multiple data types, and proposes generative self-supervised models as a solution. The paper ends by discussing future directions and challenges in learning from multiple data types.\tThe paper combines ideas from two previous works (MVAE and MMVAE) to propose a new multimodal formulation of the ELBO for VAEs. The approximate posterior consists of a mixture of subsets, with each subset a product of approximate posteriors for each modality. This generalizes previous works, and the authors show that their objective yields a lower bound on the full data log-likelihood. They also compare their approach with MVAE and MMVAE on three multimodal datasets, showing benefits in classification accuracy, coherence, and log-likelihood. Although the proposed method is not superior in all cases, it generally achieves a reasonable trade-off across performance metrics.""312\tThis paper discusses the availability of multiple data types, the promise of learning representations that generalize well across multiple modalities, and the importance of self-supervision in these settings. It also explores the relationship between different data types and the interplay between them, as well as the challenges of labeled data in learning from multiple data types. The paper proposes that self-supervised generative models are suitable for learning the joint distribution of multiple data types, and provide a efficient way to learn from multiple data types in a self-supervised fashion.\tThis paper formulates a multimodal ELBO as a mixture of product of experts. This allows them to use one encoder per mode while still allowing inference over any subset of modes without needing a new encoder for each subset. The idea is simple and appears to improve on baselines derived from a mixture or a product of experts.""313\tBayesian Optimization (BO) is a data-efficient method for joint optimization of design choices. It is popular in a wide range of areas, including hyperparameter optimization, AutoML, robotics, computer vision, environmental monitoring, combinatorial optimization, experimental design, RL, Computer Go, hardware design, and many others. It promises greater automation and increase in product quality and human productivity. BO is established in many large tech companies such as Google Vizier and Facebook BoTorch.\tThe goal of this paper is to enable the introduction of prior expert knowledge in Bayesian optimization. This is performed by defining a prior distribution for the optimal value, which is included in the pseudo-posterior used to select new points by Expected Improvement. The number of iterations it takes to overcome a potentially wrong prior information is controlled by a parameter, whose sensitivity is studied. Extensive experiments are conducted on toy examples as well are more realistic hyper-parameter test cases.""314\tBayesian Optimization (BO) is a data-efficient method for joint optimization of design choices. It gained great popularity in recent years and is impacts a wide range of areas, including hyperparameter optimization, AutoML, robotics, computer vision, environmental monitoring, combinatorial optimization, experimental design, RL, Computer Go, hardware design, and many others. It Promises greater automation so as to increase both product quality and human productivity. BO is established in many large tech companies such as Google Vizier and Facebook BoTorch.\tThis paper incorporates a prior distribution given by experts into Bayesian optimization (BO), to leverage useful human knowledge to accelerate BO. The algorithm uses an intuitive approach to combine the prior with the probabilistic surrogate model of BO to derive a pseudo-posterior, which naturally leads the EI acquisition function. As the BO progresses, the prior information is gradually overwhelmed by the observed data, which ensures the asymptotically correct behaviour.""315\tBayesian Optimization (BO) is a data-efficient method for joint optimization of design choices that gained great popularity in recent years. It is impact a wide range of areas, including hyperparameter optimization, AutoML, robotics, computer vision, environmental monitoring, combinatorial optimization, experimental design, RL, Computer Go, hardware design, and many others. BO promises greater automation so as to increase both product quality and human productivity. It is established in many large tech companies, including Google Vizier, Facebook BoTorch, and many more.\tThe paper presents a novel method to incorporate experts' knowledge into BO. This is done through introducing  Prior-guided Bayesian Optimization (PrBO). Different experiments where conducted to compare PrBO vs different baselines and to show the effect of the user provided priors in the cases where it is well-specified or mis-specified. The design of PrBO enables it to guide the search in the early iterations and as optimization progresses, more emphasis is given to the model and the effect of the prior is washed out.""316\tThis paper discusses the use of binary neural networks, which have weights and activations that are only binary-valued, to reduce the memory and computational requirements of machine learning models. The paper also discusses the potential applications of binary neural networks, including in unsupervised learning and deep generative models. The paper concludes by discussing the limitations of using binary neural networks, including their potential for accuracy issues and limited applicability to certain types of data.\tThe paper introduces the first way (to the best of authors knowledge) of building generative models with binary weights. Also the case with binary activations is considered. The authors consider two SOTA generative models (flow++ and RVAE) and develop technique to binarize all weights (and possibly activcations) in residual layers. They show that residual layers can be binarized with relatively small drop in performance and further binarization of remaining blocks in computational graph leads to significant degradation. Binary modification of weight normalization is suggested although no ablation is study is performed so it is unclear how crucial is BWN for robust learning. The training process itself is pretty standard way of training binary DNNs - they use STE + truncation of real-valued weights counter-parts.""317\tThis paper discusses the use of binary neural networks, also known as binary decision trees, for unsupervised learning and classification problems. The paper\u8ba8\u8bba\u4e86\u5982\u4f55\u4f7f\u7528\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c unsupervised learning\u548c\u5206\u7c7b\u95ee\u9898\u3002\u5728\u8ba8\u8bba\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6307\u5b9a\u76ee\u6807\u7c7b\u522b\uff0c\u8fd9\u53ef\u4ee5\u907f\u514d\u4e00\u4e9b\u9519\u8bef\u3002\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u548c\u9884\u6d4b\u65b9\u9762\u7684\u4e00\u4e9b\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\tThe authors propose to binarize weights and activations of generative VAE and Flow++ models. As Weight Normalization is commonly used in these models, the authors notice that Euclidean norm of binary [-1;1] vector is a square root of it\u2019s length, such that Weight Normalization can be reduced to affine scaling. They propose to call this scaling Binary Weight Normalization, and evaluate it on CIFAR and ImageNet datasets.""318\tThis paper discusses the use of binary neural networks, which have weights and activations that are only binary-valued, to reduce the memory and computational requirements of machine learning models. The paper\u8ba8\u8bba\u4e86\u5982\u4f55\u4f7f\u7528\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u6765\u964d\u4f4e\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8981\u6c42\u3002\u867d\u7136Binary Neural Networks \u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728 unsupervised learning \u4efb\u52a1\u4e2d\u76ee\u524d\u8fd8\u6ca1\u6709\u76f8\u5173\u7814\u7a76\u3002\u4e8b\u5b9e\u4e0a\uff0c\u8bb8\u591a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\uff0c\u5e76\u6ca1\u6709\u8003\u8651\u4f7f\u7528\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u3002\u672c\u8bba\u6587\u63d0\u4f9b\u4e86\u6709\u5173\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u8981\u6c42\u3001\u63d0\u9ad8\u6a21\u578b\u751f\u6210\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002\tThis paper describes a method to binarize weights and activations of variational autoencoders and flow-based networks.  This is an important issue as these methods are valuable to solving unsupervised problems, but are rapidly growing in size, necessitating large and expensive computing systems. And, the literature of low and binary precision hasn\u2019t considered these use-cases to date.""319\tThis paper discusses the challenges of deep learning (DL) in safety-critical domains and proposes a number of solutions. It emphasizes the importance of robust and trustworthy DL systems in\u786e\u4fdd\u5b89\u5168\u6027\uff0c especially in safety-critical applications such as medical imaging, clinical trials, and robotics. The paper describes the known challenges of DL in these domains, including its fragility to seemingly innocuous changes to the input data, and proposes a number of adversarial training algorithms and certifiable defenses as ways to address these issues. The paper also discusses the potential applications of these solutions in improving the safety and\u53ef\u9760\u6027 of DL systems in these domains.\tThe paper extends current adversarial learning approaches beyond imperceptible L_p norm perturbations. The proposed approach can handle many models of natural variation, such as a change in brightness. The main idea behind the approach is to use unsupervised approaches such as GANs to model the natural variation. Given this model of natural variation, the paper replaces the adversarial learning objective of finding the worst example in an L_p norm ball around a data point to finding the worst example based on the model of natural variation. This is expensive, so the paper also proposes more computationally efficient approaches based on data augmentation. The experimental results demonstrate that the proposed approach performs well on a variety of tasks.""320\tThis paper discusses the recent advances in deep learning (DL) and the challenges that arise when using it in safety-critical domains. It also discusses the recent efforts to address these challenges by proposed adversarial training algorithms as well as certifiable defenses. The paper starts by\u4ecb\u7ecdDL\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u7684\u8fdb\u5c55\uff0c\u4ee5\u53caDL\u5728\u8bf8\u591a\u5e94\u7528\u9886\u57df\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5305\u62ec\u5b89\u5168\u76f8\u5173\u7684\u5e94\u7528\u9886\u57df\u3002\u7136\u540e\uff0c\u5b83\u63d0\u5230DL\u5728 input data\u4e0a\u5b58\u5728\u770b\u4f3c\u65e0\u5bb3\u7684\u566a\u58f0\u65f6\u4f1a\u53d8\u5f97\u8106\u5f31\uff0c\u5e76\u5217\u4e3e\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u4f8b\u5b50\u3002\u63a5\u4e0b\u6765\uff0c\u8be5 paper \u8ba8\u8bba\u4e86\u51e0\u79cd\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6cd5\uff0c\u5305\u62ec adversarial training algorithms \u548c certifiable defenses\u3002\u6700\u540e\uff0c\u8be5 paper \u603b\u7ed3\u4e86\u8fd9\u4e9b\u52aa\u529b\u7684\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u5efa\u8bae\u3002\tThis paper proposes a model-based framework for improving the robustness of image classifiers to average-case corruptions of varying severity. The proposed framework can be thought of as adversarial training where the perturbation is replaced by a function that transforms the image according to a specific corruption. A nuisance parameter controls the instantiation and severity of the corruption that is applied to the input. The paper compares baselines to different versions of this general model-based framework with experiments on several datasets.""321\tThis paper discusses the recent concern about the fragility of deep learning (DL) systems in various application domains, including image classification, clinical trials, and robotics. It also highlights the need for adversarial training algorithms and certifiable defense methods to ensure the safety and trustworthiness of these systems. paper discusses the recent concern about the fragility of deep learning (DL) systems in various application domains, including image classification, clinical trials, and robotics. It also highlights the need for adversarial training algorithms and certifiable defenses to ensure the safety and trustworthiness of these systems. The paper begins by providing an overview of the DL framework and the current state of the art in the field. It then discusses the known fragility of DL systems to seemingly innocuous changes to the input data, including examples from image classification, clinical trials, and robotics. The paper then proposes a number of adversarial training algorithms as well as certifiable defenses for ensuring the safety and trustworthiness of DL systems. Finally, the paper concludes by highlighting the importance of these methods and discussing the future directions of the field.\tThis paper proposes a \u201cparadigm shift\u201d for augmenting datasets when training CNN-based image classifiers. On one side, traditional augmentations include blur, Gaussian noise, color distortions. On the other side, methods like adversarial training consider augmentations under norm bounds in the image space. The proposed method, instead, uses models of natural variation to augment the images. Increased out-of-domain accuracy is shown on ImageNet-C and several slices of the CURE-TSR dataset. ""322\tThis paper studies the training problem of Convolutional Neural Networks (CNNs) with ReLU activations and proposes equivalent finite dimensional convex formulations that can be used to globally optimize these architectures. The paper also characterized the role of network architecture in terms of equivalent convex regularizers. Remarkably, the proposed methods are prove to be polynomial time with respect to all problem parameters. This work\u5148\u524d\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570(\u5982ReLU)\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6a21\u578b\u3002\u7136\u800c\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7ed3\u6784\u590d\u6742\u6027\u4f7f\u5f97\u5bf9\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86CNN\u6a21\u578b\u7684\u8bad\u7ec3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86 equivalent convex \u7684\u6a21\u578b\u8868\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e globally optimizing CNN \u6a21\u578b\u3002\u8be5\u7814\u7a76\u8fd8\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u4f18\u5316CNN \u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u8be5\u7814\u7a76\u5148\u524d\u5728 Bengio \u7b49\u4eba(2006)\u548c Bach(2017) \u7814\u7a76\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u62d3\u5c55\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u5c40\u9650\u4e8e\u5177\u6709\u65e0\u9650\u5bbd\u5ea6\u76842\u5c42\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u5bf9\u4e8eCNN \u6a21\u578b\u7684\u8bad\u7ec3\u7814\u7a76\u5177\u6709\u5f00\u62d3\u6027\u3002\tThe paper studies the non-convex optimization problem of training CNNs with ReLU activations under different choices for the CNN architecture, and shows how these can be framed as convex problems with a poly time complexity w.r.t. relevant variables. The derived convex problems provide valuable insights on how the CNN architecture induces different weight regularizers by giving them in explicit form -- these show a rich connection between the architecture and regularizer.""323\tThis paper studies the training problem of Convolutional Neural Networks (CNNs) with ReLU activations and introduces equivalent finite dimensional convex formulations that can be used to globally optimize these architectures. The paper also characterized the role of network architecture in terms of equivalent convex regularizers. Remarkably, the proposed methods are polynomial time with respect to all problem parameters. This paper adds to the body of research on CNN training and provides a mathematical framework for understanding and improving the performance of CNNs on a wide range of tasks.\t[Summary] This paper focuses on training convolutional neural networks (CNNs) by using convex optimization techniques. By taking the dual of the nonconvex training problems, (and the dual of its dual), the main contribution of the paper is to show the strong duality between the convex problem and its original nonconvex training problems. This result has been proved for multi-layer CNNs with one ReLU layer and three-layer CNNs with two ReLU layers.""324\tThis paper studies the training problem of convolutional neural networks (CNNs) with ReLU activations and\u63d0\u51fa equivalent finite dimensional convex formulations that can be used to globally optimize these architectures. The paper characterizes the role of network architecture in terms of equivalent convex regularizers. Recall that CNNs have shown a remarkable success across various machine learning problems ( LeCun et al., 2015 ). However, our theoretical understanding of CNNs still remains limited, where the main challenge arises from the highly non-convex and nonlinear structure of CNNs with nonlinear activations such as ReLU. The paper suggests that using equivalent convex formulations can provide a more efficient and effective way to globalize the training problem of CNNs. The proposed methods are polynomial time with respect to all problem parameters. The paper also includes previous studies on CNN training ( Bengio et al., 2006 ; Bach, 2017 ), which are limited to two-layer fully connected networks with infinite width.\tThe paper considers several types of CNN and proposes convex reformulations for non-convex problems of training these networks. As a result a polynomial complexity is shown for the training problem. The results are also interpreted as implicit regularization induced by the choice of the architecture. Finally, numerical experiments are made to support the theoretical findings and show that in the predicted regime, SGD for the original problem converges to the global minimizer given by the convex reformulation.""325\tThis paper discusses the use of learning from demonstration (LfD) as a method for teaching a robot how to perform a particular task in its environment. LfD is a technique where a potentially imperfect demonstrator desires to teach a robot how to perform a particular task, and the robot is trained through a combination of kinaesthetic teaching and supervised learning -imitation learning. However, this approach has been shown to be unable to incorporate elaborations and corrections from the demonstrator seamlessly, leading to the need for new demonstrations when the demonstrator changes the task specification or the environment changes. To address this issue, the paper discusses an LfD setup that requires establishing a mapping from the high-level task specification to a lower-level notion or mechanism that the demonstrator and the robot use. This setup can accommodate abstract user specifications and allows for seamless integration of elaborations and corrections.\tThis is a well-written paper that discusses how to learn disentangled representations for the learning from demonstrations (LfD) task in robotics. It is shown that using weak-supervision on top of unsupervised learning frameworks (that use the variational autoencoder for instance) can work well in this case. These disentangled factors of variation in the data are shown to correspond well to the 'abstract concepts' of the human demonstrations. This is shown in the example of the PR2 robot dabbing demonstrations, including visual data as well robot trajectories. ""326\tThis paper discusses the use of Learning from Demonstration (LfD) as a method for teaching a robot how to perform a particular task in its environment. LfD involves a potentially imperfect demonstrator teaching the robot through kinaesthetic teaching and supervised learning -imitation learning. However, this approach can suffer from the need for new demonstrations when the demonstrator changes the task specification or the context, which results in the need for new instructions. The paper\u63a2\u8ba8\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5efa\u7acb\u6620\u5c04\u6765\u5c06\u9ad8\u7ea7\u522b\u6982\u5ff5\u4e0e\u4f4e\u7ea7\u522b\u6982\u5ff5\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u4fbf\u80fd\u591f seamlessly incorporate elaborations and corrections from the demonstrator.\tUnder the context of learning\u00a0from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations\u00a0from high dimensional multimodal inputs using\u00a0weak supervision. Paper argues that since robots and humans have different levels of abstractions and mechanisms, observation+action spaces between them are greatly misaligned which\u00a0complicates learning by directly observing humans. However, the underlying concepts essential for tasks\u00a0lie in a much lower-dimensional manifold. Learning this\u00a0manifold\u00a0effectively and in an interpretable way, especially using weak supervision, can significantly change how robots\u00a0can acquire skills from demonstrations and generalize them to new unseen scenarios. Towards this end, the paper proposes to learn probabilistic\u00a0generative models capturing high-level notions from demonstrations using variational inference. The strength of the paper is in demonstrating that conditional latent variable models can learn disentangled\u00a0low dimensional represented using weak supervision;\u00a0which authors effectively demonstrated\u00a0using real-world\u00a0experiments. My main reservations are in terms\u00a0of the technical novelty of the paper and the narrow scope of experimental evaluation.""327\tThis paper discusses the use of Learning from Demonstration (LfD) as a method for teaching a robot how to perform a particular task in its environment. The paper highlights the challenges that arise when using this paradigm, including the need for\u5efa\u7acb\u6620\u5c04\u6765\u878d\u5408 Demonstrator \u7684\u4fee\u6b63\u548c elaborations , and the need for a higher level of Abstraction in the demonstrator and the robot to allow for seamless incorporated of changes in the task specification or environment. The paper also discusses the potential benefits of using LfD, including the ability to teach complex tasks and the ability to accommodate abstract user specifications.\tThis paper presents a way to learn from demonstrations with weak or no labels. The premise behind this paper is that even when humans provide labels during a demonstration, those labels often do not fully describe the data (e.g., the human may say \"soft\" when \"fast\" would also apply). This paper presents a technique that uses latent variables to model the uncertainty over a group of class labels that could describe the task (e.g., slow, soft, left-of-object). The variables are modeled such that the observation is conditionally independent of the human provided labels given the latent variables. This allows the human provided labels to be decoupled (or disentangled as the paper calls it) from the observations. By doing so, it is possible to have only partial labels (weak labels). This model was applied to a task where a human would teleoperate a robot arm and apply a dabbing motion in relation to an object in the scene. The operator would provide only one of several possible applicable labels for each demonstration. The results show that the models using the weak labeling out-performed models with no labeling.""328\tThis paper discusses the use of transfer learning in natural language processing (NLP) and the potential risks associated with using large-scale language models pretrained on text data. The paper highlights the advantages of using transfer learning, such as improving performance on a wide range of NLP tasks, but also notes the potential risks, including overfitting. The paper also discusses the ways to mitigate these risks, such as using smaller models or increasing the amount of target task data. Overall, the paper provides an overview of the current state of transfer learning in NLP and offers insights into how it can be used to achieve better performance.\tThis paper studies fine-tuning BERT-like pretrained language models (PLMs) on low resource target tasks. The authors hypothesize that the general-purpose knowledge obtained by the PLMs from pre-training might be irrelevant and redundant for a given target task. When fine-tuned onto a low resource target task, overfitting is likely to happen. To this end, a fine-tuning framework based on variational information bottleneck (VIB) is proposed to address these challenges. Specifically, the sentence representation will be mapped to a latent Gaussian variable  which compresses information in the sentence and also suppress irrelevant and redundant features, and a reconstructed version of the representation is used for task prediction. Empirical evaluations on sever datasets demonstrates the effectiveness of the method over previous research.""329\tThis paper discusses the use of large-scale language models pretrained on text data for transfer learning in natural language processing. The paper highlights the benefits of using pretrained models, including their ability to exhibit state-of-the-art results on a wide range of NLP benchmarks, but also notes the potential risks of overfitting, particularly if fine-tuning is performed on small amounts of target task data. The paper also discusses the issue of task-universal language representations and the need to remove or adjust the information that is not relevant to the target task. The paper concludes by highlighting the need for further research in this area to better understand and address the risks associated with using pretrained language models for transfer learning.\tThis work applies information bottleneck as a way to compress the pre-trained representation so that only meaningful features are employed for the target task. It is applied for the number of GLUE tasks especially focusing on low resource settings and show consistent gains over previously known strong baselines, e.g., Mixout and L2-of-difference. This work also demonstrates that the learned model has generalization capacity so that the tuned model works on out-of-domain data.""330\tThis paper discusses the use of large-scale language models pretrained on text data for transfer learning in natural language processing. The paper explains the advantages of using pretrained models, such as their ability to learn general-purpose representations, their ability to transfer knowledge across domains, and their potential for overfitting. The paper also discusses the potential issues associated with fine-tuning these models, such as the risk of overfitting due to the task-universal nature of the representation and the need to carefully control the amount of target task data used. Finally, the paper provides an overview of the current state of transfer learning in natural language processing and\u672a\u6765\u5c55\u671b.\tThe paper proposes a method to avoid overfitting while finetuning the large pretrained models for downstream tasks on small scale datasets. It has been shown that many SOTA models usually overfit w.r.t. spurious correlations in the data and as a result fail miserably when tested for generalization on the out of domain datasets. The proposed method tries to maximally filter out task-irrelevant information in the feature vectors by minimizing the mutual information between the original features and the bottleneck features while simultaneously optimizing for performance. Experiments on several datasets show improved performance on both in-domain and out-of-domain datasets.""331\tThis paper discusses the use of generative adversarial networks (GANs) to reconstruct 3D shapes from 2D images. The paper begins by\u4ecb\u7ecdGANs, which are capable of modeling 2D natural image manifold with highfidelity. It then presents an example of how a GAN could shift the object in its generated images in a 3D rotation manner. This motivated the question of whether it is possible to reconstruct the 3D shape of a single 2D image byExploiting the 3D-alike image manipulation effects produced by GANs. The paper then discusses some previous attempts to solve this problem, including work by Lunz et al., and suggests that there is still much to be done in the field of 3D shape reconstruction from 2D images. The paper ends by\u603b\u7ed3\u8ba8\u8bba\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411.\t1. This is the first work that attempts to reconstruct 3D shape from 2D image in an unsupervised way using GANs. The idea is neat: Use networks to predict four 3D parameters and use GAN to generate / synthesize the images corresponding to a set of parameters. Then these synthesized images can be used as pseudo ground truth to train the 3D parameter network.""332\tThis paper explores the possibility of using generative adversarial networks (GANs) to reconstruct 3D shapes from 2D images. The paper begins by discussing the basic concepts of GANs and the ability of these networks to model 2D natural image manifold. It then presents a case study where it is shown that GAN can shift the object in its generated images in a 3D rotation manner, which leads to the question of whether it is possible to reconstruct the 3D shape of a single 2D image byExploiting the 3D-alike image manipulation effects produced by GANs. The paper\u968f\u540e presents some previous attempts to address this problem, including Lunz et al.'s work, and discusses the limitations and potential of these approaches. Finally, the paper concludes by suggesting a new approach to this problem, which involves using a combination of GAN and deep learning techniques to learn a function that can generate realistic 3D shapes from 2D images.\tThis paper studies an interesting inverse-graphics problem. It proposed a novel method to learn 3D shape reconstruction using pre-trained 2D image generative adversarial networks. Given an image containing one single object of interest, it first predicts the graphics code (e.g., viewpoint, lighting, depth, and albedo) by minimizing the reconstruction error using a differentiable renderer. The next step is to render many pseudo samples by randomization in the viewpoint and lighting space, while keeping the predicted depth and albedo fixed. A pre-trained 2D image GAN is further used to project the pseudo samples to the learned data manifold through GAN-Inversion. Finally, these projected samples are added to the set for the next round optimization. Experimental evaluations have been conducted on several categories including face, car, building, and horse.""333\tThis paper explores the possibility of using generative adversarial networks (GANs) to reconstruct 3D shapes from 2D images. It starts by discussing the use of GANs to model 2D natural image manifold of diverse object categories, which is shown to be successful. Then, it moves on to discuss the use of GANs to shift the object in its generated images in a 3D rotation manner, which motivates the question of whether it is possible to reconstruct the 3D shape of a single 2D image. The paper also explores the potential of GANs to learn 3D shape from unconstrained RGB images, but this problem is still\u8f83\u5c11 explored. Some previous attempts have been made, but the paper highlights the need for further research in this area. The paper ends with a conclusion and future directions.\tThis paper proposes an iterative method that jointly estimates viewpoints, light directions, depth, and albedo from single images, by projecting intermediate renderings to the nautral image manifold. Intuitively, the method works by generating, with pre-trained GANs, multiple views of the same object under different lightings, and then inferring 3D shapes from those variants. The key idea is to use pre-trained 2D GANs to make such data generation photorealistic. The authors also demonstrate 3D edits, such as 3D rotation and relighting, that one can perform after running their model.""334\tThe paper discusses the problem of imbalanced classification when dealing with natural data, which is often long-tail distributed over semantic classes. Existing methods try to address this by increasing the emphasis on the tail data, such as by class re-Balancing/re-weighting or ensembling over different data groups. However, this can lead to increased tail accuracies but reduced head accuracies. The paper provides a dynamic view of the training data and discusses the model bias and variance as the training data fluctuates. They propose a new long-tailed classifier called ride, which reduces the model variance with multiple experts and reduces the model bias with a distribution-aware diversity loss. ride outperforms the state-of-the-art by 5% to 7% on a specific dataset.\tThe majority of feature extraction backbone is shared among different agents and the classifiers of experts are trained with both classification loss and proposed distribution-aware diversity loss. For the second stage, an expert assignment module is trained to re-weight the expert decisions. The whole paper is generally well-organized. However, there are some technical issues authors should further address:""335\tThe paper discusses the problem of imbalanced classification in natural data, where the majority of the data is not in the same class as the majority of the examples. The paper presents a new approach to handle this issue called ride, which uses diverse experts to reduce the model variance and improve the head-tail model bias gap. ride outperforms existing methods on a test set by 5% to 7%. The paper also discusses the dynamic view of the training data and the benefits of using multiple experts.\tThis paper proposes a Routing Diverse Experts (RIDE) framework to solve the long-tailed classification problem. It has 1) a shared low-level feature extractor and multiple expert classifiers, 2) a distribution-aware diversity loss to encourage experts learning different classification strategies, 3) an expert routing module that dynamically selects a subset of experts for each test instance to make a joint decision. This paper firstly increases the performances on all three splits (many-/med-/few-shot), while most of the existing methods have to sacrifice the head for tail improvements.""336\tThe paper presents a study on how existing imbalanced classification methods cope with long-tail data distribution over semantic classes. The study shows that existing methods always increase the model variance and have a large head-tail model bias gap. To address this issue, the paper proposes a new long-tail classifier called ride, which reduces the model variance with multiple experts and reduces the model bias with a distribution-aware diversity loss. ride outperforms the state-of-the-art on two datasets by 5% to 7%.\tThis paper proposes a method termed RoutIng Diverse Experts (RIDE) for reducing both the bias and the variance of a long-tailed classifier. Specifically, RIDE consists of three crucial components: 1) a shared architecture for multiple experts; 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. Experiments are conducted on three long-tailed benchmark datasets, i.e., CIFAR100-LT, ImageNet-LT, and iNaturalist. Satisfactory classification results of long-tailed visual recognition are observed.""337\tThis paper discusses the process of network compression, in which trained neural networks arepruned to save storage and computational resources. The paper covers the following aspects of network compression:\n\n1. Introduction\n2. Pruning\n3. Procedures\n4. Criteria\n5. Strategy\n6. Conclusion\n\nThe paper also discusses different types of pruning criteria that have been proposed, including Norm-based, activation-based, Importance-based, BN-based, and others. It also shows that layer-wise pruning is a more effective strategy for reducing the size of the network while maintaining its functionality. Finally, the paper concludes by discussing future directions and potential applications of network compression.\tThe paper discusses various baseline scoring mechanisms used for filter pruning that are norm-based and finds that none of the scoring mechanisms are particularly effective at pruning filters from CNNs. Moreover, all methods seem to perform very similar to each other. These conclusions are based on a theoretical (and experimental) analysis of the various scoring mechanisms under the assumption that trained filter weights follow a Gaussian-like distribution, which reveals that in this case the scoring mechanisms are insufficient to reliably discern the importance of filters since the resulting scores are very similar. The Gaussian-like assumption, which is termed \"Convolution Weight Distribution Assumption\" (CWDA), is validated through a large range of experiments on different architectures and data sets.""338\tThis paper discusses network compression, a technique for reducing the number of parameters in a neural network, while maintaining or increasing its accuracy. Network compression is often used in neural networks for various applications, such as data analytics, computer vision, and natural language processing.\n\nThe paper discusses various techniques for channel pruning, which refers to the pruning of the filters in the convolutional layers of a neural network. The critical factors for channel pruning include the use of a certain criterion to calculate the importance of the filters, and the strategy for pruning the network alternately between one-shot andIterative methods.\n\nThe paper also discusses various types of pruning criteria, including Norm-based, activation-based, Importance-based, and BN-based criteria. The paper provides an overview of the different criteria and their advantages and disadvantages.\n\nFinally, the paper concludes by discussing the practical implications of channel pruning in neural networks and\u6307\u51fa some future directions for research.\tThe goal of the paper is to bring into attention that many norm-based pruning criteria used for structured pruning are very similar, in that their ranking of the redundant filters is highly correlated. The key ingredient is the CWDA assumption that filters in a particular convolutional layer are iid and approximately follow a Gaussian distribution, which is shown based on extensive statistical hypothesis testing. Based on this assumption, they prove that these pruning criteria are roughly the same. ""339\tThis paper provides an overview of network pruning, a common technique in network compression, for CNNs. It discusses the importance of several factors for channel pruning, including the use of different criteria for calculating filters\u2019 Importance Score, as well as the layer-wise and One-shot methods for pruning CNN channels. The paper also highlights the challenges and future directions in this field, such as the development of more efficient and effective pruning algorithms.\tThis paper analyzes the current limitations of existing magnitude-based pruning methods. First, the paper focuses on the similarities between three methods and then focuses on the redundancy in large networks. The paper also analyzes the weight distribution for a well-trained network and propose CDWA as a way to prove this distribution.  My main concern with this paper is the contribution to the field. ""340\tThis paper presents a review of pre-trained models for programming language and their applications in code-related tasks. It emphasizes the importance of considering the inherent structure of code in pre-training models and the use of unsupervised pre-training techniques to improve performance on code-related tasks. The paper also discusses the challenges and future directions in this field.\tThis paper proposes GraphCodeBERT as a Transformer-based pretrained model for programming language that incorporates data flow information in the graph representation of variables in the code. The data flow graph encodes the structure of variables based on \u201cwhere-the-value-comes-from\u201d from the AST parse. The pretrained model is jointly trained on the code, the natural language comment of the code, and the data flow graph of the code. In addition to the Masked Language Modeling objective, two new pretraining objectives are proposed including predicting the edge of the data flow graph and predicting the alignment of variables between data flow graph and code. The graph-guided masked attention is used such that the attention can only occur if two variables have an edge in the data flow graph or there is an alignment between data flow graph and code. The experiments show that GraphCodeBERT can deliver improvements on Natural Language Code Search, Code Clone Detection, Code Translation, and Code Refinement.""341\tThis paper discusses the development of pre-trained models for programming language. Pre-trained models such as ELMo, GPT, and BERT have led to strong improvements on numerous NLP tasks, and have also been used in the development of pre-trained models for programming language. These models are first pre-trained on an unsupervised large text corpus and then fine-tuned on downstream tasks such as code search, code completion, code summarization, etc. Existing works have used source code as a sequence of tokens to pre-train models, but have ignored the inherent structure of code, which provides useful semantic information that can be useful for code-related tasks. The paper highlights the need for more research in this area to fully realize the potential of pre-trained models for programming language.\tThe authors present Graph Code BERT, the first language model that leverages data flow to learn code representation. They use three objective functions: Masked Language Modeling, Edge Prediction, and Node Alignment. They claim their structure-aware pre-training can help improving performance on code-related downstream tasks, including code search, clone detection, code translation, and code refinement.""342\tThis paper explores the use of pre-trained models for programming language. It explains the success of pre-trained models in NLP and the need for them in the field of programming. It presents pre-trained models such as ELMo, GPT, and BERT and discusses their applications in various code-related tasks such as code search, code completion, and code summarization. The paper also argues that previous works that only utilize source code for pre-training are missing the inherent structure of code, which provides useful semantic information that can be used in the training of pre-trained models. Finally, the paper concludes by highlighting the need for further research in this area to explore the full potential of pre-trained models for programming language.\tThis work address the pretraining over code and text. It proposes to leverage data flow as additional inputs, and add two structure aware pre-training tasks besides the masked token prediction task. The pretrained model is evaluated on four different tasks and outperforms the CodeBERT baselines as well as other pretrained models. Further analysis confirmed the benefits from the additional tasks and data flow input. ""343\tThe paper discusses the problem of reporting bias in scientific research, which can affect the accuracy of regression models trained using literature data. The paper\u6307\u51fa\uff0c\u7531\u4e8e\u53ea\u9009\u62e9\u4ee3\u8868\u6027\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u62a5\u9053\uff0c\u5bfc\u81f4 reported results \u5b58\u5728\u504f\u5dee\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u5206\u5e03\u4e0d\u540c ( Lin et al. , 2002; Galar et al. , 2011)\u3002\u5728 pharmaceutical  development \u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c24\u5176\u4e25\u91cd\u3002\u56e0\u6b64\uff0c quantitative structure-activity relationship ( QSAR ) \u548c drug-target interaction (DTI) \u4e5f\u53d7\u5230 reporting bias \u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u901a\u5e38\u76ee\u6807\u8303\u56f4\u660e\u786e\u7684\u5206\u5b50\u5c5e\u6027\u6570\u636e\u88ab\u660e\u786e\u5b9a\u4e49 ( Liu et al. , 2015; Chen & Zhang , 2013)\u3002\u5728 regression \u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u907f\u514d\u9009\u62e9\u504f\u5dee\u4e25\u91cd\u7684\u6837\u672c\u6765\u89e3\u51b3\uff0c\u4f8b\u5982\u901a\u8fc7\u589e\u52a0\u6570\u636e\u6837\u672c\u91cf (Lin et al. , 2010) \u6216\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f (Zhu et al. , 2015)\u3002\tThe paper presents a novel approach to improve the accuracy of regression models that are learned from a skew dataset. The proposed approach consists of two parts, namely, (i) adversarial network for forcing output distributions and (ii) regularization based on an adversarial autoencoder. Experiments suggest that the proposed approach increases the accuracy of the regression model for all the four datasets considered in the paper.""344\tThis paper explores the issue of reporting bias in scientific studies, specifically in the context of regression models. The paper\u5148\u4ecb\u7ecd\u5728\u79d1\u5b66\u9886\u57df\uff0c\u5b9e\u9a8c\u548c\u9519\u8bef\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u4f46\u901a\u5e38\u53ea\u9009\u62e9\u4ee3\u8868\u6027\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u62a5\u544a\u3002\u8fd9\u5bfc\u81f4\u62a5\u544a\u7ed3\u679c\u7684\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u5f53\u4ece\u6587\u732e\u4e2d\u6536\u96c6\u6570\u636e\u7528\u4e8e\u8bad\u7ec3Regression\u6a21\u578b\u65f6\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u53ef\u80fd\u4e0e\u771f\u5b9e\u5206\u5e03\u4e0d\u540c\uff0c\u56e0\u4e3a\u6a21\u578b\u662f\u6839\u636e biased data \u8bad\u7ec3\u7684\u3002 pharmaceutical development \u7ecf\u5e38\u88ab\u8fd9\u4e2a\u95ee\u9898\u6240\u5f71\u54cd\u3002 QSAR \u548c drug-target interaction(DTI) \u4e5f\u53d7\u5230\u62a5\u544a\u7ed3\u679c\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u901a\u5e38\u76ee\u6807\u8303\u56f4\u660e\u786e\u7684\u5206\u5b50\u5c5e\u6027\u503c\u8303\u56f4\u88ab\u5b9a\u4e49\u3002\u5f53Regression \u88ab\u4f7f\u7528\u65f6\uff0c\u8fd9\u4e2a\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u7a81\u51fa\u3002\tThis paper proposed to learn a regression model using \"skewed data\", which is defined as the subset of training samples with true target above certain threshold. The model consists of two components. First, the input x was mapped to its latent space through encoder R_enc. The latent representation was further mapped to the predicted output through regressor network R_post. The predictive distribution was forced to match the true target distribution p(y) through an adversarial network. Second, the latent space representations were also forced to match the true target distribution p(y). Experimental results on synthetic benchmark data showed the proposed approach performed better than naively applying regression model on the skewed data.""345\tThe paper\u63a2\u8ba8\u4e86\u5728\u79d1\u5b66\u9886\u57df\uff0c\u5305\u62ec\u836f\u7269\u53d1\u73b0\u6216\u6750\u6599\u8bbe\u8ba1\u4e2d\uff0c\u7531\u4e8e\u9009\u62e9\u62a5\u544a\u504f\u5dee\uff0c\u53ea\u9009\u62e9\u4ee3\u8868\u6027\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u7ed3\u679c\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u5dee\u5f02\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u6587\u732e\u6570\u636e\u8bad\u7ec3Regression\u6a21\u578b\u65f6\uff0c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u53ef\u80fd\u4e0e\u771f\u5b9e\u5206\u5e03\u4e0d\u540c\uff0c\u56e0\u4e3a\u6a21\u578b\u4f7f\u7528\u504f\u5dee\u6570\u636e\u8bad\u7ec3\u3002\u8fd9\u5c24\u5176\u5728 pharmaceutical \u5f00\u53d1\u4e2d\u53d7\u5230\u5f71\u54cd\u3002Quantitative structure-activity relationship(QSAR\uff0c\u5305\u62ec\u836f\u7269-\u76ee\u6807\u76f8\u4e92\u4f5c\u7528)\u4e5f\u53d7\u5230\u62a5\u544a\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u901a\u5e38\u76ee\u6807\u8303\u56f4\u5185\u5206\u5b50\u5c5e\u6027\u7684\u7279\u5b9a\u8303\u56f4\u662f\u6e05\u6670\u7684(Liu,2015;Chen,Zhang,2013)\u3002\u56e0\u6b64\uff0c regression \u662f\u5904\u7406\u504f\u5dee\u6570\u636e\u5728 QSAR \u4e2d\u7684\u91cd\u8981\u65b9\u6cd5\u3002\tThis paper proposed a semi-supervised learning approach to improve the regression model trained on output-skewed data. The key assumption is that, though the training outputs can be skewed, it is easy to estimate the true distribution of the output. The proposed model that combines an AAE that generates the output distribution, and an adversarial model that enforces the distribution of the predicted output to resemble the true distribution of the output. On several real datasets, the ablation study shows the proposed model can improve the regression accuracy.""346\tThis paper discusses the ability of human intelligence to generalize to novel combinations from known components, also known as compositional generalization. It explains how this ability helps humans to recognize the world efficiently and be imaginative, and how it is beneficial to design machine learning algorithms with compositional generalization skills. The paper also discusses the type of out-of-distribution generalization that is enabled by recombining seen components of an unseen combination during inference. Finally, the paper presents some examples of how human and machine intelligence can use this ability in different domains.\tThis paper proposes an architecture that addresses transferability of compositionality. The proposed architecture consists of three components: a network that transforms the input X into a series of hidden representations {H_1, H_2, ... H_K}, a network that reconstructs the input X from this series of hidden representations, and a prediction network that generates a prediction from the hidden representations. The authors propose several datasets meant to address transferability of compositional generalisation, and show that their architecture significantly improves standard DNN architectures as well as humans on these datasets.""347\tThis paper discusses the ability of human intelligence to compositional generalization, which is the ability to understand and produce novel combinations of known components from which new ones can be generated. The paper also discusses the benefits of designing machine learning algorithms with compositional generalization skills, as this ability can enable them to generalize better to new data. The paper\u6700\u540e\u8ba8\u8bba\u4e86\u5f53\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u5b9e\u73b0 compositional generalization \u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\u3002\tThe paper introduces a \u201ctransferability of compositionality\u201d problem and proposes an approach to alleviate it. The said problem may arise when one trains neural models to produce \u201ccompositional\u201d representations of the input. In the paper \u201ccompositional representations\u201d consist of multiple vectors which are supposed to correspond to semantically meaningful aspects of the input, for example different objects in the case of images or different parts of compound words in the case of linguistic inputs. The transferability problem arises when there is a difference between training and test distributions, namely when certain combinations of objects have different probabilities in training & testing. The proposed solution at inference time is to project object representations to the manifold of individual object representations. The manifold is estimated by saving representations of individual object representations from the training time. ""348\tThis paper discusses the concept of compositional generalization, which is a type of out-of-distribution generalization that allows machine learning algorithms to learn new combinations of components from data that is different from the training and test distributions. The paper\u8ba8\u8bba\u4e86Compositional generalization\u4e0eOut-of- Distribution generalization\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u989d\u5916\u7684\u7279\u5f81\u548c\u8c03\u6574\u6a21\u578b\u53c2\u6570\u6765\u63d0\u9ad8\u673a\u5668 learning \u7684 compositionality \u3002\u5b83\u8fd8\u8ba8\u8bba\u4e86\u5982\u4f55\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u4e2d\u5b9e\u73b0 compositional generalization\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u5b9e\u73b0\u65b9\u6cd5\u3002\tThis paper studies \"compositionality\" and in particular the way in which it \"transfers\" on test data. They run simple baselines on three experiments (overlapped MNIST, colored MNIST and concatenated month names) and find that the baselines do not learn compositional representation. They proposed the use of an *auxiliary reconstruction network and a regularized optimization* which improves on these baselines. ""349\tThis paper focuses on the security of reinforcement learning (RL) techniques against adversarial attacks. It discusses the types of attacks that can occur during training, as well as the vulnerabilities in RL systems that can be exploited by an attacker. The paper also explores the potential solutions to this problem, such as improving the quality of the training data and developing new attack methods that cannot be executed by the attacker.\tThe paper studies poisoning attacks on RL agents, in which the attacker influences the agent's learning process by changing the feedback obtained from the environment. The focus is put on attacking policy-based deep RL agents, without necessarily having access to the underlying MDP model of the environment. The paper proposes a new poisoning algorithm, called Vulnerability-Aware Adversarial Critic Poison, and experimentally demonstrates its effectiveness on 5 different RL environments.   ""350\tThis paper discusses the security of reinforcement learning (RL) techniques against adversarial attacks. The paper focuses on poisoning attacks, which occur during the training of RL agents and can influence the learned policy. The paper discusses the challenges of training RL techniques and how poisoning attacks can be used to manipulate the learned policy. The paper also proposes some potential solutions to mitigate the effects of poisoning attacks.\tThe paper studies poisoning attacks against online reinforcement learning agents. The attacker has the power of manipulating the training data, i.e., state-action-reward trajectories, in order to achieve some attack goal. The attack can be completely black-box, meaning that the proposed method allows an attack setting where the attacker has no knowledge of the RL algorithm used by the victim agent or the environment. In this scenario, the authors proposed that the attacker can imitate the learning procedure of the victim, and then based on the imitated policy; the attacker designs how to poison the training data. The attack is formulated as a bi-level optimization, where the lower level involves the imitated learning procedure. Due to the intractability of sequential optimization, the original formulation is simplified so that only the attack only solves the attack on the current training data. This procedure is repeated in every episode to achieve sequential attacks. Experiments on a variety of tasks demonstrate the superiority of the proposed attack.""351\tThis paper focuses on understanding the security of reinforcement learning (RL) techniques against adversarial attacks, specifically poisoning attacks. RL techniques, especially deep RL, have been successfully applied in various fields, but the security of these techniques against adversarial attacks is not well understood. The paper\u8ba8\u8bba\u4e86 poisoning attacks , which occur during the training and influence the learned policy. Since training RL is known to be very sample-consuming, one might have to constantly interact with the environment to collect data, which opens up a lot of opportunities for an attacker to poison the training samples collected. The paper\u63a2\u8ba8\u4e86 poisoning mechanisms \uff0c\u4ee5\u53ca\u5728 RL \u4e2d\u5982\u4f55\u68c0\u6d4b\u548c\u9632\u6b62 poisoning \u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7b56\u7565\u3002\u6700\u540e\uff0c\u8be5 paper \u63d0\u51fa\u4e86\u4e00\u4e9b\u53ef\u80fd\u6709\u7528\u7684\u5b9e\u8df5\u5efa\u8bae\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u5e94\u5bf9 adversarial attacks \u5bf9 RL \u6280\u672f\u7684\u5f71\u54cd\u3002\tThis paper proposes a poisoning algorithm named Vulnerability-Aware Adversarial Critic Poison (VA2C-P) to attack policy-based deep reinforcement learning agents. The poisoning attack is formulated as a sequential bilevel optimisation problem (Problem Q), where the attacker either minimises the expected total rewards of the learner (non-targeted poisoning), or forces the learner to learn a target policy (targeted poisoning). To solve Problem Q, VA2C-P mainly makes two decision: (1) when to attack: a new metric named stability radius is proposed to decide the attack timing, (2) how to attack: a mechanism of adversarial critic is designed to solve a relaxed version of Problem Q by only considering the loss of the immediate next iteration.""352\tThis paper discusses a technique called dynamic training\u4f11\u606f (DTR) that allows deep learning models to train with\u8d85\u8fc7 on-device memory budget, without modifying the model's design. DTR uses tensor\u4f11\u606f (tensor rest) to free some tensors from memory and recomputing them on demand, which enables the model to train with more memory than its device has available. In simulations, DTR closely matches the performance of optimal static checkpointing, demonstrating the effectiveness of this technique in memory- limited devices. The paper also incorporate a DTR prototype into PyTorch, allowing for easy integration into existing deep learning frameworks.\tThe paper presents an online algorithm for dynamic tensor rematerialization.  Theoretically, it shows the same asymptotic order on the memory budget and tensor operations as of the optimal static approach.  By simulation, it shows the performance matches optimal static checkpointing in a few models.  A PyTorch prototype is implemented, which shows benefits of reducing memory footprint and increased batch size comparing with basic PyTorch models without checkpointing.""353\tThis paper discusses a technique called dynamic tensor relaxation (DTR) for training deep learning models with a large memory budget, while using only O(N) tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments, indicating that it can be an effective solution for training models with large memory requirements. The paper also incorporates a prototype of DTR into PyTorch and demonstrates its effectiveness in real-world applications.\tContributions: a) analyzes multiple heuristics for which tensors to evict where compute overhead of rematerialization is minimal overall, b) suggested approach is just-in-time, and thus does not require any static analysis of the network. That is, unlike prior work in this area, it covers any network type with no prior knowledge, c) offers a good formal analysis of proposed heuristic in terms of its components: staleness, memory capacity and recursive replay cost - their formalization covers previously published heuristics as well.  Experimental framework is sound. And some encouraging results are shown delivering memory capacity saving of 30% to 90% with training slowdown of 2x or less.""354\tThis paper describes a technique called dynamic training\u4f11\u606f (DTR) that can be used to train deep learning models with large memory requirements while still achieving optimal performance. DTR allows for the creation of a training batch with only O ( N ) tensor operations, while still providing a significant memory budget of\u221a N. The paper demonstrates that DTR closely matches the performance of optimal static checkpointing in simulated experiments, indicating that it is capable of providing high-quality training results even on devices with limited memory. Additionally, it is possible to incorporate a DTR prototype into PyTorch by interposing Tensor allocation and operator calls and collecting lightweight metadata on tensors. This allows for easy and efficient use of the DTR technique in real-world deep learning applications.\tThis paper proposed a simple yet effective greedy algorithm with a new heuristics on checkpointing deep learning models so that people could train large model with restricted GPU memory budgets. The proposed method operates in an online setting and do not need static analysis of computation graph, thus could be used for both static and dynamic models. In a restricted model setting of linear forward network and equal space and time cost for each node, the author proves the proposed method could reach the same bound on tensor operation and memory budget with previous static checkpointing methods. The author also establish a theorem on tensor operation numbers between the proposed dynamical method and an optimal static checkpointing algorithm. In experiment, the author compared the proposed method with static techniques including the optimal Checkmate tool of Jain et al. (2020), showing the proposed method gives competitive performance without static model analysis in prior. The author also compared the proposed heuristics with prior arts on several static and dynamic models. Finally, the author described a prototype of PyTorch implementation of the proposed method. ""355\tThis paper discusses the issue of faithfulness in neural sequence models, specifically how these models generate output that may not be faithful to the input text. The paper discusses the limitations of these models, including their lack of global logical consistency, degeneration to dull and repetitive outputs, and ability to hallucinate content that is not entailed by the input. The paper\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u81ea\u52a8\u8bc6\u522b\u548c\u91cf\u5316\u4e0d faithful content in the output\uff0c\u4ece\u800c\u51cf\u8f7b\u8fd9\u79cd\u9650\u5236\u7684\u5f71\u54cd\uff0c\u5e76\u5e2e\u52a9\u5b89\u5168\u5730\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c conditional sequence generation tasks .\tThe paper addresses the problem of \"hallucinated\" content in conditional neural generation for two specific tasks: machine translation and summarization. It proposes a new task for faithfulness assessment, which classifies each token as either hallucinated or not. The classifier uses a pre-trained LM (either XLM-R or ROBERTa) and is fine-tuned on synthetic classification data created using both 'noisified' real data and a pretrained LM (BART). Experiments on either summarization and MT system outputs labeled for hallucinations show relatively encouraging classification results (e.g., F1 of 0.46 to 0.66 for MT, and 0.56 to 0.66 for summarization).""356\tThis paper focuses on the faithfulness of machine outputs in conditional sequence generation tasks, which is a common challenge in many machine learning applications. Neural sequence models have achieved impressive results in generating fluent and human-like sentences, but recent work has shown that they can lack global logical consistency, generate dull and repetitive outputs, and hallucinate content that is not entailed by the input. This risk of generating unfaithful content impedes the safe and efficient use of machine learning techniques in many applications. The paper proposes a method for automatically identifying and quantify the faithfulness of machine outputs in conditional sequence generation tasks, and presents some experimental results to support the effectiveness of the proposed approach.\tThis paper proposes hallucination detection at the token level, which predicts if each token in the generation output is hallucinated or faithful to the source input. In contrast, previous studies usually work on the sentence level. To create synthetic training data, a denoising pre-trained LM is first used to generate (potentially) unfaithful counterparts T\u2019 of the references T. Then, token-level labels are obtained by comparing T and T\u2019 via edit distance. Finally, a standard classification model is trained on the token-level labels by concatenating the source S, true and unfaithful targets T (T\u2019).""357\tThis paper focuses on the faithfulness of machine outputs in conditional sequence generation tasks, which is a concern in many applications of neural sequence models, such as data-to-text generation, machine translation, and text summarization. The paper aims to automatically identify and quantify content in the output that is not faithful to the input text, which can prevent the safe development and application of these models. The paper\u63a2\u8ba8\u4e86 faithfulness\u7684\u6982\u5ff5\uff0c\u5206\u6790\u4e86 neural sequence models \u7684 faithfulness \u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3 faithfulness \u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6a21\u578b\u7ed3\u6784\u7684\u6539\u8fdb\u3001\u8f93\u5165\u6570\u636e\u7684\u9884\u5904\u7406\u548c\u81ea\u52a8\u63a8\u65ad\u7b49\u3002\u6700\u540e\uff0c\u8be5 paper \u8fd8\u63d0\u51fa\u4e86\u4e00\u4e9b\u53ef\u4f9b\u5b9e\u8df5\u53c2\u8003\u7684\u5efa\u8bae\u3002\tThis paper presents a method to detect hallucinated tokens in generations from neural machine translation and summarization. Given a source input S and its output G generated by a sequence generation model, this paper formalizes the task of detecting hallucinated tokens as a labeling problem on the output G. In order to train the labeler, the method synthetically generates supervision data by using a BART model. The BART model receives a text with noises ([MASK] tokens) and tries to predict [MASK] tokens. In this way, the method obtains a pseudo hallucinated text T' from a text T, and assigns hallucination labels by estimating edit operations between T and T'. The labeler is trained by fine-tuning pre-trained cross-lingual (for MT) and mono-lingual (for summarization) language models. In training, the labeler receives a source text S, true target text T, and pseudo hallucinated text T' separated by [SEP] tokens and tries to reproduce the hallucination labels on T'. Receiving a source text S and its output G, the labeler predicts hallucination labels on G during the inference time.""358\tThis paper explores the possibility of designing class-aware generators for Conditional GANs (cGANs), which use class information to guide the training of the discriminator and generator. The paper proposes a neural architecture search algorithm, calledNAS, that can automatically design class-awaregenerators. It uses reinforcement learning to learn a schedule that\u6700\u4f18\u5730\u5c06\u6bcf\u4e2a\u7c7b\u522b\u7684\u4fe1\u606f\u4f20\u9012\u5230\u76f8\u5e94\u7684\u751f\u6210\u5668\u7f51\u7edc\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8cGAN\u7684 generation effect. The results of our study show that class-awaregenerators can significantly improve the generation performance of cGANs.\tThis paper proposes an interesting method that adopts NAS to search multiple class-aware generator architectures for cGAN instead of class-agnostic type. A search space containing both normal and class-modulated convolutions are introduced to simplify the process of re-training. Besides, this paper design a mixed-architecture optimization to specifically address the computational burden issue under the setting of a multi-net search. The search results also give some insights about constructing cGAN models.""359\tThis paper studies the possibility of designing class-awareGenerators for cGANs, which use class information to guide the training of the discriminator and generator. The paper proposes a neural architecture search algorithm, calledNAS, on top of reinforcement learning to automatically design these Generators. By usingNAS, we achieve better generation effects and also investigate the effects of different generator network architectures on the performance of the cGAN.\tThis paper proposes a framework NAS-caGAN that adopts RL-based NAS to search the optimal class-aware generator architecture by directly optimizing the Inception Score (IS) using the  REINFORCE algorithm, and leverages the mixed-architecture optimization to mitigate the training data sparsity of each category. The authors design a Class-Modulated Convolution to allow for the weight-sharing among different searched architectures. The proposed NAS-caGAN outperforms the model that employs searched class-agnostic architecture on CIFAR 10 and achieves better results compared with cproj (Miyato & Koyama, 2018) on CIFAR 100. ""360\tThis paper discusses the possibility of designing class-aware Generators for Conditional GANs (cGANs), which use class information to guide the training of the discriminator and generator. The paper proposes a neural architecture search algorithm calledNAS to automatically design these Generators. By usingNAS, the generator network architecture can be designed for each class with the aim of achieving a better generation effect. Additionally, the paper discusses the challenges and limitations of designing class-aware Generators and provides future directions for this field.\tThis paper proposes an interesting idea that adopts NAS to find a distinct architecture for each class based on cGAN framework. Within the framework, the paper also proposes an operator, Class-Modulated convolution (CMconv), to allow the training data to be shared among different architectures, so as to balance the training data across classes. The proposed method leverages a Markov Decision Process (MDP) in the search algorithm, and learns the sampling policy for NAS. Comprehensive experiments demonstrate the class-aware NAS can outperform class-agnostic NAS.""361\tThis paper discusses the importance of estimating the causal effect of an intervention in order to make individual decision-making in various domains such as marketing, economics, and epidemiology. It also explains the two main methods of estimating the causal effect: randomized control trials (RCTs) and observational studies. RCT are widely recognized as the gold standard for estimating causal effects, but they are often infeasible, while observational studies use observed data to infer causal effects. The paper also discusses the potential limitations of both methods and the importance of considering multiple sources of data when inferring the causal effect.\tThe present paper introduces a new approach, deep orthogonal networks for unconfounded treatments (DONUT), that allows to estimate (average) treatment effects exploiting an orthogonality property implied by the classical unconfoundedness assumption. The authors propose a regularization framework based on the orthogonality constraint and prove that a resulting estimator is doubly robust, asymptotically normal and with efficient variance. They supply multiple simulations to demonstrate their theoretical claims and to show state-of-the-art performance of their estimator.""362\tThis paper provides an overview of the importance of estimating the causal effect of an intervention in various domains, such as marketing, economics, and epidemiology. It discusses the two main methods of estimating this effect: randomized control trials (RCTs) and observational studies. RCT's are widely recognized as the gold standard for estimating causal effects, but are often infeasible, while observational studies use observed data to infer causal effects. This paper also highlights the potential biases that can arise in both types of studies and provides some guidelines for researchers and decision-makers on how to overcome these biases.\tThe authors propose a regularized framework for estimating the average treatment effect. They assume unconfoudedness and show that it implies a specific orthogonality constraint. The main idea is to use this orthogonality constraint during estimation of the model parameters as a regularizer. On the theoretical side, the authors provide sufficient conditions under which the regularization yields an asymptotically normal estimator for the average causal effect. Based on the regularization framework, an estimator for average causal effect via feedforward neural nets is developed.""363\tThis paper provides an overview of the importance of estimating the causal effect of an intervention for individual decision making in various domains such as marketing, economics, and epidemiology. The paper also discusses the two main methods for estimating the causal effect: randomized control trials (RCTs) and observational studies. RCT are widely recognized as the gold standard for estimating causal effects, but are often infeasible, whereas observational studies use observed data to infer causal effects. The paper also highlights the potential biases and limitations of both methods and suggests that more research is needed to understand the best approach for estimating the causal effect.\tThis paper proposes a novel regularization term for designing loss functions to estimate outcome and propensity score models, where the end goal is to estimate ATE.  The regularizer is derived from the assumption of conditional independence of potential outcomes and treatment given covariates (i.e. the no hidden confounding assumption).  The authors observe that this assumption implies that residuals of potential outcomes and treatments are orthogonal.  The authors derive a loss function which yields this orthogonality at the optimum.""364\tThis paper discusses the role of affine parameters in deep learning, specifically in feature normalization techniques like BatchNorm. Feature normalization is a common technique used in deep learning for computer vision, and it involves multiplying each feature by a learned coefficient\u03b3 and adding a learned bias\u03b2before being used in the network. The paper focuses on the\u03b3 and\u03b2 parameters in BatchNorm, and\u63a2\u8ba8\u4e86\u4ed6\u4eec\u7684\u4f5c\u7528\u548c expressive power\u3002BatchNorm is nearly ubiquitous in deep convolutional neural networks for computer vision, meaning that many modern neural networks use these affine parameters to transform features\u3002 The paper\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53c2\u6570\u5728BatchNorm\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u4f5c\u7528\u548c\u8868\u8fbe\u80fd\u529b\u3002 This paper provides an in-depth analysis of the role and expressive power of the\u03b3 and\u03b2 parameters in BatchNorm, which are essential for feature normalization in deep learning\u3002\tThis paper studies the effect of training BN parameters on training deep neural networks. The conclusion is striking: learning only BN parameters is enough when increasing the depth of the network. Authors have done extensive experiments to understand the effect of increasing the depth and width of the network. To stress the important role of BN parameters, the same number of parameters are chosen randomly and trained. Yet, it is observed BN parameters can obtain far better accuracy. Furthermore, an interesting observation is conducted on the distribution of BN parameters: when training only these parameters, a sparsity pattern is observed on the optimal parameters. While learning all parameters does not reach such a sparse pattern for BN parameters. The sparsity pattern indicates that an efficient network only needs to have a particular ground-truth connection between different units and the choice of weights is not important. This shows that random features imposed by neurons can create a very interesting function class when they are connected in a proper way. ""365\tThis paper explores the role of affine transformations in deep learning and their use in feature normalization techniques, such as BatchNorm. The paper focuses on the \u03b3 and \u03b2 parameters of BatchNorm, which are commonly used to multiply and add features in deep learning. The paper discusses the practical importance of these parameters, as they are crucial for normalizing data and making it easier for the network to process. The paper also\u63a2\u8ba8s the expressive power of the \u03b3 and \u03b2 parameters, as they can control the degree of normalization and affect the performance of the network. Finally, the paper provides an analysis of the best practices for using BatchNorm in deep learning and discusses some potential limitations and areas for improvement.\tThe authors explore the representational power of BatchNorm's affine parameters (scale $\\gamma$ and bias $\\beta$). For that, they freeze the randomly-initialized parameters of different versions of ResNet and VGG, and only train the affine transformations. They also compare the expressiveness of BatchNorm coefficients with respect to the same amount of neural net parameters.  The main conclusions of this work are that BatchNorm coefficients have a greater discriminative power than the rest of network parameters. Moreover, in random networks, $\\gamma$ seems to disable non-useful features, disabling more than 25% of the channels, and in non-random networks it may prevent overshooting. They also show how these coefficients interact with networks of different depth and width, concluding that deeper random networks achieve better performance than wider random networks with the same amount of BatchNorm parameters.""366\tThis paper focuses on the role and expressive power of the \u03b3 and \u03b2 parameters in BatchNorm, a feature normalization technique commonly used in deep convolutional neural networks (CNNs) for computer vision. BatchNorm helps to improve the efficiency and performance of CNNs by normalization the values of each feature before being used in the network. The \u03b3 and \u03b2 parameters in BatchNorm are learned parameters that transform the features in a way that helps to improve the normalization process. By understanding the role and expressive power of these parameters, we can better understand how to use them in CNNs and how they can be used to improve the performance of the network. This paper provides an analysis of the performance of CNNs with and without BatchNorm, and discusses how the \u03b3 and \u03b2 parameters can be used to improve the performance of the network.\tThis paper studies the expressive power of batchnorm parameters by training only these parameters while fixing other randomly initialized parameters. With experiments on different datasets and models, the authors show that batchnorm parameters are consistently more expressive than other parameters. The authors also try to explain such phenomenon by examining the values of parameters and activations, showing that training BN only can lead to sparse values. ""367\tThis paper discusses the limitations of deep neural networks for machine learning applications, specifically how their accuracy is affected by dataset shift, a condition that occurs when the training data differs from the testing data. The paper highlights the importance of including adaptation during testing to ensure that the model can generalize to new data. The paper also discusses the potential benefits and limitations of using deep neural networks for machine learning, and provides suggestions for improving their performance.\tThis paper proposes a method to adapt a pre-trained model to a target domain, without the need to access samples from the source domain - on which the model was originally trained. The idea is to adapt layer normalization parameters at test time, by learning affine transformations. This is applied in tandem with the re-collection of the domain statistics.""368\tThis paper discusses the issue of dataset shift in deep neural networks, the impact it has on the accuracy of the model, and how to overcome it. It also highlights the need for adaptation during testing, which can be done fully in the test time. The paper provides an overview of the existing literature on this topic and discusses the possible solutions.\tThis paper tackles an interesting problem setting \u2014 fully test-time adaptation with only target data. The proposed method is to minimize the test-time entropy, and the loss is used to update the feature modulation layer only. The proposed method compares favorably with the state of the arts, on the ImageNet-C benchmark and unsupervised domain adaptation tasks.""369\tThis paper discusses the limitations of deep learning models, specifically their ability to generalize to new data and their sensitivity to dataset shift. It also discusses the need for adaptation during testing, which can be done fully during the test time. The paper provides an overview of the latest research in this area and concludes by highlighting the challenges and opportunities in developing more effective deep learning models.\tPresents Test-time Entropy (TENT) minimization, an algorithm for adapting deep models at test time to distributionally shifted data, without requiring access to source training data. At test time, the algorithm updates batch-norm parameters (that control channel-wise normalization and transformation) to minimize predictive entropy over target data. This simple approach is found to lead to state of the art performance on various corruption benchmarks for image classification, and competitive performance on simple DIGITS recognition-based domain adaptation shifts. ""370\tThis paper discusses the issue of poorly calibration in modern machine learning models, specifically in neural networks. It\u6307\u51fa\u4e86softmax activation\u548c typical neural networks\u5728\u8f93\u51fa\u6982\u7387\u4e0a\u503e\u5411\u4e8e\u7ed9\u5f02\u5e38\u6837\u672c\u9ad8\u6982\u7387\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u7ed9\u51fa\u7ed9\u51fa uncertainty estimates\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u6837\u7528\u6237\u5c31\u53ef\u4ee5\u8bc4\u4f30\u4ed6\u4eec\u5bf9\u4e00\u4e2a\u9884\u6d4b\u7684\u4fe1\u4efb\u7a0b\u5ea6\u3002\u6b64\u5916\uff0c\u5728\u591a\u4e2a\u4f7f\u7528\u573a\u666f\u4e0b\uff0c well-calibrated output probabilities are crucial\uff0c\u4f8b\u5982\u5728\u76d1\u63a7\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\uff0c\u5f53\u51fa\u73b0\u4f4e confidence\u7684\u9884\u6d4b\u65f6\uff0c hospital staff\u5e94\u8be5alerting\u3002\u56e0\u6b64 Bayesian neural networks(BNNs)\u662f\u4e00\u79cd\u4f7f\u7528\u6982\u7387\u5206\u5e03\u6765\u8bbe\u7f6e\u6a21\u578b\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002BNNs\u5728\u8f93\u51fa\u6982\u7387\u4e0a\u4f7f\u7528\u6982\u7387\u5206\u5e03\u6765\u7ea6\u675f\u6a21\u578b\uff0c\u4ece\u800c\u4f7f\u5f97\u8f93\u51fa\u6982\u7387\u66f4\u51c6\u786e\u5730\u53cd\u6620\u9884\u6d4b\u7684\u6b63\u786e\u6027 likelihood\u3002\tThis paper presents an approach to uncertainty modeling in recurrent neural networks through a discrete hidden state. The training of this discrete model is done using a reparameterizable approximation (in particular, using the Gumbel-Softmax trick). The authors show the utility of this method on a variety of problems, including showing effective out of distribution detection and improved calibration in classification tasks.""371\tThis paper discusses the issue of poorly calibration in modern machine learning models, specifically in neural networks. The paper highlights the importance of providing uncertainty estimates for predictions, as this allows users to assess the trustworthiness of a prediction. The paper also discusses the benefits of well-calibrated output probabilities, particularly in use cases where data is noisy or uncertain. The paper proposes Bayesian neural networks as a solution to the calibration problem, as they place a prior distribution on the model's output probabilities. This allows for more accurate and uncertainty-friendly predictions.\tThis work proposes a novel method to estimate uncertainties in recurrent neural networks. The proposed model explicitly computes a probability distribution over a set of discrete hidden states given the current hidden state in an RNN. Leveraging the Gumbel softmax trick, the proposed method performs MC gradient estimation. A temperature parameter is also learned to control the concentration of state transition distribution. To estimate uncertainty of a given input, the proposed model is run multiple times to draw samples for estimating the mean and variance. Experiments are conducted in a variety of sequential prediction problems, including a reinforcement learning task, demonstrating the effectiveness of the proposed uncertainty estimation method.""372\tThis paper discusses the issue of overfitting in modern machine learning models, particularly in neural networks. It also discusses the importance of providing uncertainty estimates for the predictions made by these models, as well as the use of Bayesian neural networks (BNNs) to improve the calibration of the models. The paper suggests that BNNs can be used to place a prior distribution on the model\u2019s output probabilities, which can help to prevent overfitting and provide more accurate predictions. Additionally, BNNs can provide users with more interpretable information about the model\u2019s predictions, which can help to improve the trustworthiness of the models in specific use cases.\tThis paper proposes a method to quantify the uncertainty for RNN. Different from  the traditional Bayesian RNN, the proposed method is more efficient. At each  time, based on the current hidden state and memory, it generates a probability  distribution over the state transition paths on the transition probability by  using the Gumbel softmax function. The next state is computed based on the weighted average of the sampled states and its uncertainty can be  qualified by the sample variance. The hyper-parameter tau of the Gumbel function  is learnt from data to better capture the inherent uncertainty in the data.""373\tThis paper discusses the challenges and opportunities of training high-capacity deep neural networks (DNs) with private data, specifically medical images and financial transaction data. It argues that DNs often overfit the private training data and are therefore exposed to data privacy leakage. This leakage can lead to issues such as the ability of\u653b\u51fb\u8005 to determine if an instance is in the training set, model extraction, and inversion of the target model. The paper also discusses some potential solutions to these issues, such as using techniques to mitigate overfitting, providing additional training data, and improving the quality of the private data.\tThe paper focuses on the topic of differentially private deep learning. Specifically, based on the deep residual learning, they first see it as an ODE. Then, to reduce the reversibility of the ODE, they modify the model as an SDE. By discretizing the SDE, they get a perturbed version of residual learning and use this to design DP-algorithms. The first strategy is directly followed the SDE while the second strategy is with an addition multiplicative noise of the additive noise. Finally, they show that their methods to defend membership inference attack both theoretically and practically. ""374\tThis paper discusses the issue of data privacy leakage caused by overfitting high-capacity deep neural networks (DNs) trained with private data. DNs are often used to process private data, such as medical images and financial transaction data, but they can overfit the data and memorize it, which makes them exposed to data privacy leakage. This issue has been discussed in the context of medical privacy and financial data privacy. The paper also discusses three types of attacks that can be caused by DNs: membership inference attacks, model extraction attacks, and model inversion attacks. These attacks can be used to determine if an instance is in the training set, learn a surrogate model that matches the target model, or infer certain features of a given input from the output of the target model. The paper provides an overview of the issues and suggests possible solutions to these issues.\tThe paper presents a method for training ResNets with differential privacy. Rather than the usual methods based on noisy gradient descent, the authors propose adding noise at each layer of the network during both training and testing. The authors prove differential privacy guarantees for two strategies of this type (one with additive and one with multiplicative noise). They also show some evidence that the noise can help generalization, by showing that the Rademacher complexity of a continuous linearized version of the model is lower when noise is added.""375\tThis paper discusses the privacy issues associated with high-capacity deep neural networks (DNs) trained with private data. DNs are often overfit and can memorize the private training data, which makes them vulnerable to data privacy leakage. This issue is further\u52a0\u5267 by the existence of the membership inference attack, the model extraction attack, and the model inversion attack. The paper also highlights the importance of designing robust privacy-\u4fdd\u62a4\u673a\u5236 for DNs.\tThis paper studies an important problem and proposes the novel residual perturbation to protect privacy while maintaining the ResNet models\u2019 utility.  Two SDE models are provided to inject noises with abundant theoretical proof are provided. Experimental results demonstrate the performance of privacy protection and classification accuracy on benchmark datasets. My major concern is about the utility enhancement and the DP guarantee (see cons below). Hope the authors can address my concern in the rebuttal period.""376\tThis paper discusses the recent development of pretrained language models, particularly the use of large-scale transformers and their computational and energy efficiency. It also highlights the concerns over excessive computational overhead during inference and proposes a solution, PoWER-BERT, which mitigates these issues. The paper concludes by discussing the potential applications of these models in real-world NLP tasks.\tThe paper proposed the Length-Adaptive Transformer. The model can be trained once and directly applied to different inference scenarios. To achieve this goal, the author proposed the LengthDrop method, which randomly samples the length at each layer. In addition, the author used the sandwich rule to train the model. At each step, the sandwich rule will train the largest model, the smallest model, and another bunch of randomly sampled models. In the inference phase, the paper proposed to search for the best length configuration that balances the accuracy and latency tradeoff via evolutionary search. Moreover, to generalize the model to token annotation tasks, the author proposed the Drop-and-Restore process, in which the tokens that have been dropped are used again in the final layer. Experiments show that Length-Adaptive Transformer is able to outperfom the baseline models when evaluated at the same latency level.""377\tThis paper discusses the recent development of pretrained language models, particularly large-scale transformers, and their applications in various NLP tasks. The paper also discusses the computational and energy overhead of these models, as well as recent efforts to address these concerns. The paper\u6700\u540e provides an overview of the future directions of pretrained language models and their potential applications.\tThis work introduces a method, called LengthDrop, to train a Length-Adaptive Transformer that supports adaptive model architecture based on different latency constraints. In order to make the model robust to variable input lengths, the method stochastically reduces the length of a sequence at each layer during training. Once the model is trained, the method uses an evolutionary search to find subnetworks that maximize model accuracy under a latency budget. ""378\tThis paper discusses the recent development of pretrained language models and their impact on natural language processing (NLP) tasks. It focuses on the computational overhead of large-scale transformers and how recent studies have addressed these concerns. The paper explains that pretrained language models have achieved remarkable improvements in various NLP tasks, but the excessive computational overhead during inference has hindered their use in real applications. The paper highlights the need for studies that address the computational and energy efficiency of large-scale transformers. It also discusses the recent work on PoWER-BERT, a model that addresses these concerns by using a novel architecture that reduces the computational overhead significantly.\tThe work targets an interesting direction of improving the efficiency of Transformers by reducing the sequence length. The main contributions of the work are (1) proposing LengthDrop as the way to achieve length reduction; (2) utilizing techniques developed in NAS, namely one-shot NAS, to enable proper training and allow adaptive drop ratio search after training. All these ideas are very reasonable and interesting. Empirically, the authors show that the proposed method is able to match or even outperform BERT-base model with 1/3 - 1/2 FLOPs during inference (not training).""379\tThis paper provides an overview of graph neural networks (GNNs), a type of neural network used for processing graph-Structured data. It discusses the\u80cc\u666f\uff0c\u5373 Graphs are ubiquitous in the real world, social networks, traffic networks, knowledge graphs, and molecular structures are typical graph-structured data. Graph Neural Networks (GNNs) have rapid development recently and are used to represent different graph structures. The paper also discusses the Expressive Power of GNNs, which decides the performance of GNNs on large graphs with complex topology. The paper also discusses the neighborhood aggregation scheme, which is used to encode graph structures and the weisfeiler-lehman (WL) graph isomorphism test, which is used to compare graph structures.\tof the paper: The main objective of the paper is to improve the expressiveness of the GNN by exploring powerful aggregators. The requirements to build more powerful aggregators are analysed. It is closely related to finding strategy for preserving the rank of hidden features, and implies that basic aggregators correspond to a special case of low-rank transformations.""380\tThis paper discusses the development of Graph Neural Networks (GNNs) and their use in representing and understanding graph-structured data. It also explores the impact of the neighborhood aggregation scheme (message passing) and the weisfiler-lehman (WL) graph isomorphism test on the expressive power of GNNs and their ability to represent different graph structures. The paper also highlights the challenges and opportunities in using GNNs for graph data analysis.\tThe authors propose two new layers for GNNs. CombConv and ExpandingConv are motivated by the insight that a GNN is only as expressive as the rank of the matrix that represents the coefficients of the aggregation function. To arrive at this statement, the authors formalize all GNNs as being composed of three steps: 1) generation of aggregation coefficients, 2) actual aggregation of the neighbourhood, and 3) feature extraction from the aggregation. Furthermore, it is shown that current approaches have very low distinguishing strength and that CombConv and ExpandingConv, by their construction, yield higher expressive power. ""381\tThis paper discusses the development of Graph Neural Networks (GNNs) and their ability to represent different graph structures. It also discusses the Expressive Power of GNNs, which measures their abilities to represent different graph structures and decide the performance of GNNs on large graphs with complex topology. The paper also discusses the neighborhood aggregation scheme used to encode graph structures and the Weisfeiler-Lehman graph isomorphism test used to compare graph structures.\t\tThis paper explores the representation power of graph neural networks. Unlike recent work on choosing among simple aggregation functions or combinations thereof, the authors here recognize that these aggregators are the bottleneck in the representation power and generalize simple aggregator functions commonly used in literature to an aggregation coefficient matrix. The paper supports this construction theoretically and also proposes two aggregators that satisfy the rank-preservation requirement for more expressive (distinguishing) GNNs.""382\tThis paper discusses the importance of disentanglement in deep learning, the recent progress made in achieving this goal using deep generative models, and the challenges and consistency issues encountered in quantifying the degree of disentanglement. It\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b evaluation metrics that can be used to compare the disentanglement of different models, and suggests ways to improve the quality of evaluation data and to overcome the challenges of evaluating disentanglement using existing metrics.\tThe paper presents a disentanglement metric to measure the intrinsic properties of a generative model with respect to the factor of variation in the dataset. Toward this, the paper first assumes disentangled factors reside in different manifolds. These different manifolds are the sub-manifolds of some manifold M for a given disentangled generative model. The paper considers the fact that in an entangled model the sub-manifolds are not homeomorphic and thus similarity across submanifolds can be measured to evaluate a model\u2019s disentanglement. As such, disentanglement is related to the topological similarity.  For measuring topological similarity, the paper then introduces Wasserstein Relative Living Times. The proposed metric is used to evaluate standard disentanglement methods and datasets demonstrating the importance. ""383\tThis paper discusses the importance of learning disentangled representations and the recent progress made in achieving this goal using deep generative models. It also highlights the challenges and inconsistent results that have been encountered in quantify the degree of disentanglement between the generator and the received representation. The paper suggests that evaluation metrics that do not depend on additional models or that are more flexible in terms of how they evaluate the disentanglement between the generator and the received representation are needed to provide more comprehensive and accurate results.\tIntroduces unsupervised disentangling metric that measures homeomorphic similarity between submanifolds conditioned on a given factor, and homeomorphic dissimilarity on submanifolds conditioned on different factors. The paper also includes a supervised variant which can directly assess topological similarity of submanifolds with label-spaces. The paper also introduces a novel variation of RLTs that  employs wasserstein distance instead of euclidean distance. ""384\tThis paper discusses the importance of learning disentangled representations and the recent progress made in achieving this goal by deep generative models. The paper also examines the challenges and inconsistent results encountered inquantizing the extent of disentanglement. To overcome these challenges, the paper proposes a new evaluation metric that is not dependent on additional models or training objectives. The paper also highlights the need for further research in developing more effective evaluation metrics for evaluating the performance of disentanglement-based models.\tThe paper proposes a novel metric for evaluating disentanglement by taking a manifold-topological perspective on the representations learnt. The key insight is that for a disentangled representation, when we fix a certain factor of variation at different values the topology of the conditional sub-manifolds should be similar. Using this insight the paper proposes a metric for disentangling which does not require annotations of the factors of variation and is more general than previous such tests.""385\tThis paper discusses the use of unlearnable examples in deep learning. It explains the background of the issue of data collection without mutual consent and the importance of making training examples unusable for deep neural networks. The paper proposes the use of unlearnable examples to address this issue, and provides examples of how this can be done. The paper also examines the potential consequences of using unlearnable examples, including the impact on the accuracy of deep learning models and the potential for privacy concerns. The conclusion of the paper states that the use of unlearnable examples can be a useful tool for addressing the issues associated with data collection without mutual consent, but that it is important to carefully consider the potential consequences before using them.\tThe paper's motivation is based on protecting private data and preventing its being scraped and used to train models. Even though motivation is clear and very important, the problem is the same as the works in crafting adversarial samples (i.e., the ones under data poisoning and adversarial attacks parts of the related work). The key difference is to apply Projected Gradient Descent (Mandry et al. 2018) in the reverse direction iteratively to *minimize* the loss function.  Furthermore, the performance evaluation will be the margin between models trained on completely clean data and sample-wise/class-wise adversarially corrupted data (in contrast to fooling a pretrained network in adversarial attack benchmarks). ""386\tThis paper discusses the use of unlearnable examples in deep learning. It explains how large datasets, such as ImageNet and ReCoRD, were created and how they provide a playground for developing deep learning models. However, it also mentions that some datasets were collected without mutual consent and personal data has been unconsciously collected from the internet for training commercial models. This raises public concerns about the use of personal data for unauthorized or even illegal purposes. This paper introduces unlearnable examples, which aim to make training examples unusable for deep neural networks. It also suggests that there may be other ways to address concerns about the use of personal data in deep learning.\tThe authors proposed the idea of using invisible noise to make personal data unusable to authorized deep learning models. To achieve this goal, the authors proposed the idea of error-minimizing noise crafted by a min-min optimization method. The error-minimizing noise is then added to training examples to make them unlearnable to deep learning models. The idea is very well motivated and explained. The experiments not only confirm the exceptional effectiveness of the proposed method but also show its flexibility.""387\tThis paper discusses the use of unlearnable examples in deep learning, which aim to make training examples unusable for deep neural networks. The paper argues that the availability of large-scale datasets and the use of free personal data have led to significant success in several fields, including computer vision and natural language processing, but has also raised concerns about the exploration of personal data for unauthorized or illegal purposes. The paper introducing unlearnable examples, which aim to address this concern by making training examples unusable for deep neural networks. The paper also discuss the potential applications of unlearnable examples in other fields such as medicine and\u91d1\u878d.\tThe authors studied the problem of data protection from a new perspective. They proposed one kind of error-minimizing noise to make the data (added noise) unlearnable. The noise is imperceptible to human eyes, and thus does not affect normal data utility. The idea is very interesting and inspiring. The authors conducted a series of solid experiments to validate the effectiveness of the proposed noise, and tested it on a real world task of face recognition. ""388\tThis paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero to stochastic, two-player, zero-sum games of perfect information. In NDMZ, the element of chance is formalized as a player and a policy for the chance player is determined through interactions with the environment.NDMZ is trained end-to-end in terms of policy and value, but it also aims to learn the player identity policy and the chance player policy. The assumption is that the environment dynamics are perfect and the player has to make decisions based on these dynamics. This extension of MuZero allows it to tackle more complex stochastic games and perform better in those domains.\tThe paper extends MuZero for nondeterministic domains (NDMZ). Compared to MuZero NDMZ also learns a function that determines who is to act (player 1,2 or chance) and a distribution of chance outcomes. This makes it possible to employ MCTS search adjusted to handle nondeterministic nodes on top of a tree constructed by NDMZ's neural nets.""389\tThis paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero to stochastic, two-player, zero-sum games of perfect information. InNDMZ, we formalize the element of chance as a player in the game, determine a policy for the chance player via interaction with the environment, and augment the tree search to allow for chance actions. As with MuZero,NDMZ is trained end-to-end in terms of policy and value. However,NDMZ aims to learn two additional quantities: the player identity policy and the chance player policy. With the assumption that the environment has perfect information,NDMZ is able to perform better than previous algorithms in challenging domains such as the Atari suite.\tThis paper proposes NDMZ, which extends the previous MuZero algorithm to stochastic two-layer zero-sum games of perfect information. NDMZ formalize chance as a player (chance player) and introduces two additional quantities: the player identity policy and the chance player policy. NDMZ also introduce new node classes to MCTS, which allows it to accommodate chance.""390\tThis paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero to stochastic, two-player, zero-sum games of perfect information. In NDMZ, the element of chance is formalized as a player and a policy for the chance player is determined through interaction with the environment.NDMZ augments the tree search to allow for chance actions. As with MuZero,NDMZ is trained end-to-end in terms of policy and value.NDMZ aims to learn the player identity policy and the chance player policy. The paper provides an overview of the challenges and opportunities associated with playing stochastic, two-player, zero-sum games of perfect information, and presents an efficient, learnable approach for achieving such performance.\tThis paper introduces NDMZ, short for nondeterministic MuZero, a deep reinforcement learning algorithm for model-based RL that doesn't use the rules of the game to perform search. The paper's contribution is mostly focused on describing how to construct the algorithm, and experimental results are provided at the end. A good analogy is that of a player that must play a (physical) board game by not only making decisions, but also acting out the game: producing random events, such as die rolls, and moving pieces on the board. ""391\tThis paper discusses the use ofHierarchical methods in deep reinforcement learning, specifically the options framework. It explains how the option policies can support reuse of low-level behaviours and accelerate learning. The advantages introduced by the hierarchical control scheme arepartially balanced by additional complexities, including possible degenerate cases and trade-offs regarding option length.\tThis paper introduces a novel option-learning policy gradient method, HO2. The method learns a parameterized joint distribution over options and actions and uses a soft-continuation based approach to interrupt or \"switch\" between options before option termination. The method introduces a new meta-parameter which enforces a hard limit on the number of \"switches\" that can occur, significantly reducing the variance of the option-learning method and replacing softer loss penalization based approaches. The paper demonstrates the performance of the proposed algorithm on a handful of 3D virtualized environments as well as on robotic simulation tasks.""392\tThis paper discusses the use ofHierarchical methods, such as the options framework, in reinforcement learning. The options framework allows for the integration of differentAbstractions into the agent, and can help improve data efficiency by supporting reuse of low-level behaviors. The advantages introduced by the hierarchical control scheme arepartially balanced by additional complexities, including possible degenerate cases and trade-offs regarding option length.\tThe paper considers the Hierarchical Reinforcement Learning setting, Options in particular, and proposes an algorithm that allows to learn both the high-level and low-level (option) policies at once, from off-policy samples. An original aspect of the algorithm is that it is easy to constrain the learned policies on how often they terminate an option and start a new one. This prevents the agent from learning tiny options that immediately terminate. It is unclear whether it can also be used to prevent the agent from learning a single big option that does everything.""393\tThis paper discusses the importance of adding structure to the solution space in order to improve data efficiency in deep reinforcement learning. Hierarchical methods, such as the options framework, are one approach to incorporating different Abstractions into the agent. By representing an agent as a combination of low-level and high-level controllers, option policies can support reuse of low-level behaviours and can ultimately accelerate learning. However, this approach is also balanced by additional complexities, including possible degenerate cases and trade-offs regarding option length.\tThis paper studies an important area in RL, hierarchical RL, which improves data efficiency by incorporating abstractions. In this paper, the authors proposes an efficient option learning algorithm, which utilizes a TD(0) type objective and constrains the learned policy being not too far away from the past policy. In terms of different abstractions, the paper studies action abstraction through a mixture policy, and temporal abstraction through explicitly limiting the maximum number of switches between options. ""394\tThis paper provides an overview of reinforcement learning (RL), a field of artificial intelligence that involves using algorithms to learn policies that maximize the expected return over a horizon. The paper focuses on the action-value function, which is a crucial component of RL and is used to determine the optimal action to take in each state. The action-value function is given by Q\u03c0(s,a) = E[R(t) | (s0,a0) = (s,a)] where R(t) is the reward obtained at time t, and E[|(s0,a0) = (s,a)] is the expected value of the action taken in state (s,a) over a horizon of t. The Bellman equation states that the action-value function can be derived from the expected value function, which is the product of the state value function and the action value function. The paper discusses the importance of the action-value function, the limitations of optimizing for just the reward, and the challenges of building robust RL algorithms. It also provides an overview of the various types of RL algorithms and their strengths and weaknesses.\tThis paper proposes a modified bellman equation for reinforcement learning that optimizes the maximum expected single step reward along a trajectory, instead of the maximum cumulative reward. This formulation is applied to the generation of molecules with optimized properties of interest. A recently published molecule generation algorithm, that constructs molecules step wise via the (predicted) chemical reactions of building blocks, is modified with this new bellman formulation, and shows modest improvements in optimizing for some HIV activity targets.""395\tThis paper discusses the use of action-value functions (Q-values) in reinforcement learning (RL) algorithms. The Q-value function takes as input an action and a state, and outputs the expected future reward obtained by taking that action. RL algorithms strive to maximize the expected cumulative reward over time, which can be achieved by optimizing the Q-value function over all possible actions and states. The paper discusses the basic concepts of RL, the Q-value function, the Bellman equation, and the importance of discounting the reward. It also explains why optimizing for the current state is more effective in practice than optimizing for the future rewards. The paper also presents an example of a simple RL algorithm and discusses its performance on a test set.\tMotivated by the de novo drug design, this submission proposed a new objective in reinforcement learning, i.e., to maximize the expected maximum rather than the accumulated reward along trajectories. The authors defined the corresponding Bellman operator, and then proved its theoretical properties, including monotonicity and contraction. In the experiments, the authors first showed on a simulated grid that when compared with Q-learning, the proposed Max-Q algorithm can achieve higher maximum rewards along trajectories. Finally, the authors tested on de novo drug design task, by modifying the TD target in the previous PGFS algorithm. The new variant achieved better performance across different metrics.""396\tThis paper discusses reinforcement learning (RL) algorithms and their use in building policy algorithms for a Markov Decision Process (MDP). The paper argues that the Bellman equation, which defines the action-value function in terms of the expected return and the probability transition function, is the foundation of RL. However, it suggests that it is not enough to optimize only the action-value function, but rather that the agent should also consider the environment's state-space and the environment's reward function. This is necessary to build a complete policy algorithm that can learn a\u6700\u4f18 action\u7b56\u7565 over time. The paper also discusses some of the challenges in building robust and efficient RL algorithms, such as the need for efficient state-space exploration and the need to handle\u4e45\u89c6\u6027\u5b9a\u7406 (VRT) issues.\tThis paper proposes a max reward instead of cumulative reward objective for reinforcement learning. This objective is primarily motivated by applications like chemical synthesis where the goal is for the RL agent to generate the most desirable state possible. The paper then defines the corresponding varaint of the Bellman operator (the max-Bellman operator) and proves tabular convergence guarantees by a contraction argument. Some experiments in a gridworld and simulated chemical synthesis indicate that this objective modification can improve prior algorithms. ""397\tThe paper discusses the problem of cold-start in deep learning, which occurs when a machine learning model is trained on a small number of labeled data points, and then is used to make predictions on new data that has not yet been labeled. The performance of the model is typically poor in this low-data regime, due to the cold-start problem. The paper provides an overview of the problem, including its history and existing solutions, and discusses potential future directions for addressing the problem.\tThe paper proposes a few-shot meta-learning method for recommender system that uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. The paper focuses on the cold-start problem where few samples with a new feature observed is available. The method outperforms a wide range of baselines on MovieLens-1M, a medical synthetic dataset, and a e-learning dataset.""398\tThe paper discusses the problem of cold-start in deep learning, where a machine learning model is trained on a small number of labeled data points and struggles to make accurate predictions on new, unobserved features. The cold-start problem is common in many deep learning application domains and is known to affect both linear and deep learning models. The paper proposes several solutions to this problem, including using additional data to warm up the model, using techniques such as feature engineering to improve the model's performance on new features, and using pre-trained models or large-scale datasets to overcome the cold-start problem. The paper also discusses the potential limitations of these solutions and suggests areas for further research.\tThe paper proposes Contextual HyperNetworks (CHNs) as an auxiliary model to generate parameters from existing data, and observations and other metadata associated with new feature to address cold start problem of new feature. Besides, it doesn\u2019t need either re-train or fine-tune at prediction time. The CHN is applied to P-VAE and some experimental results are provided to demonstrate its effectiveness in some application, i.e., recommender system, e-learning and healthcare tasks.""399\tThe paper discusses the problem of cold-start in deep learning, where a machine learning model is trained on a small number of labeled data points and experiences poor performance when adding new features to the data that have not been observed before. The paper defines the problem\uff0c\u8ba8\u8bba\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b0\u7684\u601d\u8def\uff0c\u5305\u62ec\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u3001\u96c6\u6210\u5b66\u4e60\u7b49\u65b9\u6cd5\u6765\u5e94\u5bf9cold-start\u95ee\u9898\u3002\tThis submission focuses on the cold start problem of new entities (new items in a recommender system, new treatments in a medical application, etc.). It combines the strengths of the *relations* between a new entity and the existing entities, and the *content* features of the new entity, by fusing the two kinds of information into a neural network that outputs the estimated representation of the new entity. The proposed method outperforms several intuitive na\u00efve strategies as well as MAML.""400\tThis paper discusses the limitations of deep neural networks (DNNs) in making predictions and in capturing uncertainty in their results. DNNs are particularly well-suited for certain applications, but they still have several shortcomings that make them less suitable for others. One of the main limitations is their poor calibration and overconfidence in their predictions, which can occur especially when there is a shift in the train and test distributions. To reliable inform decision making, DNNs must be able to robustly quantify the uncertainty in their predictions, which is difficult to do using traditional Bayesian modeling methods. This paper also discusses the recent success of Bayesian deep learning in the field.\tThe authors present a new method for Bayesian deep learning motivated by the difficulty of posterior inference in the \"overparameterized\" regime of deep neural network models. The proposed method provides a principled strategy for selecting a subset of the neural network's parameters (forming a so-called \"subnetwork\") for which a full-covariance approximate posterior can be computed. The authors use the well-studied Laplace approximation with the generalized Gauss-Newton Hessian approximation for the covariance. An empirical analysis is presented which attempts to assess the efficacy of the proposed method in prediction accuracy and uncertainty quantification.""401\tThis paper discusses the limitations of deep neural networks (DNNs), which are widely used for machine learning and other applications. DNNs are particularly well-suited for tasks such as image recognition, but they still have some limitations. One of the main limitations is that DNNs are not well-suited for making predictions, particularly when the training and test distributions are different. To accurately predict the future, DNNs must be able to capture the uncertainty in their predictions. Bayesian modeling provides a principled way to capture this uncertainty using the posterior distribution over model parameters, but DNNs are difficult to work with due to their nonlinearities. Despite recent success in the field of Bayesian deep learning, there is still much work to be done in\u514b\u670d DNNs' limitations.\tThe paper proposes to approximate the posterior distribution of a Bayesian neural network by an approximation that consists of a deterministic component. The authors select a sub network and infer approximate posterior distributions over the weights in the sub network. All other weights are estimated via MAP point estimation.  A sufficiently small sub-network allows high fidelity posterior approximations that do not make restrictive mean field assumptions to be tractable.""402\tThis paper discusses the limitations of deep neural networks (DNNs), particularly their ability to capture uncertainty in their predictions and to accurately calibration their models. DNNs are still a popular choice for many applications, but they have some critical shortcomings that make them less suitable for some applications. One of the main issues is that DNNs are often overconfident in their predictions, especially when there is a shift in the train and test distributions. To accurately inform decision making, DNNs must be able to robustly quantify the uncertainty in their predictions. Bayesian modeling provides a principled way to capture predictive uncertainty through the posterior distribution over model parameters, but this is difficult to achieve in DNNs due to their nonlinearities. Despite recent success in Bayesian deep learning, the issue of uncertainty in DNN predictions remains a significant challenge.\tThe authors focus on the important problem of scalable approximate inference in Bayesian NNs. More specifically, they propose a method for scalable BNNs via a (full-covariance Gaussian) Laplace approximation on a (Wasserstein-based) pruned subnetwork within a deterministically-trained model. They include a theoretical analysis for a simple generalized linear model, and experiments on 1D regression, tabular regression, and larger-scale image classification with CIFAR-10 (using the dataset shift setup from Ovadia et al., (2019)). From the experiments, they show that their method generally outperforms comparable methods (including deep ensembles) on metric performance and on the ability to capture in-between uncertainty.""403\tThis paper discusses the issue of training reinforcement learning (RL) agents in the presence of large state-action spaces, which is a challenge in many real-world domains. The paper explores the idea of learning representations (embeddings) for states or actions, and suggests that using these embeddings can improve the applicability of RL to real-world domains. previous work has explored the idea of using state embeddings, but these methods have not been widely adopted. The paper proposes a new approach to training RL agents that uses a combination of state embeddings and a policy learning method. The resulting agents are able to learn efficient policies for solving real-world problems in the presence of large state-action spaces. The paper also discusses the potential limitations of the proposed approach and suggests future directions for studying the issue of training RL agents in the presence of large state-action spaces.\tThe paper proposes a framework of jointly learning a state and action embedding using the model of the environment, eventually using those embeddings to learn a parameterized control policy using standard policy gradient (PG) methods. Joint learning of state and action embeddings allows us to capture the interactions between actions in different states. The framework proposes to learn an internal (embedding) policy, a state embedding, an inverse function on action embeddings, combining all the parts to form an overall policy. The paper theoretically shows that optimizing the internal policy leads to an optimal overall policy. ""404\tThis paper discusses the problem of training reinforcement learning (RL) agents in the presence of large state-action spaces, which is a challenge in many real-world domains. The paper proposes the use of state embeddings to represent the states, and discusses previous work on this topic. The paper also\u8ba8\u8bbas how to efficiently train RL agents in the presence of large state-action spaces, and\u63d0\u51fa\u4e00\u4e9b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848. The paper ends by discussing the potential applications of this work in real-world RL.\tThe paper proposes a method to jointly learn: (a) a latent state embedding; (b) a latent action embedding; (c) a state transition model; and (d) an RL policy.  The latent models should allow for better generalization over states and actions, and therefore result in improved learning, particularly for discrete action domains. The method shows improved performance over vanilla policy gradient on a grid-world task, a slot machine task, a recommender system, and half-cheetah locomotion.""405\tThis paper discusses the problem of training reinforcement learning (RL) agents in the presence of large state-action spaces, which is a common challenge in real-world domains. The paper proposes the use of state embeddings to represent the states, and explores the idea of using action embeddings to represent actions. It also discusses previous work in this area and\u63d0\u51fa\u4e86\u4e00\u4e9b possible solutions to the problem. The paper concludes by highlighting the potential benefits of using state embeddings in RL, including improved efficiency and better applicability to real-world domains.\tLearning on environments with large state-action spaces can be difficult. This paper addresses this issue by learning a joint state-action embedding and learn an internal policy(\\pi_i) on this embedded state-action space instead of the original state-action space. There are three parts of learning, 1. learning the embedding model that learns mapping from state to state embedding, 2. learning the internal policy, and 3. learning the mapping from action embedding to action space. The authors justify this approach by showing that the overall policy (\\pi_o) can be expressed in terms of the internal policy (\\pi_i). Furthermore, there is equivalence between the internal state-action-value function and overall state-action-value function and the authors show that updating \\pi_i is equivalent to updating \\pi_o. ""406\tThis paper discusses the recent advances in unsupervised representation learning in computer vision, specifically in view-based methods, which enable strong performance on transfer tasks by training models to maximize the mutual information between different views of an image. This work has been motivated by the idea that human-defined data transformations, such as changes to color and contrast, can target capabilities or invariances thought to be useful for transfer tasks. Contrastive learning of visual representations is used to train models to maximize the mutual information between different views of an image. The paper also discusses the space of possible image views and their effects on transfer learning.\tLearning representations using self-supervision requires domain expertise to identify diverse transformations of the data samples that label preserving. This can be expensive and hard to obtain in many data modalities. The paper proposes to automate this by learning to generate transformations tailed to each modality and sample. Specifically, an adversarial strategy is applied to learning transformations that are close to the original view in the input space but hard to classify for the self-supervision encoder.""407\tThis paper discusses the use of unsupervised representation learning techniques in computer vision, specifically in the context of view-based methods, which enable strong performance on transfer tasks by target capabilities or invariances thought to be useful for transfer learning. The paper also discusses the space of possible image views and their effects on transfer learning, as well as some recent advances in this area.\tThe paper presents a generative model to automatically generate data that is needed for contrastive learning, with a focus on the SimCLR framework, while the method itself is general. Experiments were conducted across multiple modalities, including image, speech and wearable sensor data. The results demonstrate the effectiveness of the proposed model as compared to data augmentation methods relying on human domain knowledge. ""408\tThis paper discusses recent advances in unsupervised representation learning in computer vision, specifically view-based methods that enable strong performance on transfer tasks by training models to maximize the mutual information between different views of an image. These methods target capabilities or invariances thought to be useful for transfer tasks. The paper also discusses the space of possible image views and their effects on transfer learning, as well as work that has investigates these topics.\tThe paper proposes a method for automatic generation of data views for contrastive self-supervised learning of representations. The method consists of learning an adversarial perturbation model that aims to maximize the distance between the original image and its perturbed views in the space of learned representations. To avoid collapse to a completely information-destroying perturbation model, authors propose to limit the perturbation strength in terms of the $l_p$ norm of the added noise. Authors apply their method on various image, speech and wearable sensor datasets where the proposed approach provides an improvement over other methods.""409\tThis paper discusses the challenges that arise when using deep neural networks (DNNs) in safety-critical systems, such as self-driving vehicles, aircraft collision avoidance, and medical diagnoses. The paper\u8ba8\u8bba\u4e86\u4ee5\u4e0b\u95ee\u9898\uff1a 1. DNN\u7684\u8106\u5f31\u6027\uff0c\u5373\u5b83\u4eec\u65e0\u6cd5\u610f\u8bc6\u5230\u81ea\u5df1\u63a5\u6536\u5230\u7684\u65b0\u8f93\u5165\u662f\u5426\u4f4d\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u4e4b\u5916\uff0c\u5e76\u4e14\u8fd9\u4e9b\u8f93\u5165\u53ef\u80fd\u662f\u5bfc\u81f4\u4e0d\u6b63\u786e\u9884\u6d4b\u7684\u5916\u90e8\u56e0\u7d20\u3002 2. DNN\u5728\u5904\u7406\u4e0d\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u5165\u65f6\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u9884\u6d4b\u3002\u8fd9\u5728\u6e38\u620f\u548c\u533b\u7597\u7b49\u9ad8\u5ea6\u53ef\u9760\u7684\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\u3002 3. \u4e3a\u4e86\u5b9e\u73b0\u5bf9DNN\u6a21\u578b\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\uff0c\u5fc5\u987b\u5728\u9ad8\u53ef\u9760\u6027\u5e94\u7528\u7a0b\u5e8f\u4e2d\u68c0\u6d4b\u5230\u4e0d\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u5165\u3002 4. \u5df2\u6709\u8bb8\u591a\u7814\u7a76\u5728\u63a2\u8ba8\u8fd9\u4e2a\u95ee\u9898\uff0c\u5305\u62ec Guo et al. (2017a), Hendrycks & Gimpel (2016), and De Fauw et al. (2018)\u3002\u8fd9\u4e9b\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\u3002 5. \u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\u5305\u62ec\u4f7f\u7528\u989d\u5916\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u7528\u6b63\u5219\u5316\u6280\u672f\u6765\u9650\u5236\u6a21\u578b\u5bf9\u4e0d\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u5165\u7684\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\uff0c\u4ee5\u53ca\u4f7f\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(dRCNN)\u7b49\u6a21\u578b\u6765\u51cf\u5c11\u9519\u8bef\u9884\u6d4b\u7684\u98ce\u9669\u3002 \u603b\u7ed3\uff1a\u8be5 paper \u8ba8\u8bba\u4e86\u4f7f\u7528 DNN \u6765\u89e3\u51b3\u5728\u5b89\u5168\u7ea7\u522b\u9ad8\u7684\u5e94\u7528\u4e2d\u51fa\u73b0\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u5305\u62ec DNN \u7684\u8106\u5f31\u6027\u3001\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u9884\u6d4b\u4ee5\u53ca\u5982\u4f55\u5728\u9ad8\u53ef\u9760\u6027\u5e94\u7528\u7a0b\u5e8f\u4e2d\u68c0\u6d4b\u5230\u4e0d\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u5165\u3002\u8be5 paper \u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u5b9e\u73b0\u5bf9 DNN \u6a21\u578b\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002\tThis paper introduces a taxonomy of OODs and proposed an integrated approach to detect different types of OODs. Their taxonomy classifies OOD on the nature of their uncertainty and they show that no single state-of-the-art approach detects all these OOD types. Motivated by this observation, they combine multiple existing OOD detection methods to detect various types of OODs. ""410\tThis paper discusses the challenges faced by deep neural networks (DNNs) when used in safety-critical systems, such as self-driving vehicles, aircraft collision avoidance, and medical diagnoses. The paper highlights the importance of making DNNs self-aware of when new inputs are outside the training distribution, which can lead to overconfidence in their predictions. The paper also discusses the need for techniques to address the issue of overfitting, which can occur when DNNs are trained on a small number of examples and become too specialized to the data. The paper\u6700\u540e suggests ways to improve the performance of DNNs in safety-critical systems by using data-driven approaches and incorporating feedback from the systems they are being used in.\tThis paper introduces a novel taxonomy for OOD outliers. The authors analyze current OOD detection approaches and uncover their limitations. They propose to fuse several existing approaches into a combined one and extensively evaluate it on various data sets (CIFAR,10, SVNH, MNIST, STL10, ImageNet, etc.). The proposed integrated OOD detection approach clearly shows superior performance.""411\tThis paper discusses the challenges faced by deep neural networks (DNNs) when used in safety-critical systems, such as self-driving vehicles, aircraft collision avoidance, and medical diagnoses. The paper highlights the difficulties in training DNNs to make accurate predictions on inputs that are outside the training distribution, and the overconfidence of the network on such inputs. The paper also discusses the need for ways to detect such inputs and prevent the use of DNNs in such systems. The paper\u6700\u540e proposes some possible solutions to these challenges and suggestions for future research.\tThe authors explore the different kinds of outliers and show that the methods previously proposed detect different kinds of OOD and not a single one can detect them all. The authors propose an interesting study of the different kind of outlier on synthetic data which  illustrates well the different characteristics of the outlier types. The authors then propose to combine different methods to increase the OOD detection rate. Experiments are conducted on 3 images classification datasets using different deep neural networks. For each dataset, samples from other databases are introduced as outliers and must be detected. The combination method yield better detection rates than baseline methods in almost all configurations. ""412\tThis paper presents a hierarchicalVAE, which outperforms the PixelCNN in log likelihood on all natural image benchmarks for the first time. The paper begins by observing that in theory, VAEs can represent autoregressive models as well as faster, better models when madesufficiently deep. Despite this, autoregressive models have historically outperform VAEs in log likelihood. The paper then scales a VAE to greater stochastic depth than previously explored and demonstrates that it achieve higher likelihoods, uses fewer parameters, generates samples thousands of times faster, and is more easily applied to high-resolution images. Qualitative studies suggest that this is because the VAE learns efficient hierarchical visual representations. The paper also releases the source code and models of the hierarchicalVAE at https://github.com/openai/vdvae.\tThe paper claims that high quality of generated samples and SOTA bpds are achievable by VAEs if the model is deep enough (deep in terms of the number of stochastic layers). The authors explain the architecture that resemblances the U-net architecture, and explain its building blocks. Interestingly, they are able to learn VAEs with up to 78 stochastic layers, and achieve SOTA bpds on CIFAR-10, ImageNet-32, ImageNet-64, FFHQ-256 (5-bit), and setting a great result on FFHQ-1024 (8bit).""413\tThis paper presents a hierarchical VAE, which outperforms the PixelCNN in log likelihood on natural image benchmarks for the first time. The paper starts by observing that in theory, VAEs can represent autoregressive models as well as faster, better models when made sufficiently deep. Despite this, autoregressive models have historically outperform VAEs in log likelihood. The paper then explores how increasing the stochastic depth of a VAE can explain this difference. It evaluated the performance of a VAE with greater stochastic depth on CIFAR-10, ImageNet, and FFHQ and found that the VAE achieved higher likelihoods, used fewer parameters, generated samples thousands of times faster, and was more easily applied to high-resolution images. Qualitative studies suggest that the VAE learns efficient hierarchical visual representations. The paper\u6700\u540e releases the source code and models of the hierarchical VAE at https://github.com/openai/vdvae .\tthe paper puts forward an idea that deep-enough VAE should perform at least as well as autoregressive models. Authors explore this in the context of image generation, and construct VAE model that is a generalisation of typical autoregressive architectures. They use several tricks to ensure stable training of very deep VAEs and show that final performance exceeds all autoregressive models. This experimentally supports their claim that very deep VAEs encompass autoregressive models.""414\tThis paper presents a hierarchicalVAE, which outperforms the PixelCNN in log likelihood on natural image benchmarks for the first time. The paper begins by observing that in theory, VAEs can represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperform VAEs in log likelihood. The paper then scales a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. The source code and models for the hierarchicalVAE are available at https://github.com/openai/vdvae.\tThis paper shows that deep hierarchical VAEs can outperform state-of-the-art autoregressive models on images. The authors first argue that autoregressive models are special cases of hierarchical VAEs and that hierarchical VAEs are universal approximators. They introduce a simple top-down (LVAE) architecture that scales past 70 layers. Furthermore, the model can be trained without using freebits or KL annealing -- although additional tricks are required (gradient skipping and prior warmup). They demonstrate that likelihood performance is correlated with depth and report state-of-the-art performances on multiple image datasets.""415\tThis paper discusses the recent advances in computer vision made using unsupervised learning, specifically contrastive objectives, in order to achieve human-level performance in a variety of visual tasks. The paper begins by discussing the challenges of using large image datasets and semantic annotations, which are necessary for widespread adoption of supervised learning. It then discusses the recent advances in using unsupervised learning, specifically contrastive objectives, to achieve these goals. The paper then ends by discussing the potential future directions of this research and the limitations of the current state-of-the-art.\tThis paper adopts semi-hard negative mining, a sampling strategy widely used for metric learning, for contrastive self-supervised learning. Specifically, the paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles (close, but not too close) in terms of the normalized feature distance. As the initial representation is not informative, the paper anneals down the percentile range. This sampling strategy improves the contrastive learning methods (IR, CMC, MoCO).""416\tThis paper discusses the recent rise in performance of computer vision algorithms in the context of large image datasets and semantic annotations. The paper highlights the challenges in widespread adoption of these algorithms, and the use of unsupervised objectives, such as contrastive learning, as a way to overcome these challenges. The paper also discusses the recent advances in pretraining and how these advances have closed the gap to human-level performance in several visual tasks.\tThis is an interesting paper that discusses the negative sample mining in visual representation learning. The authors discuss the theory and method to conditionally select the negative samples based on the dot product of representations in noise constructive estimation (NCE). Their theory shows that the NCE with negative examples sampling from a conditional distribution q is lower bounded with mutual information, and the object has higher bias and lower variance. The authors also provide the method to construct the conditional distribution by picking a ring surface where the dot product of representations is bounded within percentiles of data.""417\tThis paper discusses the recent success of unsupervised learning in computer vision, specifically in closing the gap to supervised baselines in several visual tasks, such as object detection and object tracking. The paper also examines the role of contrastive objectives in this process. The paper begins by introducing the background of computer vision and the challenges of training models on large image datasets with semantic labels. It then discusses the recent advances in unsupervised learning, including the use of contrastive objectives, and the role of these objectives in closing the gap to supervised models. The paper ends by providing a summary of the main findings and future directions of the study.\tInspired by the effectiveness of hard negative mining in deep metric learning, this papers focuses on the problem of negative mining in unsupervised learning under the contrastive setting. One of the problems in this scenario is that naively selecting difficult negatives may yield an objective that no longer bounds mutual information, which is the basis for many contrastive objectives such as the Noise Contrastive Estimator. To address this problem, this paper formally defines a family of conditional distributions where negatives can be drawn from (negatives are chosen conditional on the current instance), while maintaining a lower bound on the NCE and on mutual information, resulting in a new estimator dubbed Conditional NCE. It also shows that, even though it\u2019s a looser bound than NCE, it also has lower variance, which may lead to better local optima. Finally, within this family of conditional distributions, the paper proposes the Ring model, which takes inspiration from semi-hard negative mining approaches, and that can be applied to state-of-the-art contrastive algorithms in order to sample harder negatives, resulting in better representations.""418\tThis paper discusses the emerging machine learning framework Federated Learning (FL) and its various settings, including data partitioned or horizontal FL (HFL) and feature-partitioned or vertical FL (VFL). It also highlights the challenges and opportunities that arise when working with multiple workers in a FL environment. The paper concludes by discussing some recent advances in FL and their potential applications.\tThis work introduces CAFE, a novel training algorithm to leak training data in a federated learning setup. Extending from \"deep leakage from gradient\" fake images are optimised with respect to the difference observed from the client gradients (i.e. with the real images) and the one observed with the current version of the fake image. However, DLG does not work when the mini-batch size increases due to a messy gradient representation. In this work, the authors propose to keep track of the batch index. Indeed, it may happen that the server decides of the batch index corresponding to the training data that will be used by the client during the local training. Within such conditions, a malicious server can easily store fake images corresponding to specific indices and therefore optimise correctly each fake images w.r.t the corresponding real image. ""419\tThis paper discusses the emerging field of Federated Learning (FL), a machine learning framework where a central server and multiple workers collaborate to train a machine learning model. The paper highlights the different settings in which FL can be applied, including data partitioning or horizontal FL (HFL) and feature-partitioned or vertical FL (VFL). The paper also discusses some of the recent advances in FL, such as the use of distributed architecture and big data processing. The paper\u6700\u540e concludes by discussing the future directions and challenges of FL.\tThis paper studies the data leakage issue in the federated learning. More precisely, when the servers have access to model parameters and gradients. It can recover the input data via gradient matching, and the authors claim that their method performs well even with large training batch sizes, e.g. over 40. Finally, the author also studies the possibility of attacking during learning, where they suggest that multiple updates of fake data helps. However, their contribution seems incremental, gradient matching is used in previous literature [zhu et al 2019], and their main modification is extra two regularization terms: total variation and internal representation regularization, and a data index alignment technique (whose exact meaning is unclear in the paper).""420\tThis paper presents an overview ofFederated Learning (FL), a machine learning framework where a central server and multiple workers collaborate to train a machine learning model. The paper discusses the different settings in which FL can be applied, including data partitioned or horizontal FL (HFL) and feature-partitioned or vertical FL (VFL). The paper also highlights some of the challenges and opportunities in using FL in real-world applications.\tThe submission considers the problem of reconstructing private data from gradients in a Federated Learning system, which has been recently shown to a threat in distributed learning systems. Two types of federated learning systems are considered. Vertical federated learning (VFL) refers to the case where different agents hold different features of the same data points while  Horizontal federated learning (HFL) refers to the case where different agents how all the features of different subsets of the data.""421\tThis paper discusses the issue of how to infer dynamic relationships between multi-agent trajectory data, which has practical applications in physics, vision, and robotics. The paper proposes a deep generative model called DYnamic multi-Agent Relational Inference (DYARI), which can reason about dynamic relations. The paper uses a simulated physics system to study various dynamic relationship scenarios and performs a comprehensive study on the trade-off between dynamic and inference period, as well as the impact of training scheme and model architecture on dynamic relational inference accuracy. The paper also showcases an application of DYARI to infer coordination and competition patterns from real-world multi-agent trajectory data.\tThis paper presents a method for dynamic relational inference for multi-agent trajectory prediction. The method extends the neural relational inference (NRI) (Kipf et al., 2018) by changing the static relations between agents to dynamic relations. This equates to inferring time-varying latent variables $z_t^{ij}$ as opposed to learning time-independent latent variables $z^{ij}$. The paper conducts experiments on physics simulations and basketball trajectories to show the superiority of the proposed method against different variants of NRI.""422\tThis paper discusses the problem of dynamic relational inference in multi-agent trajectory networks, where interactions between agents change over time. It proposes a deep generative model called DYnamic multi-Agent Relational Inference (DYARI), which can reason about dynamic relations. Using a simulated physics system, the paper studies various dynamic relation scenarios and performs comprehensive studies on the trade-off between dynamic and inference period, the impact of training scheme, and model architecture on dynamic relational inference accuracy. The paper also showcases an application of  dyARI to infer coordination and competition patterns from real-world multi-agent trajectory networks.\tThe authors propose a novel Relational Inference system that learns to predict the graph structure underlying the data as well as the updated state of the system. Relational reasoning has received considerable attention in recent year. Predicting the graph structure underlying a system from data in a dynamic way is an great next step, which could help alleviate some of the scalability issue currently afflicting these methods.""423\tThis paper discusses the problem of dynamic relational inference in multi-agent trajectory systems. It proposes a deep generative model called DYnamic multi-Agent Relational Inference ( dyARI ), which can reason about dynamic relations between multi-agent systems. The paper uses a simulated physics system to study various dynamic relation scenarios and performs comprehensive studies on the trade-off between dynamic and inference period, the impact of training scheme, and model architecture on dynamic relational inference accuracy. It also showcases an application of the model to infer coordination and competition patterns from real-world multi-agent systems.\tThis paper builds on Kipf et al. (2018)\u2019s Neural Relational Inference. In particular, this work introduces a latent variable model which treats the interactions (i.e. relations) between different agents as dynamic and time-varying. As in NRI, the interaction variable between any two agents is conditioned on the history of those agents\u2019 states. An agent\u2019s future state is conditioned on its history of states as well as its interaction variables with other agents.""424\tThis paper discusses the problem of matrix completion, which is a type of recommendation problem where a user-item rating matrix is partially observed and the goal is to predict the missing entries. The paper proposes a new approach to matrix completion, called the matrix completion with item-level interactions (MCI). The MCI approach uses item-level interactions to improve the performance of recommendation systems. The paper also discusses the challenges and limitations of the MCI approach and\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848. The paper\u6700\u540e presents a performance analysis of the MCI approach and discusses its potential applications in various domains.\tThis work explores a popular problem, i.e., collaborative filtering, in an inductive setting, which is very important for real-world recommender systems. To address the challenges in the inductive settings, i.e., learning accurate representations for users who do not occur in the training data, the authors propose to construct a relational graph between users in the training data and new users based on a standard matrix factorization model and then use an attentive message passing framework to inductively compute user-specific representations. Besides, the authors prove the expressive and generalization capabilities of the proposed framework. Extensive experiments are conducted to demonstrate the effectiveness of the proposed framework both in transductive and inductive settings, as well as the scalability.""425\tThe paper provides an overview of the matrix completion problem and its application in recommender systems. The author discusses the importance of capturing user interests and preferences in personalized recommendations and presents some existing methods for solving the problem. The paper also analyzes the performance of these methods and discusses the challenges and future directions in this field. The author concludes by highlighting the need for more research in this area to provide more effective and practical recommendations.\tThis paper proposed an inductive collaborative filtering method, called IRCF. The goal is to possess expressiveness (against feature-driven methods) as well as generalization (against one-hot encoding based methods). In IRCF, there are a matrix factorization model for support users and a relation model for query users. The former is trained with transductive learning to obtain support users embeddings and item embeddings. The relation model then generates query user embeddings as weighted sum of support user embeddings by examining relational graph between support and query users.""426\tThis paper discusses the matrix completion problem, a fundamental challenge in recommender systems, and presents a new approach to solve it using a graph-based model. The paper also analyzes the performance of the proposed model and compared it to existing methods. The paper argues that the proposed model has several advantages, including ease of implementation and better interpretability. The paper\u6700\u540e proposes a future research direction to further explore the problem and its applications.\tThis work proposed an inductive recommendation framework on user-item relation graphs. Such a framework relies on the user-item relations without the requirement of side-information and perceives certain flexibility in terms of the parametrization for user/item representations. The authors also provided theoretical analysis to highlight some mathematical insights out of this framework. The proposed method is evaluated on three real-world datasets and compared with several baselines.""427\tThis paper discusses a new approach to disentanglement in autoencoder-based image representation learning using a multi-stage modeling approach. The first stage involves learning the disentangled factors using an existing method such as \u03b2-TCVAE, followed by an additional stage where a deep generative model is trained to capture the missing correlation between the latent factors and add detail information while maintaining conditioning on the previously learned factors. The overall approach\u53ef\u4ee5\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5c40\u9650\u6027\u548c\u7f3a\u70b9\uff0c\u4ece\u800c\u63d0\u9ad8\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u548c\u53ef\u89c6\u5316\u6548\u679c\u3002\tDisentangled representation (DR) of data is useful in downstream tasks. However, VAE-based DR fundamentally suffers from a trade-off between high-quality reconstruction images and disentangling. To overcome this point, the paper approaches VAE from the multi-stage modeling (so-called MS-VAE). The proposed method starts from the other standard DR method which learns low-quality image(Y) and then, improves the quality of the image via training additional encoded representation(Z). The widely-used techniques in style transfer (FILM and AdaIN) are used to adopt 'Z' into 'Y' for a high-quality reconstructed image. The authors evaluated the proposed method through FID(high-quality image) and MIG(disentanglement). At the similar scale of complexity, the proposed method obtained high FID score and low MIG score than the baselines.""428\tThis paper presents a multi-stage modeling approach for image data where autoencoder-based disentanglement is achieved by penalizing the aggregate posterior to encourage statistical independence of the latent factors. The model is then trained with an additional deep generative model to improve the low-quality reconstruction. The approach overcome the trade-off between disentanglement and reconstruction quality introduced by the previous approach, where the model was trained with an autoencoder alone. The resulting model can learn more correlated latent variables that capture detail information present in most image data, while maintaining conditioning on the previously learned disentangled factors.\tThis article introduces a method for learning high-quality generative model with disentangled latent representation by splitting the learning process into two steps. The first step consists in learning a generative model on the data using a method with strong disentanglement constraints, producing a low-quality generation. As a second step, a conditional generative model is trained to turn this low-quality sample into an high quality one. This intermediate generation acts as an observed variable, and thus separates the latent spaces of the two models, effectively preventing disruptive interference in the learning of the \"independent factors\" on the one hand and the \"dependent factors\" on the other, as has been previously observed as a difficulty in the literature. The authors provide detailed empirical analysis of the performance of the model.""429\tThis paper discusses a multi-stage modeling approach for achieving disentanglement in image data using autoencoder-based techniques. The approach involves learning the disentangled factors using an existing disentanglement method, such as \u03b2-TCVAE, and then improving the reconstruction quality using a second deep generative model. The paper presents evidence that the approach is effective in achieving high-quality disentanglement and improving the reconstruction quality of image data. The approach also addresses the trade-off between disentanglement and reconstruction quality introduced by current autoencoder-based methods.\tThe paper studies the problem of learning disentangled representations while maintaining good data reconstruction. As common modeling, the latent representation is decomposed into disentangled representation C and correlated representation Z. Then a hierachical generative process is proposed, where the first stage is to reconstruct a preliminary version of the data given the disentangled representation C, and the second step is to reconstruct a full version of the data given C and correlated representation Z. The two stages are learned separately, with the first stage using the previous \u03b2-TCVAE model to learn C, and the second stage using the Feature-wise Linear Modulation (FiLM) technique.""430\tThis paper discusses the problem of state representation in deep reinforcement learning (RL) for high-dimensional observations, such as camera images. The paper provides an overview of the current state of the art in state representation learning and proposes several solutions. The paper also discusses the importance of understanding whether or not an objective is guaranteed to yield sufficient representations, which can be a crucial factor in ensuring successful state representation learning. The paper\u6700\u540e presents future directions and challenges for state representation learning in deep RL.\tThe paper discusses three mutual information (MI) objectives for representation learning in RL, referred to as forward, state, and inverse. The forward MI objective models latent dependencies given the action. The state MI objective models latent dependencies alone. And the inverse MI objective models dependencies between actions and future states (empowerment). The paper shows that of these three common objectives, only the forward objective is sufficient for learning the optimal policy / value function. This is demonstrated using simple examples and experiments on a simple game environment.""431\tThis paper discusses the challenges that arise in deep reinforcement learning (RL) when learning policies from high-dimensional observations, such as camera images. The paper\u8ba8\u8bba\u4e86\u5728 deep RL \u4e2d\u5b66\u4e60\u653f\u7b56\u4ece\u9ad8\u7ef4\u89c2\u6d4b\u4e2d\u63d0\u53d6\u6709\u7528\u8868\u793a\u7684\u6311\u6218\u3002\u4e00\u4e2a\u6709\u7528\u7684\u8868\u793a\u5e94\u8be5\u8db3\u4ee5\u5b66\u4e60\u5e76\u4ee3\u8868\u6700\u4f18\u7b56\u7565\u6216\u6700\u4f18\u4ef7\u503c\u51fd\u6570\uff0c\u800c\u4e22\u5f03\u65e0\u5173\u548c\u5197\u4f59\u7684\u4fe1\u606f\u3002\u7406\u89e3\u662f\u5426\u67d0\u4e2a\u76ee\u6807\u4e00\u5b9a\u8db3\u4ee5\u63d0\u4f9b\u8db3\u591f\u7684\u8868\u793a\u662f\u81f3\u5173\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u8fd9\u4f7f\u67d0\u4e9b\u95ee\u9898\u65e0\u6cd5\u89e3\u51b3\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u8868\u793a\u4e2d\u6ca1\u6709\u5305\u542b\u5173\u4e8e\u524d\u65b9\u505c\u6b62\u4fe1\u53f7\u706f\u7684\u989c\u8272\u4fe1\u606f\uff0c autonomous vehicle \u5c06\u65e0\u6cd5\u5b89\u5168 navigate\u3002\tof the work: This work studies which mutual-information representation learning objectives (1. forward information, 2. state-only transition information, 3. inverse information) are sufficient for control in terms of representing the optimal policy, in the context of reinforcement learning (RL). As a result, they find a representation that maximizes 1 is sufficient for optimal control under any reward function, but 2 and 3 fails to provide that guarantee in some MDP cases. They provide both proof and interesting counter examples to justify the findings. Besides, they conduct some empirical studies on a video game (i.e. Catcher) and show that the sufficiency of a representation can have a substantial impact on the performance of an RL agent that uses that representation. ""432\tThis paper discusses the problem of state representation learning in deep reinforcement learning (RL) for high-dimensional observations, such as camera images. It explains that in practice, policy learning in RL faces a bottleneck in acquiring useful representations of the observation space, which is caused by the lack of a sufficient number of features to represent the data. State representation learning approaches aim to overcome this issue by learning structured and compact representations on which to perform RL. The paper explains that a useful state representation should be sufficient to learn and represent the optimal policy or the optimal value function, while discarding irrelevant and redundant information. It also explains that understanding whether or not an objective is guaranteed to yield sufficient representations is important because insufficient representations make it impossible to solve certain problems. The paper provides an overview of the problem and the state of the art in state representation learning, and discusses potential future directions.\tThis paper studies which commonly-used mutual information objectives for learning state representations are sufficient for reinforcement learning. In particular, they provide counterexamples to show that state-only and inverse MI objectives are not Q*-sufficient, while proving that forward MI is Q*-sufficient. They validate their findings empirically with experiments in a simple RL domain.""433\tThis paper studies vector-output two-layer ReLU neural networks from an optimization perspective. The neural networks are the building blocks of deep networks and have been found to perform tremendously well for a variety of tasks. The paper finds that vector-output networks regularized with standard weight-decay have a convex semi-Infinite strong dual, which is a convex program with infinitely many constraints. However, this strong dual has a finite parameterization, and expressing this parameterization is non-trivial. The paper also finds that expressing a vector-output neural network as a convex program requires taking the convex hull of completely positive matrices. This connection between neural network training and copositive programs is novel. The paper describes algorithms which can be used to find the global minimum of the neural network trajectory.\tThis paper showed that a two-layer vector-output ReLU neural network training problem is equivalent to a finite-dimensional convex copositive program. Based on this connection, the authors gave the first algorithm that finds the global min of the network training problem, which has running time polynomial in the number of samples but exponential in the data matrix. For CNN, the running time is only exponential in the filter size, which is usually a constant. The authors also described circumstances in which the global min can be efficiently found by soft-thresholded SVD; provided a copositive relaxation that is exact for certain cases. The effectiveness of the proposed algorithms is verified in experiments.""434\tThis paper studies vector-output two-layer ReLU neural networks from an optimization perspective. It analyzes the strong dual of these networks and finds that they have a convex semi-infinite strong dual, which is a convex program with infinitely many constraints. However, the strong dual has a finite parameterization, which is non-trivial to express. The paper also finds an intimate connection between neural network training and copositive programs, which are programs over the set of completely positive matrices. It describes algorithms which can be used to find the global minimum of the neural network traget.\tThe draft is a vector extension of [1] on studying how to approximately solve the global optima of a two-layered Relu network. The key of the analysis is to enumerate all possible sign patterns of the ReLU unit generating from specific data. Once we have the enumeration, we can also enumerate the linear area separated by ReLU, and the whole optimization problem will become a non-convex quadratic optimization problem. The non-convex quadratic can be approximately solved with its convex dual (or exactly under some conditions), or we can relax it to a copositive program (which might still be NP-hard to solve). With some assumption on the data, the sign pattern of the ReLU is a singleton, then we will have efficient algorithms to exactly recover the global optima of the two-layered network.""435\tThis paper studies vector-output two-layer ReLU neural networks from an optimization perspective. The neural networks, which are the building blocks of deep networks, have been found to perform tremendously well for a variety of tasks. The paper finds that vector-output networks regularized with standard weight-decay have a convex semi-infinite strong dual, which is a convex program with infinitely many constraints. However, this strong dual has a finite parameterization, and expressing this parameterization is non-trivial. The paper also finds that expressing a vector-output neural network as a convex program requires taking the convex hull of completely positive matrices. The paper describes algorithms which can be used to find the global minimum of the neural network training traget.\tThe paper proposes a convex formulation for shallow neural networks with one hidden layer and vectorial outputs. This is an extension on a line of previous works (Ergen & Pilanci, 2020a) and (Ergen & Pilanci, 2020b) where similar results have been established for the case of scalar outputs. A Frank-Wolfe algorithm for finding the global optimum of the resulting convex program is proposed and evaluated on smaller datasets. ""436\tThis paper discusses the role of language in object segmentation in humans and machines. It explains that young infants develop object-based attention and the ability to individuate objects through a combination of spatial-temporal information, object property information, and language. The paper also discusses the general picture from cognitive science of object perception and language co-develop in support of one another. The paper\u6700\u540e presents an example of how language can support object segmentation in machines.\tThis paper proposes to combine the neuro-symbolic concept learner for visual reasoning from language (NS-CL; Mao et al., 2019) with recent unsupervised approaches to learning object-centric representations such as MONet (Burgess et al., 2019) and Slot-Attention (Locatello et al., 2020). While NS-CL normally relies on pre-trained object-detectors (in a supervised fashion) to extract visual representations, the proposed combination (dubbed LORL) use MONet or Slot Attention for this. By additionally back-propagating error signals from language-driven visual reasoning tasks obtained via NS-CL into MONet/Slot-Attention, it is shown how LORL is better able at learning object-centric representations and perform instance segmentation. ""437\tThis paper explores the relationship between language and object segmentation in cognitive development and the possible applications of this relationship in machine learning. The paper starts by discussing the role of object-based attention in\u5a74\u513f\u7684\u8ba4\u77e5\u53d1\u5c55\u4ee5\u53ca\u5b83\u4ece\u4e0d\u540c\u6765\u6e90\u83b7\u53d6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5305\u62ec\u7a7a\u95f4-\u65f6\u95f4\u4fe1\u606f\u3001\u7269\u4f53\u5c5e\u6027\u4fe1\u606f\u548c\u8bed\u8a00\u3002\u968f\u540e\uff0c\u5a74\u513f\u53ef\u4ee5\u5229\u7528\u8bed\u8a00\u83b7\u53d6\u5173\u4e8e\u7269\u4f53\u8eab\u4efd\u548c\u7c7b\u578b\u7684\u7ebf\u7d22\uff0c\u4ee5\u89e3\u51b3 object individuation \u95ee\u9898\u3002\u4eba\u7c7b\u8ba4\u77e5\u53d1\u5c55\u548c\u8bed\u8a00\u53d1\u5c55\u4e4b\u95f4\u6709\u5171\u540c\u7684\u80cc\u666f\uff0c\u673a\u5668\u7ffb\u8bd1\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u90fd\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u8fd9\u79cd\u5173\u7cfb\u5e94\u7528\u5230\u673a\u5668\u4e2d\u3002\u672c paper \u7684\u91cd\u70b9\u662f\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u652f\u6301\u7269\u4f53\u5206\u5272\u3002\u8bb8\u591a\u6700\u8fd1\u7684\u7814\u7a76\u90fd\u7814\u7a76\u4e86\u65e0\u8bed\u8a00\u4e0b\u7269\u4f53Representation learning \u7684\u95ee\u9898\uff0c\u4f8b\u5982\u901a\u8fc7\u56fe\u50cf\u6765\u8bad\u7ec3\u65e0\u76d1\u7763\u7269\u4f53\u8868\u793a\u3002\u7136\u800c\uff0c\u65e0\u8bed\u8a00\u4e0b\u7269\u4f53\u8868\u793a\u5b66\u4e60\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u901a\u8fc7\u8bed\u8a00\u4e0e\u7528\u6237\u8fdb\u884c\u4ea4\u6d41\u3002\u672c paper \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u4e0e\u8bed\u8a00\u4ea4\u4e92\u6765\u5b9e\u73b0\u65e0\u8bed\u8a00\u4e0b\u7269\u4f53\u8868\u793a\u5b66\u4e60\u3002\tThe paper proposes a framework for object-centric representation learning with additional language supervision such as e.g. questions and answers, denoted as Language-mediated, Object-centric Representation Learning (LORL). The authors combine two ideas from prior work, the unsupervised object-centric representation learning and the neural-symbolic concept learning, in one architecture. The model obtains object representations by learning to reconstruct the input image (as in MONet and Slot Attention). The learned representations are used as input to the neural-symbolic program executor, which learns to answer questions about objects. The entire model is trained in three stages: first the reconstruction objective, then the QA objective, and, finally, jointly. Experiments on two datasets demonstrate that the obtained object segmentations have better quality that those of the original unsupervised models. The learned representations are also shown to be effective in several other down-stream tasks.""438\tThis paper discusses the role of language in object segmentation, which is a skill developed by human infants and later recognized by machines. The paper highlights the importance of spatial-temporal information, object property information, and language in the development of object individuation skill. It also discusses how language can support object segmentation, by providing clues about object identity and type. The paper ends by discussing the potential applications of object segmentation in various fields, such as computer vision and natural language processing.\tThis paper proposed an interesting idea that uses language to learn the concept and aid downstream tasks such as segmentation and referential expression interpretation. The authors combine the unsupervised segmentation method (MONet and Slot Attention) with neural symbolic concept learning (NS-CL). By joint training these two objectives, the authors show improvements in the object segmentation and several downstream tasks. ""439\tThis paper proposes a new framework for knowledge graph completion, named EM-RBR (Embedding and rulebased reasoning). The main goal of the framework is to improve the representation of the knowledge graph by utilizing the background information provided by logic rules from the knowledge base implicit in fact triplets. The framework combines the advantages of reasoning based on rules and the state-of-the-art models of embedding to achieve this goal. The EM-RBR framework is capable of predicting the missing links among the knowledge graph and improving the representation of the graph.\tThe work utilizes relational background knowledge contained in logical rules to conduct multi-relational reasoning for knowledge graph (KG) completion. This is different from the superficial vector triangle linkage used in embedding models. It solves the KG completion task through rule-based reasoning rather than using rules to obtain better embeddings. Experiments on FB15K, WN18, and a new dataset FB15K-R demonstrate the effectiveness of the proposed model EM-RBR. ""440\tThis paper proposes a general framework called EM-RBR (Embedding and rulebased reasoning) for knowledge graph completion. The framework combines the advantages of reasoning based on rules with the state-of-the-art models of embedding to overcome the limitations of mainstream embedding methods that ignore the rich background information provided by logic rules driven from knowledge base implicitly. EM-RBR utilize relational background knowledge contained in rules to conduct multi-relation reasoning link prediction, which can provide a more accurate and complete representation of the knowledge graph.\tThe paper seek to improve KG representation (e.g. for link prediction and question answering) by combining logical reasoning (logical rule templates) with statistical methods (TransE). Rules are mined from the KG using AMIE and recursive backward steps are taken, using the mined rules, to determine if a fact is true.""441\tThis paper proposes a new framework for knowledge graph completion, called EM-RBR (Embedding and rulebased reasoning). The main idea of the framework is to utilize the background information provided by logic rules driven from knowledge base implicit in the embedding models to conduct multi-relation reasoning link prediction. By combining the advantages of reasoning based on rules and the state-of-the-art models of embedding, the representation of the knowledge graph is improved and accuracy is increased. The framework is shown to be effective in predicting the missing links among the knowledge graph and has been implemented and tested on a real-world data set.\tThe paper proposes a framework (EM-RBR) for doing Knowledge Base (KB) completion. Instead of the direct triple score from an embedding based method, EM-RBR allows the triple score to be calculated as a composition of the scores of the rules mined from the KB. EM-RBR uses a BFS type algorithm that recursively searches for reasoning paths connecting the triple while also updating the score.  The authors show that EM-RBR when used as an addendum to a translation-based embedding method (such as TrasnE, TransH) is able to outperform them. They show their results on FB15k and WN18. ""442\tThe paper discusses the role of short-term memory in visual cognition and the object file (OF) as a form of state memory. It explains that visual cognition requires the use of short-term memory to keep track of an object\u2019s location, properties, and history. The OF is a temporal persistent reference that permits object constancy and permanence. Additionally, the paper explains that abstract knowledge about an object is complementing information in the OF. This knowledge allows the agent to have a better understanding of the object and its relationship to the world. The paper also discussed the role of cognitive flexibility in the use of the OF and how it can be influenced by factors such as context and motivation.\tThis paper proposes a new type of recurrent neural network architecture called schema / object-file factorization (SCOFF). This model contains multiple weight-sharing GRU cells. The input information is fed into each GRU cells through an attention layer. The output information is fetched from these GRU cells and mixed with another attention layer. The model is tested on several intuitive physics benchmarks and basic reinforcement learning environment. This model demonstrates superior performance than other modular RNN architectures such as RIM on specific tasks.""443\tThe paper discusses the role of short-term memory in visual cognition and how it helps maintain object constancy and permanence. It explains that an object file, also known as an object file, is a type of short-term memory that keeps track of an object\u2019s location, properties, and history. The OF allows an intelligent agent to maintain object constancy and permanence as it navigates the world. The paper also discusses the role of abstract knowledge in visual cognition and how it helps to understand the world around us. It suggests that abstract knowledge can be used to create a sense of understanding about the objects and the world around us. Overall, the paper provides an overview of how visual cognition and short-term memory work together to allow us to persevere in the face of change and understand the world around us.\tThe authors propose SCOFF, a novel architectural motif, one with memory, which, as they describe, can serve as a drop-in for an LSTM or GRU within any architecture. It is inspired by the notion that when modeling a structured, dynamic environment (such as one with objects moving around), one must keep track of both declarative knowledge and procedural knowledge. They propose that these two types of knowledge be factored, creating an architecture consisting of \"object files\" (OF) whose evolution is governed by input, all objects, and  \"schemata\" which can be selectively applied to each OF.""444\tThe paper presents the concept of an object file, a short-term memory that keeps track of an object's location, properties, and history, in the context of visual cognition. The object file serves as a temporally persistent reference to an external object, allowing for object constancy and permanence. Additionally, the paper discusses the relationship between the object file and abstract knowledge about the world, as well as the potential implications of this technology for intelligence and the ability to interact with the world in a meaningful way.\tThe motivation and the proposal for splitting the schema from the procedural (representational) block makes sense. This is a good idea. A the authors build on top of RIMs, which have shown reasonable ways to model dynamical systems. However the paper itself needs to be improved and we need to evaluate the model more before publication. ""445\tThis paper presents a new neural module network called VilNMN, which is designed to model the information retrieval process in video-grounded language tasks. VilNMN decomposes language components into entities and actions, and then uses these parameters to instantiate neural module networks to extract visual cues from the video. The experiments show that VilNMN can achieve promising performance on two video-grounded language tasks: video QA and video-grounded.\tThis paper introduces the Visio-Linguistic Neural Module Network (VilNMN) consisting of a pipeline of dialogue and video understanding neural modules. Motivated by Hu et al. (2017), Kottur et al (2017), this paper extends the NMNs on video tasks for interpretable neural models. The model explicitly resolves entity references (dialog understanding) and detects actions from videos (video understanding) for response generation. Experiments show that NMNs achieve competitive results on AVSD (video-dialog) and TGIF-QA (video-QA) benchmarks. ""446\tThis paper presents a new neural module network called Visio-Linguistic Neural Module Network ( VilNMN ), which is designed to model the information retrieval process in video-grounded language tasks. VilNMN takes as input a video question and outputs entities and actions that are relevant to the question. It then uses these outputs as parameters to instantiate neural module networks and extract visual cues from the video. The paper describes the performance of VilNMN on two video-grounded language tasks: video QA and video-grounded. The experiments show that VilNMN can achieve promising performance on these tasks.\tThis paper studies the language grounding aspect of video-language problems. It proposes a Neural Module Network (NMN) for explicit reasoning of visually-grounded object/action entities and their relationships. The proposed method is demonstrated to be somewhat effective in the audio-visual dialogue task and has been shown superior to existing works on video QA. Overall, the paper is motivated clearly and is delivered with good clarity. The followings need to be clarified.""447\tThis paper presents a new approach to modeling the information retrieval process in video-grounded language tasks, calledVisio-Linguistic Neural Module Network ( VilNMN). VilNMN uses a pipeline of neural modules to explicitly resolve entity references and detect action-based inputs from questions in video. These inputs are then used to instantiate neural module networks and extract visual cues from the video. The paper demonstrates that VilNMN can achieve promising performance on two video-grounded language tasks: video QA and video-grounded.\tThe paper studies the application of neural module network to video-grounded language tasks. They propose a method dubbed Visio-Linguistic Neural Module Network (VilNMN) to retrieve spatio-temporal information in a video through a linguistic-based parsed program. In particular, VilNMN first extracts entity references and their corresponding actions in linguistic cues. This information is then being used to locate relevant information in the visual cue to arrive at the correct answer. The proposed method is evaluated on two large scales benchmarks AVSD and TGIF-QA, demonstrating competitive performance with state-of-the-art methods.""448\tThis paper discusses the relationship between reinforcement learning and game theory in multiagent systems. It explains the importance of finding a master policy in single-agent RL and the challenges of searching over a complex joint policy space in a multiagent environment. The paper also discusses the use of empirical game-theoretic analysis (EGTA) to approximate game models and estimate empirical games. EGTA involves using game reasoning to simulate combinations of strategies from a large space of possible strategies and then analyzing the results to find the optimal policy for the multiagent system. The paper provides examples of how EGTA can be used in real-world multiagent systems and discusses potential applications.\tThe paper suggests two techniques to improve the calculation of empirically figuring out a Nash equilibrium using an iterative application of best-response dynamics. One method learns the best-response to the previously used strategy. The other uses that technique to model the opponent, and then best-responds to the modeled opponent. The experiments show a faster reaching to NE than without these changes.""449\tThis paper discusses the use of empirical game-theoretic analysis (EGTA) in multiagent reinforcement learning (RL) systems. EGTA is a method used to approximate game models defined over a restricted strategy set and to estimate the outcome of a multiagent game using combinations of strategies from a large space of possible strategies. The paper explains the importance of including game reasoning in RL and the benefits of using a strategy set over a policy space in multiagent systems. The paper also discusses the challenges of direct search over a complex joint policy space for game solutions in a multiagent system and suggests alternative approaches to solving these challenges.\tThe paper proposes two new methods in the Policy-Space Response Oracle framework. These approaches permit to reuse past knowledge in order to reduce the amount of data required for the RL training. The first algorithm Mixed-Oracles transfers the previous iteration of Deep RL, instead of the second one, Mixed-Opponents, transfers existing strategy action-value estimates.""450\tThis paper discusses the use of empirical game-theoretic analysis (EGTA) in multiagent reinforcement learning (RL) systems. EGTA involves approximate game models defined over a set of strategy sets, and the use of game reasoning to estimate an empirical game from these strategies. The paper discusses the challenges of directly searching for game solutions in a complex joint policy space, and suggests that interleave learning and game analysis iteratively is more commonly used approach. The paper also discusses the benefits of using EGTA, including improved performance and reduced computational complexity.\tThe paper focuses on resolving the computational and sample efficiency challenges with current PSRO style approaches. To this end it proposes two different modifications to the standard PSRO setup: 1) Mixed Oracles, and 2) Mixed Opponents. These approaches allow avoiding resetting learning after each outer loop epoch and reduce the stochasticity of dynamics during training. Thee efficacy is demonstrated on relatively simple games but using Deep RL policies where the proposed approaches are at least on par with standard PSRO approach in terms of final performance while drastically improving the sample efficiency.""451\tThis paper presents Non-Attentive Tacotron, a text-to-Speech (TTS) model based on the Tacotron 2 model that\u66ff\u6362\u4e86\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5f15\u5165\u4e86 explicit duration predictor \u3002\u8be5\u6a21\u578b\u5728\u5927\u578b robustness \u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982 Unaligned duration ratio \u548c Word deletion Rate \u7b49\u6307\u6807\uff0c using a pre-trained speech recognition model \u3002\u4f7f\u7528Gaussian upsampling ,Non-Attentive Tacotron  achieve 5-scale mean opinion score \u4e3a\u81ea\u7136ness 4.41 , slightly outperforming Tacotron 2 \u3002 duration predictor Enables both utterance-wide \u548c per-phoneme control of duration at inference time \u3002\u5f53\u51c6\u786e\u7684\u76ee\u6807durations \u5728\u8bad\u7ec3\u6570\u636e\u4e2d scarce\u6216\u4e0d\u5b58\u5728\u65f6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u7528 fine-grained variational auto-encoder \u6765\u8bad\u7ec3 duration predictor \uff0c\u5728 semi-supervised \u6216 unsupervised  manner \u4e2d\uff0c\u5176\u8868\u73b0\u51e0\u4e4e\u4e0esupervised \u8bad\u7ec3\u76f8\u540c\u3002 2 \u65b9\u6cd5\u548c\u5b9e\u9a8c . \u65b9\u6cd5\u90e8\u5206\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f7f\u7528\u975e\u81ea\u9002\u5e94Tacotron \u8fdb\u884c\u8bed\u97f3\u5408\u6210\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u4f7f\u7528\u4e86Tacotron 2 \u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4fee\u6539\uff0c\u4ee5\u66ff\u6362\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5f15\u5165 explicit duration predictor \u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528Gaussian upsampling \u6765\u63d0\u9ad8\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u4e00\u7cfb\u5217\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u7684 robustness \u3002\u6211\u4eec\u5f97\u51fa\uff0c\u4e0eTacotron 2 \u76f8\u6bd4\uff0cNon-Attentive Tacotron \u76845\u7ea7mean opinion score \u4e3a4.41\uff0c\u7a0d\u9ad8\u4e8eTacotron 2 \u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u76ee\u6807durations \u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7a00\u7f3a\u6216\u4e0d\u5b58\u5728\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u7528 fine-grained variational auto-encoder \u6765\u8bad\u7ec3 duration predictor \u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u8868\u73b0\u4e0esupervised \u8bad\u7ec3\u76f8\u4f3c\u3002 3 \u7ed3\u8bba . \u7ed3\u8bba\u90e8\u5206\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4f7f\u7528\u975e\u81ea\u9002\u5e94Tacotron \u8fdb\u884c\u8bed\u97f3\u5408\u6210\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684 robustness \u3002\u6211\u4eec\u4f7f\u7528 explicit duration predictor \u66ff\u6362\u4e86\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002\u901a\u8fc7\u4f7f\u7528Gaussian upsampling \uff0c\u6211\u4eec\u53d6\u5f97\u4e86\u81ea\u7136ness \u4e3a4.41 \u7684\u597d\u6210\u7ee9\uff0c\u7565\u9ad8\u4e8eTacotron 2 \u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u51c6\u786e\u7684\u76ee\u6807durations \u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528 fine-grained variational auto-encoder \u6765\u8bad\u7ec3 duration predictor\uff0c\u5176\u8868\u73b0\u4e0esupervised \u8bad\u7ec3\u76f8\u4f3c\u3002\tIn this paper the authors tackle the problem of alignment between input tokens and output acoustic features. The key contribution of this paper is replacing the attention mechanism of the Tacotron 2 with an explicit representation of token durations. The attention mechanism is vulnerable to issues such as pauses, repetitions, and skips, and hence using durations directly takes care of such issues. The challenge lies in obtaining the durations. The authors propose different methods toward that end. ""452\tThis paper presents a text-to-Speech (TTS) model called Non-Attentive Tacotron, which is based on the Tacotron 2 model and\u66ff\u6362 the attention mechanism with an explicit duration predictor. The model improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.\tIn this paper, the authors introduce a text-to-speech model based on Tacotron 2, called Non-Attentive Tacotron. Instead of an attention mechanism, a duration predictor is utilized to improve robustness, which is evaluated by two metrics, unaligned duration ratio (UDR) and word deletion rate(WDR). The authors propose semi-supervised and unsupervised duration modeling with a fine-grained variational auto-encoder (FVAE).""453\tThis paper presents Non-Attentive Tacotron, a text-to-speech model based on the Tacotron 2 model that replaces the attention mechanism with an explicit duration predictor to improve robustness and naturalness. The model is trained using a pre-trained speech recognition model and achieving good results in terms of both metrics, including unaligned duration ratio and word deletion rate. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, the model is trained using a fine-grained variational auto-encoder in a semi-supervised or unsupervised manner, with results almost as good as supervised training.\tThis paper presents an approach based on the Tacotron model for speech synthesis, where the attention mechanism is replaced by a duration predictor. It also presents a short study on semi-supervised and unsupervised training. The paper also introduces two metrics to evaluate the robustness of the model. The experiments shows that the proposed model is on par with the Tacotron baselines in terms on MOS score and better in terms of the new metrics.""454\tThis paper provides an overview of the importance of layout estimation in robotics and the use of sensors such as LiDAR and cameras for generating bird's-eye view (BEV) layouts. The paper discusses the challenges of using LiDAR and Cameras for layout estimation, and suggests that LiDAR is the best choice for high-quality, high-scan-rate data, while cameras can be used for low-cost, passive sensors. The paper also explains the use of BEV layouts in autonomous driving cars and the benefits of having a semantic occupancy map of the scene.\tThis paper presents SBEVNet, a neural network architecture to estimate the bird's-eye view (BEV) layout of an urban driving scene. Given an image captured by a stereo camera, SBEVNet performs an inverse perspective mapping (IPM) to obtain an initial feature volume, which is further processed to generate the BEV layout. The system is trained end-to-end in a supervised learning setup.""455\tThis paper discusses the importance of layout estimation in robotics and the use of sensors such as LiDAR and cameras for generating bird's-eye view (BEV) layouts. The task of generating a BEV layout is important for planning the path of a robot in autonomous driving cars and other applications, as it allows for the estimation of object presence and the creation of semantic occupancy maps. LiDAR is used to generate point clouds that are then used to create the BEV layout, while cameras are used for additional information such as depth information. The paper\u8ba8\u8bba\u4e86LiDAR\u548ccamera\u8fd9\u4e24\u79cd\u4f20\u611f\u5668\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8bf4\u660e\u4e86\u5728 robotics \u5e94\u7528\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u7684\u573a\u666f\u3002\tThe paper proposes an end-to-end network for layout estimation from stereo images. The approach is built off previous stereo matching networks, which built and process a 3D disparity volume. The stereo estimate is used to project image features into a birds-eye-view representation which is processed using a U-net which predicts a semantic scene layout. The approach is evaluated on the KITTI and Carla generated datasets.""456\tThis paper discusses the importance of layout estimation in robotics and the use of sensors such as LiDAR and cameras for generating a bird \u2019 s eye view (BEV) layout. The BEV layout is a semantic occupancy map containing per pixel class information that is important for planning the path of a robot in autonomous driving cars. The paper\u8ba8\u8bba\u4e86LiDAR\u548ccamera\u8fd9\u4e24\u4e2a\u4f20\u611f\u5668\u5728\u751f\u6210BEV layout\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528LiDAR\u548c\u6444\u50cf\u5934\u7684\u66ff\u4ee3\u65b9\u6848\u3002\tThe paper proposed to estimate the semantic layout in the bird eye's view from a pair of stereo images. The main novelty/contribution lies in how to organize and exploit the information from the stereo images. The proposed framework builds upon inverse perspective mapping, and projected stereo feature volume. The performance was evaluated on the KITTI and CARLA datasets. Given a pair of stereo images, there are various options to exploit the image information, where this paper provides a framework by exploiting the stereo information in the bird eye's view. ""457\tThis paper discusses the problem of training long-range patterns in multi-relational graphs using graph neural networks (GNNs). GNNs are a type of neural network that are well-suited for handling large-scale graph data, but they have been shown to suffer from decreased performance as the number of layers is increased. Zhao & Akoglu (2020) point out that this can be due to over-fitting, vanishing gradients, and over-smoothing, which can lead to over-fitting, over-squashing, and possible vanishing gradient problems.\n\nSeveral works have investigated the problem of over-fitting in GNNs, including Vashishth et al. (2020), where they propose a new architecture that is less prone to over-fitting. Over-smoothing is also a known issue in GNNs, and Li et al. (2018) and Chen et al. (2019) investigate ways to address this issue by introducingLayer Normalization, which helps to prevent over-smoothing.\n\nAnother approach to address over-fitting in GNNs is to use over-squash problems, where Alon & Yahav (2020) propose a method that uses negative samples to prevent over-fitting. Over-vanishing gradient problems can be addressed by using deep learning techniques such as ReLU (Rectified Linear Unit) ortanh.\n\nOne simple but effective solution to these issues is to use a small number of layers, which can help to prevent over-fitting and improve performance. Li et al. (2019a) and Rahimi et al. (2018) show that using a smaller number of layers can lead to improved performance over a larger number of layers.\n\nOverall, this paper provides an overview of the issues that arise when training GNNs on multi-relational graphs and suggests some effective solutions to these issues.\t    The authors propose a new gating based recurrent graph attention networks for multi-relational graphs to capture long-range neighbor dependencies. The authors provide an interesting analysis of current gated GNN models (in the appendix + Figure 3) in light of their ability to capture long-range dependencies in graphs. Experimental results are reported for node classification with two synthetic datasets and two real-world datasets.  ""458\tThis paper focuses on learning long-range patterns in multi-relational graphs using graph neural networks (GNNs). GNNs are a type of neural network that are particularly well-suited for learning complex relationships between nodes in a graph. However, as with any deep learning model, GNNs have limitations when it comes to training with increasing number of layers. Zhao & Akoglu (2020)\u6307\u51fa\uff0c\u5f53\u5c42\u6570\u589e\u52a0\u65f6\uff0cGNN\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u53ef\u80fd\u539f\u56e0\u5305\u62ec\u8fc7\u62df\u5408\u3001vanishing\u68af\u5ea6\u548c\u8fc7\u62df\u5408\u3002\u8fd1\u5e74\u6765\uff0c\u8bb8\u591a\u7814\u7a76\u63a2\u8ba8\u4e86GNN\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5305\u62ec\u8fc7\u62df\u5408( Vashishth et al.,2020)\u3001\u8fc7\u62df\u5408\u548c\u538b\u7f29(Li et al.,2019a)\u3001\u4ee5\u53ca\u53ef\u80fd\u7684\u8fc7\u62df\u5408(Li et al.,2020,Rong et al.,2019,Yang et al.,2020)\u3002\u4e00\u4e9b\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e9b\u7b80\u5355\u4f46\u6709\u6548\u7684\u63aa\u65bd\u6765\u907f\u514dGNN\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4f8b\u5982\u51cf\u5c0f\u5b66\u4e60\u7387(Li et al.,2019b)\u3002\n\n\u6700\u540e\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3GNN\u5728\u8bad\u7ec3\u65f6\u9047\u5230\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5373\u5728\u4fdd\u7559\u957f\u8ddd\u79bb\u5173\u7cfb\u7684\u540c\u65f6\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002\tThis paper proposes a new GNN model (GR-GAT) for multi-relational graphs. The proposed method has better ability of capturing the long-range information. Essentially, the proposed GR-GAT is modified from GAT so that it can apply to the multi-relational graphs. Since the modifications are common and frequently used techniques, the novelty of this work is not enough. Also, why these modifications can help to capture long-range information is not well explained in this paper. Overall, this work is ok but not good enough for ICLR.""459\tThis paper focuses on learning long-range patterns in multi-relational graphs using graph neural networks (GNNs). GNNs are a type of neural network that are particularly well-suited for handling large-scale, complex graph data. However, as the number of layers in GNNs increases, they can suffer from over-fitting,vanishing gradients, and over-smoothing, which can lead to poor performance. Zhao & Akoglu (2020) provide a thorough analysis of these issues and suggest that they are caused by (1) over-fitting, (2) vanishing gradients, and (3) over-smoothing. several works have also investigate over-fitting, over-smoothing, and over-squashing in GNNs (Vshishth et al., 2020; Li et al., 2018; Chen et al., 2019; Zhao & Akoglu, 2020; Rong et al., 2019; Yang et al., 2020). One simple but effective solution to these issues is to use\u591a\u5c42 perceptrons (MLPs), which can provide better performance without the issues of over-fitting andvanishing gradients. Additionally, recent works have also shown that by using a combination of GNNs and MLPs, we can achieve state-of-the-art performance on a wide range of graph neural network (GNN) tasks (Rong et al., 2019; Rahimi et al., 2018).\tThis paper presents a graph attention architecture that captures long-range interactions. The novelties in the architectures are (1) vector-based parameterization of edge type in modeling message, (2) slight modification of graph attention (Section 3.2), and (3) GRU-based node update function. The experiments are primarily on synthetic tasks. However, it is unclear if modeling such long-range interaction is useful in real tasks. The paper fails to demonstrate convincing results on the real tasks of entity classification in knowledge graphs.""460\tThis paper discusses the use ofSatisfiability Modulo Theory (SMT) solvers formodel explanation, specifically how they can be used to verify the properties of large-scale deep neural networks. The paper explains that current SMT solvers are difficult to scale to large networks and that model explanation is a specific domain where they have been used but are limited to very small sizes. The paper also presents a new application of SMT solvers for explainable neural network decisions, which uses gradient information to identify the minimal set of features that are critical to the model's prediction. This work addresses the issue of scalability of SMT solvers and enables the use of these tools for different applications.\tThis paper provides an interesting pos-hoc explanation method to identify relevant features in an input that may inform a trained neural model's prediction. The task is to identify a binary mask over input image/text such that the masked input yields almost similar prediction as original input. The author formulates this as an SMT solver task, but instead of making sure that the output prediction is similar (which involve multiple time consuming pass over potentially huge networks), they make sure that high influential neurons in first layer of the network are still activated. This provides a less time consuming way to evaluate invariance of masked input.""461\tThis paper discusses the use of satisfiability modulo theory (SMT) solvers to explain decisions made by deep neural networks. The paper\u5148\u4ecb\u7ecd\u4e86SMTsolver\u7684\u57fa\u672c\u6982\u5ff5\u548c\u529f\u80fd\uff0c\u7136\u540e\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7SMT\u6c42\u89e3\u5668\u6765\u9a8c\u8bc1\u548c\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u8bb8\u591a\u9886\u57df\uff0c\u4f8b\u5982\u8f6f\u4ef6\u9a8c\u8bc1\u548c\u6a21\u578b\u89e3\u91ca\uff0c\u4f46\u73b0\u6709\u7684SMT\u6c42\u89e3\u5668\u5f88\u96be\u5904\u7406\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u4f5c\u8005\u63d0\u51fa\u4e86\u4f7f\u7528\u68af\u5ea6\u4fe1\u606f\u6765\u63d0\u9ad8SMT\u6c42\u89e3\u5668\u7684 scalability\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u7684\u6c42\u89e3\u3002\u6700\u540e\uff0c\u8be5\u4f5c\u8005\u4ecb\u7ecd\u4e86\u4ed6\u4eec\u7684\u7814\u7a76\u6210\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\tThis paper addresses the question of identifying which input features are most important for a neural network's decision. To do so, it frames the problem as an SMT problem that seeks to select the best input features, without changing the state of the first layer too much. The paper shows experiments, primarily on image classification, and also examples of how the approach may be applied to text classification.""462\tThis paper discusses the use of satisfiability modulo theory (SMT) solvers to explain neural network decisions. The paper begins by introducing the basic concepts of SMT and deep neural networks. It then presents an example of how SMT solvers can be used to explain neural network decisions, and discusses the challenges and opportunities that arise when using this technology. The paper\u6700\u540e presents a solution to the scalability problem with SMT solvers and discusses how this solution can be applied to other problems in the field of machine learning.\tThis paper presents a method to encode the minimal input feature discovery problem -- finding the minimal set of features in a input that is necessary for a prediction -- into a form that can is amenable to satisfiability modulo theory (SMT) solvers.  In particular they first use the integrated gradients methods to score first-layer neurons on the degree to which they influence the prediction.  Then, they produce and solve an SMT problem that finds the minimal mask that changes these influential neurons.  They demonstrate their approach on several problems.""463\tThe paper discusses the increasing success of deep convolutional neural networks (DCNNs) in performing 3D pose estimation, but\u6307\u51fa\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u8003\u8651 partial occlusion \u548c\u4ece\u4ece\u672a\u89c1\u8fc7\u7684 pose \u8003\u8651\u65f6\u4e0d\u591f robust\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5728 real-world \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4ea7\u751f\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u88ab\u5b66\u672f\u754c\u89e3\u51b3\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e keypoint \u70b9\u7684\u65b9\u6cd5\u6216\u4e8c\u5206\u7c7b\u65b9\u6cd5\u3002\tThis paper tackles the task of pose prediction and takes a render-and-compare approach. However, instead of rendering pixel colors, the key insight is to render features -- each mesh vertex is associated with 3D (learned) features which are encouraged to match computed 2D image features. This 'neural mesh' representation allows pose inference via SGD as one can optimize for pose s.t. the rendered features best match the image features, and is also robust to foreground occlusion. The paper demonstrates results on ObjectNet3d and PASCAL3D+ where the proposed approach is shown to be more robust to occlusion and also better at precise pose estimation.""464\tThe paper discusses the issue of robustness in 3D pose estimation, which is a fundamental problem in computer vision with realworld applications. The authors present an experiment that shows that current approaches to object pose estimation are not robust to partial occlusion and when objects are viewed from a previously unseen pose. The lack of robustness can have serious consequences in real-world applications and needs to be addressed by the research community. The paper proposes that keypoint-based approaches, which detect a sparse set of keypoints and subsequently align a 3D object representation to the corresponding points, are more robust to these types of challenges.\tThe paper presents a novel approach for 3d pose estimation by combining render-and-compare (analysis-by-synthesis) and contrastive feature learning. The key idea is to render and compare learned latent features instead of synthesized RGB colors to optimize 6D pose parameters. The proposed method learns latent feature vectors on a template mesh as well as target images via backbone neural networks such that matched regions have similar features while latent features are as distinctive as possible. The paper evaluates the novel formulation on  PASCAL3D+, the occluded PASCAL3D+, and ObjectNet3D dataset, demonstrating the render-and compare optimization with the proposed approach is more robust to appearance change and partial occlusions.""465\tThe paper discusses the recent advances in 3D pose estimation in computer vision and their limitations. It explains that the performance of current methods is improved by the use of deep convolutional neural networks, but they are not robust to partial occlusion and when objects are viewed from a previously unseen pose. The paper suggests that this lack of robustness can have serious consequences in real-world applications and needs to be addressed by the research community. The paper also discusses two main approaches for 3D pose estimation: keypoint-based and edge-based methods.\tThe authors propose a novel 3D neural mesh model of objects that is generative. They demonstrate that standard deep learning approaches to 3D pose estimation are highly sensitive to partial occlusion. Since their method works in a render and compare manner, it enables the method to be more robust to artifacts in general and partial occlusion in particular. They also achieve a highly competitive 3D pose estimation performance on popular dataset. They go on to show that even very crude prototypical approximation of the object geometry using a cuboid. ""466\tThis paper discusses the need for feature compatible learning (FCL) in large-scale retrieval-based applications, where updating the entire library of embedding vectors is expensive. The paper also explains that existing approaches for FCL mostly rely on old training data and classifiers, which are not available in many industry settings. The paper presents an approach for FCL that does not inherit the old classifier and training data, but instead requires only features extracted by the old model's backbone and new training data. The paper also proposes a unified framework for FCL that can handle the case where the old model is a black-box. The paper concludes by discussing the potential benefits and limitations of FCL.\tThis paper addresses an interesting problem in retrieval system - compatible features learning. Given the old feature extractor and a new dataset, the objective is to learn a new feature extractor, so that the features extracted by two (old and new) feature extractors are comparable to each other. In the proposed setting, the old dataset (including its statistics), old classifier, and the parameters of the old model are not available.""467\tThis paper discusses the need for Feature Compatible Learning (FCL) in large-scale retrieval-based applications, where updating the entire library of embedding vectors is expensive. The paper\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e0d\u9700\u8981\u7ee7\u627f\u65e7Classifier\u548c\u8bad\u7ec3\u6570\u636e\u7684Feature Compatible Learning\u65b9\u6cd5\uff0c\u79f0\u4e3aNon-Inherent Feature Compatible Learning\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u8981\u63d0\u53d6\u65e7\u6a21\u578b backbone \u548c\u65b0\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u5047\u8bbe\u65e7\u6570\u636e\u548c\u65b0\u6570\u636e\u4e4b\u95f4\u7684\u91cd\u53e0\u7a0b\u5ea6\u3002\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684FCL\u6846\u67b6\uff0c\u5e76\u53ef\u4ee5\u5904\u7406\u65e7\u6a21\u578b\u662f\u9ed1\u76d2\u7684\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u5728\u63d0\u5347 embedding \u6a21\u578b\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u8fdb\u5c55\u3002\tThis paper deals with an interesting problem of feature compatible learning that the features produced by new model should be compatible with old features. The proposed method uses nearest class\u2013mean classifier instead of linear classifier. Random walk is applied to refine the class means. The proposed method is compared with several baseline methods and shows good performance.""468\tThis paper discusses the need for Feature Compatible Learning (FCL) in large-scale retrieval-based applications, where updating the entire library of embedding vectors is expensive. The paper\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e0d\u9700\u8981\u7ee7\u627f\u65e7Classifier\u548c\u8bad\u7ec3\u6570\u636e\u7684Feature Compatible Learning\u65b9\u6cd5\uff0c\u5373Non-Inherent Feature Compatible Learning\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u8981\u65e7\u6a21\u578b backbone \u548c\u65b0\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u5047\u8bbe\u65e7\u6570\u636e\u548c\u65b0\u6570\u636e\u4e4b\u95f4\u7684\u91cd\u53e0\u7a0b\u5ea6\u3002\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684FCL\u6846\u67b6\uff0c\u5e76\u80fd\u591f\u5904\u7406\u65e7\u6a21\u578b\u662f\u9ed1\u76d2\u5b50\u7684\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u5728FCL\u9886\u57df\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u5728\u8bb8\u591a\u5de5\u4e1a\u73af\u5883\u4e2d\u4ecd\u7136\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\tThis work proposes a new problem setting by adding extra constraints to the Feature Compatible Learning problem. The new constraints avoid using old training data and the old model\u2019s parameter when learning a new model. The paper gives a baseline method and its variants for the problem by generating pseudo classifiers to regularize a new model\u2019s learning. The experiments show that the proposed method can satisfy the empirical criterion about success.""469\tThis paper discusses the generalization performance of deep learning through stochastic gradient descent-based optimization. It initializes the analysis from the learning dynamics perspectives and extends to the upper bound estimation of generalization errors. Research has also shifted focus to provide some theoretical or empirical measures on generalization performance, such as model selection methods, with respect to deep architectures, hyper-parameters, data distributions, and learning dynamics. This work studies the use of generalization performance measures for model selection and examines the performance of different models in terms of their generalization ability.\tThis paper studies the gradient norm as a measure of generalization in deep learning. The authors first an approximation to the gradient norm (GN) that is the norm of the gradients for only fully connected layers (AGN). Then they empirically evaluate the correlation between AGN and GN as well as GN and the generalization error. In Section 2.1, the authors conclude that AGN is highly correlated with GN and both are correlated with generalization error. In Section 3, the authors conclude that the correlation between AGN and generalization error is not consistent in a wider family of models. In Section 4, authors propose to use AGN for model selection and conclude that AGN is not good for model selection unless the hyperparameter for mixing AGN with another metric is optimal.""470\tThis paper discusses the analysis and evaluation of the generalization performance of deep learning models through stochastic gradient descent-based optimization. The focus of the paper is on providing theoretical or empirical measures of generalization performance, with a particular emphasis on model selection. The paper also explores the use of these measures for model selection in deep learning.\tThe paper empirically investigates the sum of gradient norms as a measure to determine the generalization abilities of a neural network. The approach is inspired by the theoretical work of Li, et al. 2020 which showed that the generalization gap can be upper bounded by a function of the sum of the full gradient norms of the training path. ""471\tThis paper discusses the generalization performance of deep learning through stochastic gradient descent-based optimization. It initializes the analysis from the learning dynamics perspectives and extends to the upper bound estimation of generalization errors. Additionally, it provides some theoretical or empirical measures on generalization performance with respect to deep architectures, hyper-parameters, data distributions, and learning dynamics. This work studies the use of generalization performance measures for model selection and discusses the potential implications of using these measures in deep learning.\tIn this paper, they provide the empirical studies  to understand the effectiveness and efficiency of the use of the gradient norm (induced by [the Li et al., 2020]) as the model selection criterion. To speed up the calculation process the of the gradient norm, they first propose an approximate gradient norm (AGN) based on the depth-wise, sample-wise and epoch-wise accelerations.  Their empirical studies find that the use of AGN can select the models with lower generalization error, but fails for bandit-based or population-based algorithms, and fails to predict the generalization performance of models based on different architectures.  In conclusion, they do not recommend using (approxiamte) gradient norm for model selection in practice.""472\tThis paper provides an overview of the current state of entity retrieval, which is a fundamental component of many applications, including commercial recommendation systems, chat-bots, and question answering systems. It discusses the challenges and opportunities in this field, including the need for efficient and accurate entity detection and disambiguation, as well as the use of various retrieval algorithms and techniques. The paper also provides an overview of recent advances and challenges in this area, and suggests future directions for research in the field.\tThe paper introduces a new method to retrieve entity by auto regressively generating unique entity name as a sequence of word pieces, instead of pinpointing the ID representing an entity. This method stands out in novelty compared to existing various entity retrieval methods, which always assigns a single ID to each entity. Practically, the proposed method has two nice properties: (1) When the entity vocabulary is very large, this approach requires less parameter space and memory compared to other methods (as shown clearly in Table 4) (2) The model can address novel entities, which was unseen during the training. The paper is clearly written and extensively evaluated on three relevant tasks, entity disambiguation, entity linking, and entity retrieval.""473\tThis paper discusses the importance of entity retrieval in natural language processing and its various applications. It presents a review of the existing literature on the topic, highlighting the challenges and recent advances in the field. The paper discusses the use of entity retrieval in commercial recommendation systems, chat-bots, and question answering systems, as well as in other applications such as natural language understanding and data mining. The paper also highlights the importance of using a multi-modal approach to entity retrieval, including both visual and\u542c\u89c9 evidence. The paper\u6700\u540e\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411 and recommendations for the field.\tThis paper proposes to tackle the entity linking task using a sequence-to-sequence neural model, trained by producing unique entity names, in autoregressive fashion. The paper makes a case that this approach can scale better with larger entity vocabularies than previous methods with dedicated entity representations both in terms of memory as well as computation costs. The model is studied under a number of tasks including entity disambiguation, entity linking and document retrieval for question answering.""474\tThis paper provides an overview of the entity retrieval problem, which is the process of finding the correct entity from large Knowledge Bases (KBs) given a textual input. The paper explores the various challenges and approaches that are faced in this task, including the need for robust machine learning algorithms and a well-defined dataset. The paper also highlights the importance of incorporating context information into the retrieval process, such as the relationships between entities, to improve the accuracy of the retrieved entity. The paper\u6700\u540e provides a practical example of how entity retrieval can be used in real-world applications, such as in chat-bots and question answering systems.\tThe paper proposed to use autoregressive approach to solve entity-based problems. They proposed a uniform framework and showed that their model achieved the state of the art performance on 3 different types of tasks (~20 datasets). The GENRE model also significantly reduced the memory usage compared to previous models that stored a big memory table. It's also capable of linking novel entities at inference time. This paper is clearly written. The experiment results are convincing.""475\tThe paper discusses a routing algorithm that uses adversarial routing requests to estimate the total cost of a path over a given time horizon. The algorithm observes the congestion function on the road network and uses this information to make routing decisions. The paper presents an algorithm with cumulative regret \u00d5 ( |E|t2/3 ), where the regret on each time step is the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. The algorithm has space complexity O ( |E|t1/3 ) and time complexity O ( |E| log t ), and empirical validation is also provided using graphs from New York City road networks. The algorithm is important because it can help routing applications be more effective in managing large scale mobility solutions.\tIn this submission a routing problem is studied. In the considered model with each edge of the given graph a congestion function is associated that specifies the congestion depending on the current load of the edge. Then cars have to be routed through the network where each car has a source and a destination and one aims at choosing a path from the source to the destination with the smallest total congestion. However, the congestion functions of the edges are a priori unknown and hence one cannot trivially use a shortest path algorithm. Instead one gains information about the congestion functions only by routing the cars. When a car is routed one observes for each edge on its path the current congestion up to some random additive term. These observations can then be used for future routing decisions.""476\tThis paper presents an algorithm that can routing decisions in a navigation application with cumulative regret \u00d5 ( |E|t2/3 ) . The algorithm observes the congestion function cte of the road network and uses this information to make routing decisions. The routing requests are supplied adversarially, meaning that the algorithm is given random samples from the congestion function to make decisions on future routing paths. The algorithm has space complexity O ( |E|t1/3 ) and time complexity O ( |E| log t ) , and empirical data validate its performance on a real-world road network. The algorithm is designed to be efficient in terms of both space and time and can be used in a variety of navigation applications.\tThis work introduces an interesting generalization of stochastic combinatorial semi-bandits for routing in a static graph. The main differences are: (1) the expected loss of an edge e is f_e(x^t_e) where the flow x^t_e is revealed at the beginning of each round (for each edge) and f_e is an unknown Lipschitz function (with known Lipschitz constant); (2) the regret is dynamic, computed against the sequence of optimal paths. When f_e is a constant function for each edge, then we recover a version of the stochastic combinatorial semi-bandit.""477\tThe paper discusses an algorithm for routing paths in a road network that has a cumulative regret of \u00d5 ( |E|t2/3 ), where the regret on each time step is the difference between the total cost incurred by our chosen path and the minimum cost among all valid paths. The algorithm works by observing the congestion function on each edge of the road network and using this information in future routing decisions. The paper also discusses the space complexity and time complexity of the algorithm, which are O ( |E|t1/3 ) and O ( |E| log t ), respectively. The algorithm is empirically validated using graphs from New York City road networks.\tThe paper uses the bandit learning framework to study the online learning problem for routing in a city network . After each routing decision, the learning agent observes the actual delay on each edge, which is given by the congestion function on the given flow plus a random noise, and the reward is the total delay on all edges. The paper proposes a learning algorithm similar to the UCB approach, provide the regret bound result, and conduct simulations on the New York City network to verify performance of the algorithm. ""478\tThis paper discusses a common problem with the pretraining of Masked Language Models (MLMs, such as BERT) - Masked tokens are randomly masks and allow the MLM to minimize its training objective by latching onto shallow local signals. This leads to pretraining efficiency but also suboptimal downstream performance. To address this problem, the paper proposes PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI). PMI-Masking jointly masks a token n-gram if it exhibits high collocation over the corpus, motivates, improves upon, and unified previous more heuristic approaches to Masked Language Model pretraining, such as whole-word masking, entity/phrase masking, and random-span masking. The paper also shows that PMI-Masking reaches the performance of prior Masking approaches in half the training time and consistently improves performance at the end of training.\tThis paper proposes an improvement to how tokens are selected for masking in pre-training large masked language models (BERT and family). Specifically, it stipulates that purely random choice of words (or word pieces) makes the MLM task insufficiently hard. It then goes on to propose a data-driven approach for selecting n-grams to mask together. The approach, based on an extension of pointwise mutual information for n-grams, is shown to outperform random token and random spans masking strategies on performance of downstream tasks.""479\tThis paper discusses a common problem with the pretraining of Masked Language Models (MLMs, such as BERT) - uniform token masking. It shows that uniform maskering allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining efficiency and suboptimal downstream performance. To address this problem, the paper proposes PMI-Masking, a principled maskering strategy based on the concept of Pointwise Mutual Information (PMI). PMI-Masking motivates\uff0c\u56e2\u7ed3\u5e76\u6539\u8fdb\u4e86\u5148\u524d\u66f4 heuristic approaches\uff0c\u5982 whole-word masking\u3001entity/phrase Masking \u548c\u968f\u673a-span Masking\uff0c\u65e8\u5728\u89e3\u51b3 uniform maskering \u5f15\u8d77\u7684 drawbacks \uff0c\u5982 BERT \u7b49 MLM \u7684 pretraining \u6548\u7387\u4e0d\u4f73\u548c\u4e0b\u6e38\u6027\u80fd\u4e0d\u8db3\u3002\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0cPMI-Masking \u5728 half \u7684\u65f6\u95f4\u5185\u8fbe\u5230\u4e86\u5148\u524d maskering \u65b9\u6cd5\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5e76\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\u3002\tThis paper presents a masking strategy for training masked language models (MLM). The proposed strategy builds on previous approaches that mask semantically coherent spans of tokens (such as entire words, named entities, or spans) rather than randomly masking individual tokens. Specifically, the proposed method computes the PMI of spans (and the generalization for spans of size >2) over the pretraining corpus, and randomly masks from among the 800K spans (lengths 2-5) with the highest PMI. Masking based on PMI removes the ability for the model to rely on highly local signals to fill in the mask and instead focus on learning higher level semantics. They motivate this hypothesis with an experiment demonstrating that as the size of the WordPiece vocabulary decreases (and words are more frequently split into multiple tokens rather than being their own token), the transfer performance of the resulting MLM decreases. However, using whole-word masking with this same vocabulary size recovers much of the original performance, indicating that allowing the model to rely on these strong local signals harms the transfer quality of the resulting model.""480\tThis paper discusses a common problem with the pretraining of Masked Language Models (MLMs, such as BERT) - namely, the use of uniform token masks that allow the MLM to minimize its training objective by latching onto shallow local signals. The paper proposes a principled masking strategy based on the concept of Pointwise Mutual Information (PMI) that jointly masks a token n-gram if it exhibits high collocation over the corpus, leading to improved performance in the pretraining and downstream tasks. The experimental results show that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training.\tThe paper proposes a variant on the MLM training objective which uses PMI in order to determine which spans to mask. The idea is related to recently-proposed Whole Word Masking and Entity-based masking, but the authors argue the PMI-based approach is more principled. The method is straightforward--it involves computing PMIs for ngrams (in this case, up to length 5) over the training corpus, and then preferring to mask entire collocational phrases rather than single words during training. The intuition is that masking single words allows models to exploit simple collocations, thus optimizing their training objective without learning longer-range dependencies or higher level semantic features of the sentences, and this makes training less efficient than it could be. One contribution of the paper is a variant on the PMI metric that performs better for longer phrases by reducing the scores of phrases that happen to contain high-PMI subphrases, e.g. \"George Washington is\" should not have a high score despite the fact that \"George Washington\" does have a high score.""481\tThis paper presents an overview ofVariational inference, a technique for learning deep latent-variable models (LVMs), which are a type of deep generative model. The paper covers the history of LVMs, the importance of variational inference, and the efficient amortized variant of variational inference. The paper also discusses the framework of variational auto-encoders, which adds the reparameterization trick for low-variance gradient estimates to the learning process. The paper\u6700\u540e provides a summary of the main contributions of the paper and\u5c55\u671b the future directions of LVMs and variational inference.\tThis paper investigates the effect of partial conditioning on amortized inference in variational auto-encoders, focusing specifically on sequential data sources where it is common practice to have a posterior that is factorized in such a way that conditioning is partial (usually only conditioning on past signals in the sequence). Given a true posterior that is conditioned on the entire observed datapoint, the authors discuss the effect of having an approximate posterior that is only conditioned on part of the input. As the approximate posterior cannot adapt to the part of the input that is left out of the conditioning, the evidence lower bound becomes less tight, due to the larger KL divergence between the approximate posterior and the true posterior. The authors compare this to the work by Cramer et al. [1], where the distinction was made between having a restricted family of possible distributions for the approximate posterior (approximation gap) and the gap between an amortized approximate posterior with an inference network shared for all datapoints and a non-amortized approximate posterior that is optimized for each datapoint separately (amortisation gap). They argue that partial conditioning leads to a third type of gap which is distinct of the aforementioned inference gaps. Through an example with discrete observations the authors derive that when the true posterior is conditioned on the full data, and the approximate posterior is only partially conditioned, the optimal approximate posterior is something akin to a product of true posteriors over the unconditioned information, and not a mixture where the left out information is marginalized out. Through a 1D example they show that this could lead to overly sharp posteriors that have high densities in regions where the true posterior has very low density. ""482\tThis paper discusses the use ofVariational inference to learn deep latent-variable models (LVMs), which are a type of generative model used to model complex relationships between variables. The paper also presents an efficient variant of variational inference, amortized variational inference, which produces the parameters of the variational distribution for each observation by a single forward pass, in contrast to classical variational inference with a full optimisation process per sample. The paper also discusses the framework of variational auto-encoders, which add the reparameterization trick for low-variance gradient estimates to make learning of LVMs more efficient and flexible. The paper concludes by discussing the applications of variational inference in machine learning, including the use of LVMs in natural language processing and data science.\tThe paper considers the problem of Bayesian inference with partially conditioned variational posterior. Namely, this work describes the phenomena of ill-behaved variational posterior for the case of partially observed data. The paper's main theoretical finding is that the partially conditioned variational posterior behaves like a product of experts, resulting in a degenerate solution. Speaking intuitively, the true posterior can be seen as a mixture of distributions: the sum over the unobservable variable. At the same time, the optimal variational posterior mixes as a product of distributions. Clearly, the product of densities hardly depicts features of the mixture since a near-zero value of a single member is enough for zeroing out the product's density.""483\tThis paper discusses the use ofVariational inference (VI) to learn deep latent-variable models (LVMs). VI is a powerful tool for approximate learning, where the true distribution of the data is not known apriori. By using VI, we can learn the parameters of an LVM, which can then be used to generate new data or perform other tasks. In this paper, we discuss the principles of VI, the efficiency of amortised VI, and the use of deep neural networks in VI. We also discuss the limitations of VI and how it can be used in practice.\tThe paper reviews the issue of partial conditioning of the amortized posterior in sequential latent variable models, typically state-space models trained with a VAE-style loss, but where the posterior used is the filtering rather than smoothing posterior. The author show that training a model with posterior with missing information can lead to a gap in estimating both the posterior and the corresponding model. They show the benefits of using the correct posteriors in simple examples.""484\tThis paper provides an overview ofKernel Ridge Regression (KRR), a popular nonparametric learning method that has excellent theoretical guarantees but has poor scalability in large-scale settings. The paper discusses several scalability-based approaches that have been proposed to address this issue, including distributed learning, random features, and Nystr\u00f6m methods. The paper also discusses the challenges and limitations of KRR, such as its computational complexity and the need for large training datasets. Finally, the paper concludes by highlighting the potential of KRR in the context of large-scale machine learning and proposes future directions for research.\tThe paper analyses generalization properties of distributed kernel ridge regression (DKRR) with random features and communications. It studies optimal learning rates of the generalization bounds both in expectation and in probability. In the case of DKRR with random features, the optimal learning rate in expectation is shown to achieve by relaxing the requirement on the number of partitions from $O(1)$ (Li et al., 2019a) to $O(|D|^{0.5})$ (Theorem 1). Within the same setup of random features, the number of partitions is relaxed to $O(|D|^{0.25})$ guaranteeing optimal generalization performance in probability (Theorem 2). The latter bound $O(|D|^{0.25})$ on partition count is much smaller then $O(|D|^{0.5})$. However, as proved in Theorem 3, allowing multiple communication rounds in DKRR-RF, up to $O(|D|^{0.5})$ partitions can be handled depending on the number of communication rounds. In other words, it can exploit more partitions at the cost of more communication rounds.""485\tThis paper discusses a popular nonparametric learning method called Kernel Ridge regression (KRR), which is used to fit a function to a dataset by finding a mathematical Ridge in the function. KRR has theoretical guarantees of high accuracy and does not require any prior knowledge of the function being fit. However, KRR has poor scalability in large-scale settings, due to its high time and memory complexities. The paper presents a variety of techniques, including distributed learning, random features, and Nystr\u00f6m methods, that can address the scalability issues and improve the performance of KRR in large-scale settings. The paper also discusses the challenges and limitations of using KRR in practice and provides some insights into the potential applications of KRR.\tThe paper investigates an algorithm for distributed learning with random Fourier features. The main idea is to sample M random Fourier features and split the data into m chunks. Each chunk is processed on a separate machine that outputs a linear hypothesis using the sampled M random features. The hypotheses coming from different machines are then aggregated on the master machine via importance weighting. In particular, each hypothesis is assigned importance weight proportional to its data chunk size (see Eq. 3). The regularization parameter is fixed across different machines. The main contribution of the work is a consistency bound. In comparison to a previous bound on the divide & conquer algorithm (Li et al., arXiv 2019), this one does not require a constant number of machines (in my understanding of the related work section).""486\tThis paper provides an overview of kernel ridge regression (KRR), a popular nonparametric learning method that has excellent theoretical guarantees but is not well-suited for large-scale settings due to its high time and memory complexities. The paper discusses several recent advances in addressing the scalability issues of KRR, including distributed learning, random features, and Nystr\u00f6m methods. The paper also presents an analysis of the performance of these methods on a range of data sets and discusses their potential applications in fields such as finance and health care.\tThis paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting in the attainable cases.  Numerical results are given for the studied new algorithms. The algorithms and the derived results are new and interesting to me. However, the presentations as well as the citations need some major revision before the publication. ""487\tThis paper provides an overview of neural architecture search (NAS), a technique used to discover novel deep neural network (DNN) architectures in complex search spaces and outperform human-crafted designs. Early NAS works like NASNet and AmoebaNet used reinforcement learning or evolutionary algorithms to search for DNN architectures by training a substantial amount of independent network architectures from scratch. However, these methods had significant computation and time costs, and researchers gradually shifted their focus to one-shot NAS, which is more efficient and can deliver satisfying outputs within a few GPU-days. There are two main types of one-shot NAS: differentiable NAS (DNAS) and continuous NAS (CNAS). DNAS uses a continuous relaxation of the search space to reduce the number of training examples required to find a satisfying solution, while CNAS uses a binary relaxation of the search space to find architectures that can perform at least as well as human-crafted designs within a shorter amount of time. These methods have been successfully applied to a range of applications, including image and speech recognition, reinforcement learning, and machine learning.\tMotivated by exploring the ranking correlations of the existing RandomNAS in NASBench-201, this paper proposes EPS to improve the search efficiency and keep good ranking correlations by evolving the proxy search space (PS) in RandomNAS. Specially, EPS contains three stages: 1) training the supernet in PS, 2) validating the architectures among the PS and 3) evolving the PS by tournament selection with the aging mechanism. Furthermore, a model-size-based regularization is introduced in the selection stage. Experiments on some popular benchmarks demonstrate the effectiveness of the method.""488\tThis paper provides an overview of neural architecture search (NAS), a machine learning technique that has been successfully used to discover novel DNN architectures in complex search spaces and outperform human-crafted designs. Early NAS works like NASNet and AmoebaNet used reinforcement learning or evolutionary algorithms to search for DNN architectures by training a substantial amount of independent network architectures from scratch. However, these search architectures came with tremendous computation and time costs, which led researchers to shift their focus to one-shot NAS, which is more efficient and can deliver satisfying outputs within a few GPU-days. There are two main types of one-shot NAS: differentiable NAS (DNAS) and continuous NAS (CNAS). DNAS uses a continuous relaxation of the search space to reduce the number of training examples required, while CNAS uses a continuous relaxation of the architecture space to search for architectures that are more flexible and can withstand various inputs. Both types ofNAS have been shown to outperform human-crafted designs in various tasks, and they are becoming increasingly popular in the field of deep learning.\tThis paper claims that random search-based NAS methods show a low ranking correlation among top-20% candidate architectures in the search phase. To address this issue, this paper proposes to introduce a proxy search space consisting of good architectures and evolve it using evolutionary algorithms. This paper also proposes a simple size regularization to help the NAS algorithm escape from the small architecture traps. The experimental results show that the proposed approach achieves competitive performance with baseline methods.""489\tThis paper discusses the use of neural architecture search (NAS) to discover novel DNN architectures and outperform human-crafted designs. It also explores the two main types of one-shotNAS: differentiableNAS(DNAS) and continuousNAS(CNAS). CNAS uses a continuous relaxation of the search space to search for architectures that have a lower energy than the current best architecture. DNAS uses a relaxation method that is based on the gradient of the energy function to search for architectures with a lower energy. Both types ofNAS are able to discover architectures that outperform human-crafted designs in complex search spaces. However, DNAS is more efficient and can deliver satisfying outputs within a few GPU-days, which is an advantage over CNAS. The paper also examines the challenges and limitations of usingNAS, including the computational and time costs, as well as the need for further research to optimize the performance ofNAS.\tThis paper proposes Evolving the Proxy Search Space (EPS) as a new RandomNAS-based approach. The goal is to find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. EPS runs in three stages iteratively: Training the supernet by randomly sampling from a PS; Validating the architectures among the PS on a subset of the validation dataset in the training interval; Evolving the PS by a tournament selection evolutionary algorithm with the aging mechanism.""490\tThis paper discusses the limitations of two popular approaches to teaching an agent a new task: Reinforcement Learning (RL) and Imitation Learning (IL). Both methods require a large amount of data to learn, but RL requires exploration while IL requires demonstrations. Meta-RL and meta-IL, which leverage a meta-training dataset, are promising solutions to this problem, but they have their own limitations. Meta-RL requires hand-crafted reward functions to describe each new task, which is tedious for nonexperts, and is limited by the similarity between the new task and the meta-training data. Meta-IL, on the other hand, is more natural and can not continue to improve the policy in the way that RL methods can, due to the similarity between the new task and the meta-training data.\tThis work seeks to efficiently learn new tasks by combining meta-RL and imitation learning (IL). Such a combination is a natural thing to try, as both lines of work improve sample complexity of learning a new task: meta-RL by leveraging experience on prior related tasks, and IL by leveraging demonstrations. Demonstrations also form a natural way of specifying a new task to the agent.""491\tThe paper discusses two popular approaches for teaching an agent a new task: Reinforcement Learning (RL) and Imitation Learning (IL). However, both methods require a large amount of data to learn, which is difficult to obtain for most agents. In recent years, meta-RL and meta-IL have emerged as promising solutions to this problem, by leveraging a meta-training dataset of tasks to learn representations which can quickly adapt to this new data. However, both these methods have their own limitations. Meta-RL typically requires hand-crafted, shaped reward functions to describe each new task, which is tedious for nonexperts. A more natural way to describe a task is to provide demonstrations, as with meta-IL, but after adaptation, these methods can not continue to improve the policy in the way that RL methods can, and are limited by the similarity between the new task and the meta-training data.\tThis work proposes PERIL, a method for combined Meta Imitation Learning and Meta Reinforcement Learning using context-based meta-learning. Given a set of demonstrations, a latent variable representing the desired task is inferred, and trajectories are generated conditioned on the inferred latent variable.  The data from the expert demonstrations and trajectories are used for meta-learning updates.""492\tThis paper discusses two popular approaches for teaching an agent a new task: Reinforcement Learning (RL) and Imitation Learning (IL). Both require a large amount of data to learn, but in their standard forms, RL and IL require different types of data to learn. In recent years, meta-RL and meta-IL have emerged as promising solutions to this problem by leveraging a meta-training dataset of tasks to learn representations which can quickly adapt to this new data. However, both these methods have their own limitations. Meta-RL typically requires hand-crafted, shaped reward functions to describe each new task, which is tedious for nonexperts. A more natural way to describe a task is to provide demonstrations, as with meta-IL. But after adaptation, these methods can not continue to improve the policy in the way that RL methods can, and are limited by the similarity between the new task and the meta-training data.\tThis paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. To this end, the authors combine ideas from metaRL (specifically from PEARL (Rakelly et al. 2019) and Humplik et al (2019)) where a set encoder is used to encode trajectories to a latent vector describing the task, with imitation learning techniques by (a) training this encoder also with demonstrations (b) initialising the latent vector at test time by feeding demonstrations through the encoder, and (c) having additional losses inspired by metaIL techniques. The motivation is that using demonstrations allows us to learn tasks that are difficult otherwise, for example because the rewards (at test time) are sparse. ""493\tThis paper discusses the issue of overfitting inConvolutional Neural Networks (CNNs) and the lack of generalization guarantee for models with too many parameters. The paper also explores the recent efforts to provide such guarantee for overparameterized CNNs. Despite the efforts, the current guarantee is still limited and more research is needed to better understand the underlying mechanism and provide a general guarantee for overparameterized CNNs.\tThis paper is concerned with the question of generalization of convolutional neural networks. For that, the authors study a simple toy model, where each data point consists of several patterns. All patterns are assumed to be orthogonal to each other. Those images should be learned with a 3-layer neural network. The contributions of this paper are as follows:""494\tThis paper discusses the problem of overfitting inConvolutional Neural Networks (CNNs) and how current generalization guarantee methods for CNNs do not address this issue. The paper also discusses the potential solutions to this problem, such as reducing the number of parameters or adding additional constraints to the network.\tThis paper studied a simplified image classification task with orthogonal non-overlapping patches and is learned by a 3-layer CNN. The authors observed pattern statics inductive bias (PSI) in experiments. They proved that if a learning algorithm satisfies PSI, the sample complexity is nearly quadratic in the filter dimension; while the VC dimension of the network is at least exponential in the filter dimension. The authors also verified PSI in some task based on MNIST that has non-orthogonal patches.""495\tThe paper focuses on the problem of overfitting inConvolutional Neural Networks (CNNs) and the current limitations of providing generalization guarantees for such models. The paper also discusses the recent efforts to address this issue, including Long & Sedghi (2020) and Li et al. (2018). The main finding of the paper is that overfitting is often caused by overpopulation of the training data, which can be addressed by using more advanced techniques such as data augmentation and early stopping. Despite these efforts, the current state-of-the-art generalization guarantee methods either rely on specific constraints on the weights or have limited applicability to overparameterized CNNs.\tIn this manuscript the authors derive theoretical analysis for the generalization guarantees of a na\u00efve CNN (3-layers) where the task is a simplified binary classification task, under the assumption that the images contain orthogonal patches (a na\u00efve assumption). They define a statistical phenomenon that holds in SGD in the proposed setting and call it Pattern Statistics Inductive Bias (PSI). Informally, this means that the magnitude of the dot-product between the learned pattern detectors and their detected patterns is correlated with the distribution of the patterns in the data.   They prove that if a learning algorithm  satisfies PSI then its sample complexity is O(d^2 log (d)), where d is the dimension of the filter. According to their empirical derivation SGD satisfies this property. In contrast there exist learning algorithms that have exponential sample complexity. ""496\tContrastive learning is a representation learning technique that uses natural occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, the paper presents a proof that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. The paper applies this procedure in a semi-supervised setup and empirically demonstrates that linear classifiers with these representations perform well in document classification tasks with very few training examples.\tThis submission considers contrastive learning approach to representation learning under topic modeling assumptions. It proves that the proposed procedure can recover a representation of documents that reveals their underlying topic posterior information in case of linear models. It is experimentally demonstrated that the proposed procedure performs well in a document classification task with very few training examples in a semi-supervised setting.""497\tContrastive learning is a type of representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. It does this by finding\u9690\u85cf\u7740\u9690\u85cf\u7740 information in the data by looking for patterns that are not evident on their own. In the context of document classification under topic modeling assumptions, this paper demonstrates that contrastive learning can recover a representation of documents that reveals their underlying topic posterior information to linear models. This representation can then be used to improve the performance of linear classifiers in document classification tasks with very few training examples.\tThis paper tries to learn a document level representation from document level contrastive estimation. The training task is try to predict where two half of a document are from the same document. The author proved the contrastive estimation reveals topic posterior information given the topic modeling assumptions. And in experiments, linear models can get relatively good performance. ""498\tThis paper presents a study on contrastive learning, a representation learning approach that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, the paper demonstrates that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. The paper also applies this procedure in a semi-supervised setup and empirically demonstrates that linear classifiers with these representations perform well in document classification tasks with very few training examples. 1.1 Introduction . Representation learning is a field of machine learning that aims to learn useful representations of data that can be used for tasks such as classification, regression, and natural language processing (NLP) (Reinforcement Learning, 2013). The use of embeddings, which are\u77ed\u6682\u7684\uff0c continuous, and highly informative mathematical objects that represent the relationships between data points, is a central idea in representation learning. Contrastive learning is an approach to representation learning that utilizes natural occurring similar and dissimilar pairs of data points to find useful embeddings of data (Bahdanau et al., 2015). Contrastive learning involves learning a representation that captures the important information in data by analyzing the relationships between data points and finding pairs that are similar or dissimilar. By utilizing these pairs, contrastive learning can learn more informative embeddings that can be used for tasks such as classification and regression. 1.2 background . Document classification is a type of NLP task that involves\u5206\u7c7b\u6587\u672c\u6587\u6863\uff0c\u5373\u5c06\u6587\u6863\u5206\u914d\u7ed9\u4e00\u4e2a\u7c7b\u522b\u3002\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u5206\u7c7b\u4efb\u52a1\u7684\u76ee\u6807\u662f\u8ba9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u6587\u6863\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u4fbf\u5c06\u6587\u6863\u5206\u914d\u7ed9\u6b63\u786e\u7684\u7c7b\u522b\u3002\u5728\u5206\u7c7b\u8fc7\u7a0b\u4e2d\uff0c\u901a\u5e38\u4f1a\u4f7f\u7528\u6587\u672c\u7279\u5f81\u6765\u63cf\u8ff0\u6587\u6863\uff0c\u4f8b\u5982\u6587\u672c\u7684\u5185\u5bb9\u3001\u7ed3\u6784\u3001\u8bed\u8a00\u98ce\u683c\u7b49\u3002 document classification \u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u4efb\u52a1\uff0c\u4e5f\u662f\u76ee\u524d\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u7814\u7a76\u7684\u70ed\u70b9\u4e4b\u4e00\u3002\u4f20\u7edf\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u7279\u5f81\u5de5\u7a0b\uff0c\u4f8b\u5982\u7279\u5f81\u63d0\u53d6\u3001\u7279\u5f81\u9009\u62e9\u548c\u7279\u5f81\u8f6c\u6362\u7b49\uff0c\u800c\u8fd1\u5e74\u6765\u7684\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u4e00\u4e9b\u672a\u88ab\u5145\u5206\u6316\u6398\u7684\u7279\u5f81\u53ef\u80fd\u4f1a\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002 topic modeling \u662f\u4e00\u79cd\u5e38\u7528\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5b83\u5047\u8bbe\u6587\u672c\u5185\u5bb9\u662f\u7531\u4e00\u7cfb\u5217\u76f8\u5173\u4e3b\u9898\u7ec4\u6210\u7684\uff0c\u6bcf\u4e2a\u4e3b\u9898\u4e4b\u95f4\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u5173\u7cfb\u3002 topic modeling \u5047\u8bbe\u6587\u672c\u5185\u5bb9\u53ef\u4ee5\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u5305\u542b\u6587\u672c\u7684\u6bcf\u4e2a\u4e3b\u9898\u548c\u8be5\u4e3b\u9898\u4e4b\u95f4\u7684\u4fe1\u606f\u3002\u5728 document classification \u4e2d\uff0ctopic modeling \u53ef\u4ee5\u7528\u4e8e\u6784\u5efa\u5206\u7c7b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5c06\u6587\u672c\u6587\u6863\u5206\u914d\u7ed9\u6b63\u786e\u7684\u7c7b\u522b\u3002 1.3 Conclusion . In this paper, we have presented a study on contrastive learning, a representation learning approach that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we have demonstrated that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We have also applied this procedure in a semi-supervised setup and empirically demonstrate that linear classifiers with these representations perform well in document classification tasks with very few training examples. Overall, our study suggests that contrastive learning can be a powerful tool for representation learning and document classification tasks.\tThis paper presents a new contrastive learning algorithm for document representation. The main idea is to generate pseudo labeled two texts, whether the texts are coming from the same document. To learn the discriminating function between the two texts, the learning algorithm minimizes the cross-entropy loss function between them. With the function, the authors suggest the embedding function for a document with selected landmark documents. The authors also show the learned function can be represented by combining the topic posterior distribution and topic likelihood distribution. Experiments show that the suggested learning algorithm can identify hidden topics from a synthetic dataset. And the authors also show the usefulness of the representation in semi-supervised learning by classification performance and visualization.""499\tThe paper discusses the importance of multimodal data in language understanding and the challenges of collecting and using it. It provides an overview of the current state of the art in language-vision systems and discusses some of the recent advances in using multiplemodal data to improve the performance of these systems. The paper also suggests some potential future directions for research in this area.\tThe paper tries to minimize the difference of the PMI between related and unrelated pairs of multimodal data but arrives at a very different objective with many approximations. It can be plugged into existing VAE based methods and improve learning performance and data efficiency. My major concern is about the derivation and the connection between motivation and the final objective.""500\tThis paper discusses the use of multiple sensory information from differentmodalities to describe concepts in the real world. The paper highlights the benefits of learning from multiple modalities, such as improving the generalization of representations, but also notes that the multimodal framework is not always data-efficient, especially in more complex settings like language-vision. The paper also discusses the challenges of constructing a suitable dataset for learning from multiplemodalities.\t The paper proposes a contrastive objective that (1) minimizes the distance between \"related\" samples while (2) maximizing the distance between randomly paired samples. Existing multimodal VAEs optimize (1) via different multimodal ELBOs. The novelty lies in the optimization of (2) which can further benefit from unimodal samples for which no \"related\" samples of the other modality are available---this can be viewed as a semi-supervised approach for weakly-supervised multimodal data.  For the estimation of (2), the paper experiments with two different estimators, IWAE and CUBO.""501\tThe paper discusses the use of multiple perception signals (modalities) to learn more general representations of concepts. It is shown that using complementary and overlappingmodalities can lead to more general representations that can be applied to new tasks. However, the paper also mentions that the multimodal framework is not always data-efficient, as it requires a lot ofannotated unimodal data to construct a suitable dataset. The paper also discusses the challenges of working with complex multimodal settings such as language-vision.\tThis work presents a generative model for multimodal learning. The paper maximizes or minimizes the pointwise mutual information between data from two modalities considering a novel random variable relatedness to dictates if data are related or not.  This is realized by casting multimodal learning as max-margin optimization with the contrastive loss for the objective. For the optimization, the paper considers the IWAE estimator. As per the experiments, the paper considers MNIST-SVHN and CUB Image-Captions dataset and perform evaluations across four metrics. Using the experiments, the paper demonstrates that the proposed approach improves multimodal learning, data-efficient learning, and label propagation. ""502\tThis paper proposes an EBM prior for Variational Autoencoders (VAEs), which is designed to bring the base prior (p(z)) closer to the aggregate posterior (q(z)) of the model. The prior is achieved by using the product of a reweighting factor (r(z)) and the base prior. The paper presents an analysis of the effectiveness of the proposed prior, and provides experimental results that demonstrate its ability to improve the expressivity of the posterior. The paper also discusses the potential applications of the proposed prior in various fields.\tThe authors highlight an important problem in VAE - the prior-hole problem - which is that the approximate posterior and the simple gaussian prior do not match in spite of the KL term in the ELBO which makes sampling an issue - leading to the prior putting probability mass on latents that are not decoded to high probability mass regions in data manifold. Prior approaches have overcome this problem by increasing the expressivity of the prior through autoregressive models, and/or using hierarchical latents, EBMs with MCMC sampling. This paper proposes a very simple two stage method - (1) train a regular VAE, (2) train a binary classifier in NCE style to distinguish samples from prior and approx. posterior; use the re-weighting term from the NCE score to sample from a better re-weighted prior - either through langevin dynamics or re-sampling. The authors combine this approach with the use of hierarchical latents and produce really good performing generative models on a host of benchmarks with good looking samples.""503\tThis paper proposes an EBM prior for Variational Autoencoders (VAEs), which is designed to bring the base prior closer to the aggregate posterior of the VAE. The prior is created by using the product of a base prior p ( z ) and a reweighting factor r ( z ), which is designed to balance the prior\\'s ability to capture the underlying distribution of the data and its ability to generate new data. The paper shows that the proposed prior can improve the expressivity of the VAE\\'s approximate posterior and can be used for various applications of VAEs, such as image generation, music synthesis, and speech generation.\tAuthors approach the \"hole problem\" of variational autoencoders where the aggregate posterior fails to match the prior, causing some areas of the prior distribution to be left out. Consequently, the decoder is not trained properly to operate in such regions, and the whole generate models is then subject to suboptimal performance. To attack this problem authors introduce two changes:""504\tThis paper proposes an EBM prior for a generative model called the Variational Autoencoder (VAE), which is a powerful likelihood-based generative model with applications in image generation, music synthesis, speech generation, imagecaptioning, semi-supervised learning, and representation learning. The paper discusses the benefits of using a product of a base prior and a reweighting factor, r ( z ), to bring the base prior closer to the aggregate posterior, q ( z ), of the VAE. The prior is designed to be more similar to the posterior than the base prior, which is a prior with a low prior weight. The paper shows that the proposed prior can improve the expressivity of the posterior and improve the quality of the generated data. The paper also presents an example of how the prior can be used in image generation using the VAE.\tThe goal of the paper is to model the marginal over latents in VAEs in such a way to minimize the mismatch with the aggregated posterior. The paper proposes a new class of marginal distributions over the latent space that is a product of two experts: the first expert is a non-trainable probability distribution, and the second expert is an unnormalized probability distribution parameterized using neural networks. Since training a product of experts requires to apply an approximate inference (e.g., MCMC sampling), the authors propose to use the likelihood ratio trick. Eventually, a VAE is trained in two stages. First, they assume the marginal over z's to be simply the non-trainable distribution, and the VAE is trained. At the second stage, they propose to train the second expert (i.e., the binary classifier that distinguishes z ~ q(z) and z ~ p(z)) in order to obtain the final NCP that better matches the aggregated posterior. Further, the idea is extended to hierarchical VAEs, and a separate binary classifier is trained per each stochastic level.""505\tThis paper discusses the use of policy regularization in reinforcement learning, a technique that can be used to improve the performance of reinforcement learning algorithms by removing Policy Gradient Optimization (PGO)'s need for a given reward function. Geist et al. (2019) proposed a theoretical foundation for regularized Markov decision processes (MDPs), a framework that uses strongly convex functions as policy regularizers. This allows for the unique existence of an optimal policy, which can be more confident than the existence of multiple optimal policies in the absence of policy regularization. The paper also discusses the benefits and limitations of policy regularization, including its use in challenging domains such as games and robot control, and its impact on the performance of reinforcement learning algorithms.\tThis work considers a regularized IRL setup, where instead of the entropy regularization used in maximum entropy IRL, an arbitrary convex regularizer $\\Omega$ is used. The work presents a number of theoretical results for this general setting, and it is shown that when $\\Omega$ is Tsallis entropy, the $RL \\cdot IRL$ is equivalent to minimizing a Bregman divergence defined based on the Tsallis entropy and the expert state-action distribution. A practical algorithm is presented for IRL with the Tsallis entropy. A number of experiments are performed to obtain understanding of various components.""506\tThis paper discusses the use of policy regularization in reinforcement learning, specifically how strong convex functions can be used to uniquely determine the optimal policy in a regularized Markov decision process (MDP) framework. Policy regularization is an advanced RL method that is often used to improve the performance of policies that are not known apriori. The paper provides a theoretical foundation for using strongly convex functions as policy regularization, and discusses the advantages and limitations of this approach.\tThis paper proposes a new method for regularized inverse RL. The paper builds upon work by Geist et al. who studied regularized MDPs with convex policy regularizers. The Shannon entropy is a special case of such a policy regularizer. The paper extends the analysis of Geist et al. for regularized IRL and devises tractable solutions to regularized IRL that only depend on the analytic knowledge of the regularizer. The paper further proposes regularized adversarial IRL (RAIRL), an extension of AIRL by Fu et al., as an algorithm for IRL in regularized MDPs. The algorithm is validated on a number of domains.""507\tThis paper discusses the use of policy regularization in reinforcement learning, a technique that helps to ensure that the optimal policy is unique and can be trained using a single learning rate. Geist et al. (2019) proposed a theoretical foundation for regularized Markov decision processes (MDPs), a framework that uses strongly convex functions as policy regularizers. The paper argues that this approach allows for unique training of the optimal policy, while ensuring that the policy is not multiple solutions to the problem. Additionally, the paper provides insights into the benefits and limitations of policy regularization.\tThis paper shows a formulation of regularized Markov Decision Processes (MDPs), which is slightly different from that of Geist et al. (2019). Then, the authors propose a novel inverse reinforcement learning under regularized MDPs. One of the contributions is that policy regularization considered here is more general than that of Yang et al. (2019). ""508\tThis paper discusses the role of model architecture design in inject inductive biases into neural network layouts, which can allow algorithms to favor certain representations over others, independent of the observed data. The paper focuses on multilingual neural machine translation (MNMT), where the learning objective is typically cast as a multi-task learning problem. The paper\u8ba8\u8bba\u4e86\u5728MNMT\u4e2d\uff0c\u51b3\u5b9a\u54ea\u4e9b\u7ec4\u4ef6\u5e94\u8be5\u5728\u4efb\u52a1(\u8bed\u8a00)\u4e4b\u95f4\u5171\u4eab\uff0c\u4ee5\u53ca\u54ea\u4e9b\u7ec4\u4ef6\u5e94\u8be5\u7559\u7ed9\u7279\u5b9a\u4efb\u52a1\u6216\u8bed\u8a00\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u53ef\u4ee5\u662f\u6574\u4e2a\u5c42\u6808\u3001\u5355\u4e2alayers\u751a\u81f3\u67d0\u4e9bsub-layers\u3002\u6ce8\u610f\u5230\uff0c\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u7684\u89c4\u6a21\u589e\u52a0\u6216\u589e\u52a0\u4efb\u52a1(\u8bed\u8a00)\u7684\u6570\u91cf\uff0c\u641c\u7d22\u7a7a\u95f4\u4e2d\u5171\u4eab\u53c2\u6570\u548c\u7c92\u5ea6\u7684\u5927\u5c0f\u4e5f\u4f1a\u8fc5\u901f\u589e\u957f\u3002\tIn this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation. To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages. The language specific sub-layers are incorporated throughout the network. The training objective allow budgetary constraints on the amount of language-specific parameters. The paper does a systematic analysis on the role of language specific parameters using the proposed architecture. Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally. The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc. ""509\tThis paper discusses the role of model architecture design in inject inductive biases into neural network layouts, which can allow a learning algorithm to favor certain representations over others, independent of the observed data. The paper also discusses the search space of which components to share and at which granularity in multilingual neural machine translation (MNMT), where the learning objective is often cast as a multi-task learning problem. It explains that as the neural network is made larger or the number of tasks ( languages) increases, the search space of which parameters to share and at which granularity grows rapidly. The paper also discusses the potential applications of this research in MNMT, such as improving the performance of the algorithm and reducing the amount of data required to train the model.\tThe work proposes a hybrid architecture that has: (1) language-specific (LS) components; (2) as well as the components that are shared across all the languages -- a trade-off between specificity and generality.  A key conclusion of the work is that the best architectures typically are. the ones that have ~10-30% language-specific capacity. ""510\tThis paper discusses the role of model architecture design in injecting inductive biases into neural network layouts, which can allow a learning algorithm to favor certain representations over others, independent of the observed data. The paper focuses on multilingual neural machine translation (MNMT), where the learning objective is commonly cast as a multi-task learning problem. The paper\u8ba8\u8bba\u4e86\u5728MNMT\u4e2d\uff0c\u51b3\u5b9a\u54ea\u4e9b\u7ec4\u4ef6\u5e94\u8be5\u5728\u4efb\u52a1(\u8bed\u8a00)\u4e4b\u95f4\u5171\u4eab\uff0c\u4ee5\u53ca\u54ea\u4e9b\u7ec4\u4ef6\u5e94\u8be5\u7279\u5b9a\u4e8e\u4efb\u52a1\u6216\u8bed\u8a00\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u53ef\u4ee5\u662f\u6574\u4e2a\u5c42Stack\u3001\u5355\u4e2alayers\u751a\u81f3\u67d0\u4e9b\u5b50layers\u3002\u6ce8\u610f\u5230\uff0c\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u7684\u5927\u5c0f\u589e\u52a0\u6216\u589e\u52a0\u4efb\u52a1(\u8bed\u8a00)\u7684\u6570\u91cf\uff0c\u9009\u62e9\u5171\u4eab\u54ea\u4e9b\u53c2\u6570\u4ee5\u53ca\u4ee5\u4f55\u79cd\u7c92\u5ea6\u5171\u4eab\u4f1a\u8fc5\u901f\u589e\u52a0\u3002\tIn this work, the authors present a conditional language-specific routing (CLSR) scheme for transformer-based multilingual NMT systems. They introduce a CLSR layer after every transformer encoder and decoder layer; each such layer is made up of hard gating functions conditioned on token representations that will either select a language-specific projection layer or a shared projection layer. Further, a budget is imposed on the language-specific capacity measured by aggregating the number of gates that allow for language-specific computations; this budget constraint forces the network to identify the sub-layers that will benefit most from being language-specific.""511\tThis paper proposes a new algorithm, WDN, which can handle noisy labels and improve the accuracy of classification tasks byExploiting the geometric relationship between uncertain and certain samples to extract useful information even from uncertain samples.To this end, the papersplits the data into uncertain and certain samples based on small loss criteria. It investigates the geometric relationship between these two different types of samples and enhance this relation to exploit useful information.To imposing geometric constraints on the uncertain samples, itnormalizes them into the Wasserstein ball centered on certain samples. Experimental results demonstrate that the WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels. The proposed WDN is highly compatible with existing classification methods, meaning it can be easily plugged into various methods to improve their accuracy significantly.\tThis paper propose a computationally efficient Wasserstein distributional normalization algorithm for accurate classification of noisy labels. An explicit upper bound for the Wasserstein-2 distance is derived and such a bound can be used as an estimator to determine if a network is over-parameterized. Empirical results on CIFAR-10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches. ""512\tThis paper proposes a new algorithm, WDN, which can handle noisy labels for accurate classification. The main goal of WDN is to Normalize the distribution of uncertain samples to better reflect the true distribution of the data. By doing so, it can be more confident in the classification of these samples. The paper investigates the geometric relationship between uncertain and certain samples and enhances this relation toExploit useful information even from uncertain samples. To this end, it impose geometric constraints on the uncertain samples by normalizing them into the Wasserstein ball centered on certain samples. Experimental results demonstrate that WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels. The proposed WDN is highly compatible with existing classification methods, meaning it can be easily plugging into various methods to improve their accuracy significantly.\tThe paper is a contribution that aims at solving the label noise problem. In this setting, the labels are possibly corrupted, this yielding a potentially significant underperformance of a (neural network) classifier when minimizing the empirical risk. This problem is ubiquitous and important in real life scenarii. The paper builds on the idea of small loss criteria, which favors learning on certain samples in the beginning of the learning process, and gradually incorporate uncertain samples along iterations. The paper proposes a novel type of distributional normalization based on Wasserstein distance. It projects uncertain samples on a Wasserstein ball defined wrt. the certain samples. This process is done with a particle based stochastic dynamics, based on a Ornstein-Ulenbeck process. A theoretical Analysis is given, along with results on classical datasets in the symmetric noise setting, open noise and a real world dataset (clothing 1M), for which it achieves very good performances compared to state of the art competing methods. ""513\tThis paper proposes a new algorithm, WDN, which can handle noisy labels for accurate classification. The WDN algorithmsplits the data into uncertain and certain samples based on small loss criteria. It investigates the geometric relationship between these two different types of samples and enhances this relation toExploit useful information, even from uncertain samples. To this end, it impose geometric constraints on the uncertain samples by normalizing them into the Wasserstein ball centered on certain samples. Experimental results demonstrate that the WDN outperforms other state-of-the-art methods on the Clothing1M and CIFAR-10/100 datasets, which have diverse noisy labels. The proposed WDN is highly compatible with existing classification methods, meaning it can be easilyPlug into various methods to improve their accuracy significantly.\tThe paper introduces a novel objective function by imposing geometric constraints on the logits of uncertain samples. The authors' approach is to map the distribution logits of uncertain samples onto the 2-Wasserstein ball centered on the measure of certain samples. To overcome the dilemma of selecting the ball radius, the authors propose a surrogate objective, namely Wasserstein Normalization. An SDE grad flow is proposed for solving the Wasserstein normalization. The paper also keeps the Gaussian parameters as moving average during training in light of batch normalization. The paper both theoretically and empirically validate their method.""514\tThis paper discusses the use of computer vision classifiers in medical settings. The author explains how such classifiers can be used to make high-stakes medical decisions, including the importance of rule out harmful diagnoses. The paper also suggests that the classifier should provide actionable uncertainty quantification in addition to an estimate of the most likely diagnosis.\tThe paper proposes a new conformalized procedure for computing uncertainty sets in classification tasks. The key feature of the method is that the size of the uncertainty sets are regularized via a penalty on the size. The issue of large uncertainty sets produced by conformalized procedures is an interesting one, which the paper does well to highlight. The proposed solution of using an additive regularizer is reasonable, and appears to be effective for sensible choices of the hyper-parameters. However, the paper has some significant weaknesses.""515\tThis paper discusses the use of computer vision classifiers in medical decision-making. The author argues that while maximum likelihood diagnosis with a probability can be useful, it is not the most important piece of information. Instead, the doctor should also rule in or rule out harmful diagnoses\uff0c\u4ee5\u786e\u4fdd patient health. The author suggests that the classifier should also provide actionable uncertainty quantification, such as a set of predictions that provably cover the true diagnosis with a high probability (e.g., 90%).\tIn this paper, the authors propose a regularized conformal score for use in a conformal prediction framework. This regularizer is motivated by the instabilities of top-p variations on conformal scores (cf. Romano et. al., 2020) and the resulting high-variance in output conformal prediction set sizes. The proposed regularizer smooths top-p scores with top-k scores, which empirically results in more robust predictive sets. The authors also perform a large-scale evaluation on ImageNet with modern architectures, which serves as a helpful benchmark for conformal prediction algorithms.""516\tThis paper discusses the use of computer vision classifiers in medical settings. The author suggests that the classifier should be used in conjunction with uncertainty quantification to make the best decisions. The author argues that even if the most likely diagnosis is a stomach ache, it is important to rule out stomach cancer to ensure the health of the patient. The classifier should provide a set of predictions that provably cover the true diagnosis with a high probability.\t Prediction sets are used to quantify the uncertainty of classification. The naive approach which include the labels until a pre-specified coverage probability is satisfied often leads to large prediction sets. Adaptive Prediction Sets (APS) can output prediction sets with desired coverage but set sizes are still not satisfyingly small and the results are unstable, especially when many probability estimations fall into the tail of the distribution. ""517\tWasserstein barycenters are a measure of the average location of a probability distribution over a metric space. They are particularly useful in applications where the space of probability measures is large and the distribution is not easily represented as a continuous function. In this paper, we discuss the importance of Wasserstein barycenters, their computational complexity, and the applications of them in various fields. We also review recent advances in the development of efficient algorithms for computing Wasserstein barycenters.\tThis work introduces a new Wasserstein-2 barycenter computation method. The authors first derive the dual formulation of the Wasserstein-2 barycenter problem, and then parametrize the convex potentials by ICNNs. The congruent and conjugacy conditions are enforced by regularization terms, respectively. They then show that the algorithm can find a good barycenter if the objective function is properly minimized.""518\tThis paper discusses the Wasserstein barycenter, a geometric representation of the average of probability measures, and its applications in various computational problems. The Wasserstein barycenter is used in image processing, geometry processing, online machine learning, and Bayesian inference for various tasks such as color and style transfer, texture synthesis, shape interpolation, and probabilistic predictions of experts. Additionally, fast and accurate barycenter algorithms are available for discrete distribution problems.\tThe paper derives the barycenter mapping problem as an optimization over *congruent* convex functions---each convex potential corresponding to a component distribution.  Congruency is a property on the set of optimal potential functions that ties them together.  However, this optimization is quite challenging and so the paper derives an principled objective function that includes two regularization terms.  The first regularization term encourages congruency of the set of convex functions and can be seen as a variational bound on an ideal congruency regularization.  The second regularization term encourages the pairs of convex functions to be conjugate.  The paper proves that the optimal solution of this objective is the true potentials and thus no bias is introduced.  The proposed approach is demonstrated on the tasks of generative modeling (2-256 dimensions), posterior inference, and color pallete barycenters (3D)""519\tThis paper discusses the Wasserstein barycenter, a geometric representation of the average of probability measures, and its applications in various fields. The paper provides an introduction to Wasserstein barycenters, and discusses their advantages over other methods for representing probability measures. It also describes some of the applications of Wasserstein barycenters, such as color and style transfer, texture synthesis, shape interpolation, and online machine learning. Finally, the paper discusses some recent advances in the field of Wasserstein barycenters, including fast and accurate algorithms for discrete distributions.\tThe paper considers the Wasserstein Barycenter problems in the continuous setting. In particular, the authors propose an algorithm to compute the Wasserstein-2 barycenter when only samples from the marginals are accessible. Some theoretical analysis of this method is presented. Several numerical examples are carried out to compare this method with two other recently proposed methods.""520\tThis paper discusses the issue of rigorous explanations for performance of deep neural networks in machine learning and computer vision applications. The paper proposes the formulation of a multiple manifold problem, a binary classification problem where the classes are two disjoint submanifolds of a low-dimensional space. The paper discusses the challenges of training deep neural networks on multiple manifold problems and suggests that using this type of problem can provide rigorous performance guarantees without the complex interaction between models, architectures, data, and algorithms in neural network training. The paper also suggests various network properties that can be used to reflect the properties of the data and to clarify the mechanisms of deep learning.\tThe authors consider a binary classification task. As a model the authors use a deep fully-connected neural network and train it to separate the submanifolds, representing different classes. They assume that sub-manifolds belong to the unit sphere. Also, the authors restrict their analysis to a one-dimensional case. The main claim is that by increasing depth we can improve model generalization of a network, trained by SGD.""521\tThe paper discusses the issue of rigorous explanations for deep neural network performance on machine learning and computer vision tasks, particularly in the context of low-dimensional data. The authors propose the formulation of a multiple manifold problem, in which the classes are two disjoint submanifolds of a higher-dimensional space, as a way to capture the essential features of the data and to provide rigorous performance guarantees. The problem is then solved using a deep neural network and the results compared to traditional machine learning algorithms. The paper\u8ba8\u8bba\u4e86\u51e0\u79cd\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\uff0c including using support vector machines to guide the training of the deep network, and analyzing the role of various network properties in the model's performance. The authors also provide examples of applications of the multiple manifold problem in machine learning and computer vision, and discuss the potential of this approach to clarify the mechanisms by which deep networks succeed.\tThe paper studies the conditions for a deep fully-connected network to separate low-dimensional data classes. A binary classification setting is considered, where the two classes are modelled as two different manifolds. The manifolds are assumed to be one-dimensional for the ease of analysis. It is shown that the network depth should be sufficiently large so as to adapt to the geometrical properties of data (e.g. the manifold curvature); the network width should increase polynomially with the network depth; and the number of data samples should also scale at a polynomial rate with the network depth. The authors show that if these conditions are met, with high probability a randomly initialized network converges to a classifier that separates the two class manifolds. The proof technique relies on conditioning the network parameters of the l-th layer on the parameters of the previous layers using a Martingale model, which gives sharp concentration guarantees. ""522\tThe paper discusses the challenges in understanding and explaining the performance of deep neural networks in machine learning and computer vision, particularly in applications where the data has low dimensionality. The paper suggests the use of model problems that capture the essential features of applications (such as low dimensionality) but are simple enough to admit rigorous end-to-end performance guarantees. This approach has the potential to clarify the roles of various network properties and to better understand the mechanisms by which deep networks succeed. The paper also discusses the multiple manifold problem, a binary classification problem in which the classes are two disjoint submanifolds of different dimensions. The problem is posed as a problem of graph theoreticians and has been studied extensively in the context of deep learning. The paper provides a mathematical framework for understanding and solving the problem, and suggests that there may be other problems that can be used as models in machine learning and computer vision that can help clarify the performance of deep networks.\tThe paper under review studies the question of whether gradient descent can solve the problem of calibrating a deep neural network for separating two submanifolds of the sphere. The problem studied in the paper is very interesting and as been the subject of recent increasing interest in the machine learning community. The contribution is restricted to a simple set up and addresses the question in the finite sample regime. The framework of the analysis hinges on the Neural Tangent Kernel approximation of Jacot et al. ""523\tThis paper proposes an off-policy algorithm for model-free RL that is simple, easy to implement, and can incorporate off-policy data. The algorithm is called advantage-weighted regression (AWR) and each iteration consists of two supervised regression tasks. The paper discusses the advantages and disadvantages of traditional policy gradient algorithms and how AWR can overcome these issues. Additionally, the paper presents an example of how AWR can be used to train an agent to learn a complex behavior in a real-world scenario.\tThis study presents a deep reinforcement learning method, Advantage-Weighted Regression (AWR). The policy update of AWR is constrained as in a similar manner as REPS (Peters et al., 2010). Although the benefit of AWR is not clear in the reinforcement learning tasks, AWR exhibits its advantages in the context of imitation learning and off-policy learning with static datasets. ""524\tThis paper proposes an off-policy algorithm for model-free RL. The AWR algorithm uses advantage-weighted regression to learn a policy that maximizes the expected return. It is simple to implement and easy totune, as it does not require any additional parameters or sophisticated algorithms. The AWR algorithm can easily incorporate off-policy data, by using supervised regressions to train on additional data. This paper provides a practical and effective solution to the problem of developing an RL algorithm that is simple and stable.\tThis paper focuses on developing an RL learning algorithm that is simple and can significantly improve performance over existing algorithms. The paper presents the algorithm AWR, which is an extension of the algorithm reward weighted regression. The primary extensions of RWR are using the advantage function instead of the q function and the ability to use experience replay. Experiments on common environments are conducted to evaluate the performance of AWR and compare it to other algorithms. There are ablation experiments to justify the choice of some of the extensions. ""525\tThis paper proposes an off-policy algorithm for model-free RL. The AWR algorithm uses advantage-weighted regression to learn actions based on off-policy data. Each iteration of the algorithm involves two supervised regression tasks. The paper\u8ba8\u8bba\u4e86AWR\u7b97\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c06\u5176\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u3002\u5b83\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e9b\u793a\u4f8b\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5b9e\u73b0\uff0c\u65b9\u4fbf\u8bfb\u8005\u8fdb\u884c\u5b9e\u9a8c\u548c\u9a8c\u8bc1\u3002\tThis paper presents a reinforcement learning algorithm that applies advantage-weighted regression. In each iteration, it samples trajectories from a mixture of previous policies, estimates the value function and then computes the advantage value to estimate the policy. The idea is very similar to the work published in \u201cNeumann, Gerhard and Peters, Jan R, Fitted Q-iteration by advantage weighted regression, Advances in neural information processing systems, 2009\u201d, starting from reward-weighted regression and further developing to advantage weighted regression.  The difference in this paper is to add a constraint on the policy search, requiring the policy to be similar to the sampling policy. However, this constraint has also been studied in the paper \"Christian Wirth and Johannes Furnkranz and Gerhard Neumann, Model-Free Preference-based Reinforcement Learning, AAAI 2017\" (It seems not in reference). Overall, it may enhance this paper if it has more technical novelty when developing a new algorithm.""526\tThis paper discusses the use of sinusoidal regularization in deep neural networks (DNNs) to improve the accuracy of DNN models trained using quantized training data. The paper proposes the use of WaveQ, a sinusoidal regularization term that is added to the training loss, to prevent overfitting in the quantized domain. The paper also discusses the challenges of training DNN models on quantized data, including the need to discover the appropriate bitwidth of quantization for each layer while considering the accuracy, and the need to learn weights in the quantized domain for a given set of bitwidths. The paper proposes an approach to solving these challenges using a gradient-based joint optimization problem. The main insights of the paper are: (1) The use of sinusoidal regularization can prevent overfitting in the quantized domain, and (2) The proposed approach can be used to discover the appropriate bitwidth of quantization for each layer while considering the accuracy.\tThe paper proposes using a sinusoidal regularizer for neural network quantization. The regularizer \u201cWaveQ\u201d (sin^2) pushes floating-point parameters towards quantized values. Because the period of the function is highly related to the required bit-width, it can be used to determine the bit-width while keeping good characteristics - continuous and trainable. The authors provide experiments on both CNN and Transformers. The proposed method is widely adaptable and easy-to-use with quite promising results.""527\tThis paper discusses the use of deep quantization to reduce the memory footprint and compute requirements of neural networks. Deep quantization is a form of binary coding that uses a large number of small images to represent a single large image. The use of small images allows for a more efficient representation of the large image, which can be used to reduce the memory footprint of the network. However, deep quantization can lead to accuracy\u4e0b\u964d when used with traditional neural networks. To address this issue, this paper proposes the use of a novel sinusoidal regularization term in the training loss of the network. This term helps to regularize the network's weights and prevent them from becoming too large or small, which can improve the accuracy of the network. Additionally, this paper discusses the challenges that come with learning weights in the quantized domain and proposes several methods to overcome these challenges.\tThis paper proposed a regularization term to control the bit-width and encourage the DNN weights moving to the quantization intervals. The key in such regularization is the Sinusoidal function, where the penalty is maximized in the middle of quantization levels and minimized at the quantization points. The sinusoidal period is regarded as the continuous representation of the bit-width.""528\tThis paper discusses the use of sinusoidal regularization in neural networks to improve the accuracy of deep neural networks (DNNs). The main goal is to reduce the computational requirements of DNNs while maintaining high accuracy. The paper presents an analysis of the effects of using sinusoidal regularization on the accuracy of DNNs and suggests that it can be an effective way to improve the performance of these networks. Additionally, the paper discusses the challenges of using sinusoidal regularization in practice, such as ensuring that the regularization term is correctly defined and that it is effective in improving the accuracy of the network. The paper also proposes a solution to these challenges by presenting a method for incorporating sinusoidal regularization into the training process using a gradient-based joint optimization problem.\tWhen training quantized neural networks, one typically first fixes the desired bitwidth $b$ (of weights and activations). While training, one maintains and updates full-bitwidth weights during backprop and \"cheats\" by using $b$-quantized versions of these full-precision weights during forward propagation. The quantization scheme used may vary.""529\tThis paper discusses data augmentation techniques in neural machine translation (NMT). In NMT, a neural network is used to translate a source language into a target language. Data augmentation is used to generate additional training samples when the network is thirsty for data. The paper discusses the benefits and challenges of using data augmentation in NMT, including the need for manually\u810a\u80cc\u586b\u5199 the transformations, and the potential for overfitting with too many examples. The paper also presents some existing data augmentation techniques and discusses future directions for research in this area.\tThe paper proposes a GAN process for training neural machine translation models. The noise generator in this approach uses a switching-aligned-words technique where they randomly switch a word in the source sentence with its translation in the target sentence. They use fast-align to get alignments between source and target sentences. The experiments show that the noisy sentence pair generator performs best with the proposed switch and align approach in comparison with other (more random) methods. ""530\tThis paper discusses data augmentation techniques in neural machine translation (NMT). In NMT, a model is used to translate from one language to another. Data augmentation is used to generate additional training samples because deep learning models are thirsty for data. The paper focuses on image data augmentation and its use in Computer Vision and natural language processing, but also discusses its use in NMT for text tasks. It provides examples of how data augmentation can improve the performance of NMT models.\tThis paper describes a method for data augmentation and/or regularization for machine translation that works by running a word aligner on the parallel data, and then with some probability \\gamma, replacing a source token with its corresponding target token or vice versa. A proposed variant also mixes the embeddings of the two words. Small improvements are shown over simpler noising strategies such as replacing words with placeholder tokens or with random words from the vocabulary.""531\tThis paper discusses the use of data augmentation techniques in neural machine translation (NMT). It explains the importance of using additional training examples when training deep neural networks on large amounts of data and the benefits of using image data augmentation in Computer Vision and text for NLP tasks. The paper also discusses some of the commonly used data augmentation techniques in NMT and their advantages and limitations.\tThe paper proposes a data augmentation technique where source sentences are perturbed by replacing (or mixing) source words with their aligned counterparts from the target language (while the target sentences remain as is). Alignments can be either obtained from an unsupervised aligner like fast-align or from the attention distribution of an NMT model. Perturbations are aimed to be semantically invariant to preserve the meaning of the source sentence. In addition to simply replacing the source word with the aligned word, authors also try out inputting a weighted combination of both the source word and the target word and refer to this method as \u201cmixing\u201d. Empirical observations suggest that simply replacing the source word with the aligned target yields better results.""532\tThis paper proposes a real-time contribution measurement method to distribute rewards in aFederated Learning framework for protecting distributed data privacy in a commercial union. The method defines the impact of each agent and comprehensively considers the current round and the previous round to obtain the contribution rate of each agent. The paper conduct pseudo-distributed training and an experiment on the Penn Treebank dataset and compare the Shapley Value in game theory with the proposed method to verify its effectiveness. The comparative experiment result shows that the proposed method is more sensitive to both data and contribution, which can better protect distributed data privacy and distribution of rewards in a commercial union.\tThe paper is to measure each client\u2019s contribution to training the federated learning model. In particular, the contribution is measured by the distance between the local model and the global model in each iteration. The targeting problem is interesting, and the use of attention-based model divergence is also an interesting idea to measure the contribution. However, the paper lacks strict theoretical discussion to prove the proposed solution is a reasonable one rather than a heuristic method. Moreover, the experiment is too weak to support the claims. The paper\u2019s technique contribution and originality are also limited. ""533\tThis paper proposes a real-time contribution measurement method to distribute rewards in aFederated Learning framework for protecting distributed data privacy in a commercial union. The method defines the impact of each agent and comprehensively consider the current and previous round to obtain the contribution rate. The paper conduct pseudo-distributed training and an experiment on the Penn Treebank dataset and compare the Shapley Value in game theory with other methods. The experimental result show that the proposed method is more sensitive to both data and contribution, and is effective in distributed learning.\tThe paper proposes a low computational complexity method for weighting contributions of clients in a federated learning setting. The main contributions are to compare the weighting method with Shapley values and their sensitivity to low data volume and quality. The paper is based on the FedAtt paper that calculates weights based on the Euclidean distance between the server model and each client and for each layer.""534\tThis paper proposes a real-time contribution measurement method to distribute rewards in aFederated Learning framework for protecting distributed data privacy in a commercial union. The method defines the impact of each agent and comprehensively considers the current round and the previous round to obtain the contribution rate. It conduct pseudo-distributed training and an experiment on the Penn Treebank dataset to verify the effectiveness of the proposed method. The comparative experiment result shows that the proposed method is more sensitive to both data and contribution than the traditional Shapley Value method.\tThe paper proposes a new contribution measurement approach for federated learning. The basic idea is that the agent with a larger model update has a larger contribution. Specifically, based on FedAtt [1], the impact of a client is computed as the local updates plus the impact of the previous round times a decay rate. The experiments on a dataset show that the proposed approach can have a similar contribution measurement compared with Shapley Value.""535\tThis paper studies the problem of learning Bayesian networks in which an fraction of the samples aredversarial. Bayesian networks are a family of probabilistic graphical models that represent conditional dependence by a directed graph. The goal of learning Bayesian networks is to infer the true distribution of the data from which the network was trained. This problem has been studied extensively in the context of machine learning and has many applications in fields such as finance, natural language processing, and computer vision. In this paper, we discuss the problem of learning Bayesian networks in the presence ofdversarial samples. We propose a new learning algorithm that can learn the network with high accuracy even in the presence ofdversarial samples. We also discuss the challenges and limitations of the problem and the potential applications of our approach.\tThe paper considers the problem of robustly learning fixed structure Bayesian networks in nearly-linear time. Previous work by Cheng et al. gives a runtime of O(Nd^2/eps). The paper improves this to O(Nd). The algorithm works by directly relating the problem to robust mean estimation, and then leveraging the algorithm of Dong et al. for robust mean estimation which works in nearly-linear time. The authors have to modify the runtime analysis of the algorithm of Dong et al. to work in time linear in the sparsity, rather than dimension.""536\tThis paper studies the problem of learning Bayesian networks in which an fraction of the samples aredversarial. Bayesian networks are a family of probabilistic graphical models that represent conditional dependence by a directed graph. The problem of learning Bayesian networks has been studied extensively in the literature, and there are several different approaches that can be used to address this problem. In this paper, we discuss the problem of learning Bayesian networks in the presence ofdversarial samples, and we propose a new approach that uses a combination of deep learning and probabilistic model-fitting techniques. We also evaluate the performance of our approach on a real-world dataset and show that it is able to learn a correct model of the data while handlingdversarial samples.\tThe paper studies the problem of robust learning of fixed-structure Bayesian networks under the eps-adversarial corruptions model. Fixed-structure means a known structure of the underlying Bayesian network. Robust learning is an important area of research and this particular question has been studied in prior work. The main contribution of this work is in improving the running time of the algorithm. On a d-node Bayes net, let m denote the total number of parental configurations possible. Prior work of Cheng et al showed a robust learning algorithm using O(m/eps^2) samples and runs in time O(md^2/eps^2).""537\tThis paper studies the problem of learning Bayesian networks where an\u7279\u5b9a fraction of the samples aredversarial. Bayesian networks are a family of probabilistic graphical models that represent conditional dependence by a directed graph. The goal of learning Bayesian networks is to generate a model that represents the distribution of interest, while taking into account thedversarial samples. This problem has been studied extensively in the literature, with many approaches proposed for handlingdversarial samples. In this paper, we explore several different approaches to handlingdversarial samples, including usingdversarial training\uff0c\u5bf9\u6297\u8bad\u7ec3\uff0c anddversarial testing. We also discuss the potential limitations of these approaches and the challenges they face in real-world applications.\tThis paper studies the problem of learning Bayes nets using adversarially corrupted data. The model is that $N$ samples are made from a Bayes net on d nodes, out of which an unknown $\\varepsilon$ fraction are changed arbitrarily. The structure of the Bayes net is already given, but it remains to learn the probability distribution.""538\tThis paper discusses the recent advancements in model-based reinforcement learning (RL) and autonomous skill acquisition directly from image inputs. It also highlights the challenges and opportunities in planning and optimization for vision-based systems. The paper highlights the benefits of using explicit learning of latent representation spaces and gradient-free action sampling as the underlying optimizer for vision-based planning. The paper also discusses the current state-of-the-art in this area and\u6307\u51fa\u8fd8\u6709\u5f88\u591a\u63d0\u9ad8\u7684\u7a7a\u95f4\u3002\tIn this paper, the authors propose to replace commonly-used shooting-based methods for action sequence planning in learned latent-space dynamics models by a collocation-based method. They argue that shooting-based methods exhibit problematic behavior especially for sparse-reward and long-horizon tasks, as shooting methods do not allow for planning trajectories which (slightly) violate the learned dynamics. The authors propose a collocation method based on Levenberg-Marquard optimization with a scheduled Lagrange multiplier which outperforms two shooting methods (CEM and gradient-based) on a set of robotic tasks.""539\tThis paper presents an overview of recent advances in autonomous agents and their ability to process high-dimensional sensory inputs, such as images, and reason over long horizons about the potential effects of their actions. The paper highlights the benefits of model-based reinforcement learning (RL) and how these advancements have allowed for impressive results in autonomous skill acquisition directly from image inputs. However, the paper also notes that there is still much room for improvement on the planning and optimization side, especially in using gradient-free action sampling as the underlying optimizer. The paper\u6700\u540e proposes a future direction for autonomous agents based on these advancements, including the use of deep learning approaches with better planning and optimization capabilities.\tThis paper introduces a vision-based motion planning approach using collocation. Many existing approaches to vision-based control rely on computationally expensive planning approaches using shooting to perform model-based control, which is often only useful in simple control tasks. Collocation approaches are effective in settings with difficult path constraints, and thus exploited by this work to dramatically improve model-based reinforcement learning.""540\tThis paper discusses the recent advances in autonomous agents and their ability to process high-dimensional sensory inputs, such as images, and reason over long horizons about the potential effects of their actions. It also discusses the challenges that remain in developing effective planning and optimization algorithms for these agents. The paper highlights the importance of improvements in modeling and planning, as well as the need for more efficient and effective optimization algorithms. The paper also discusses the recent work in model-based reinforcement learning (RL) that has shown impressive results in autonomous skill acquisition directly from image inputs.\tThe paper studies the problem of planning in domains with sparse rewards where observations are in the form of images. It focuses on solving this problem using model-based RL with emphasis on better trajectory optimization. The proposed solution uses latent models to extract latent representations of the planning problem that is optimized using the Levenberg-Marquardt algorithm (over a horizon). The experimental results show improvements over a) zeroth-order CEM optimization, b)  PlaNet (Hafner et al., 2019) and c)  gradient-based method that optimizes the objective in Eq. 1.""541\tThis paper discusses the performance of Bayesian neural networks (BNNs) and suggests that improving the posterior distribution can lead to better predictive performance. The paper also discusses the issue of temperature in stochastic gradient Langevin dynamics (SGLD) and how it can be used to achieve sharper posteriors. The paper also highlights the importance of using the right prior in combination with Bayes decision theory for optimal performance. The results of the paper are puzzlement, as the Bayesian viewpoint suggests that the posterior distribution should give optimal decision making, but the approaches used in the paper often don't always emphasize this factor.\tThe work propose a theory suggesting that the cold posterior phenomena arises solely due the the curated nature of image benchmarks. A generative model is proposed where multiple annotators label datapoints, and only unanimously labeled datapoints are accepted into a dataset. This theory is studied under a toy-problem using VI and a relabelled version of the CIFAR-10 test set with SGLD. ""542\tThis paper discusses the issue of how to improve the performance of Bayesian neural networks (BNNs) by \u201csharpening\u201d the posterior distribution. The paper also discusses the effects of multiplying the log-posterior by a temperature, which is known in stochastic gradient Langevin dynamics (SGLD) as a way to \u201ctemper\u201d the KL term, and how this approach has been used in recent papers to obtain good performance. The paper also notes that the importance of this factor can vary depending on the context and the prior distribution used. Overall, the paper highlights the need for a better understanding of the relationship between the posterior distribution and the prior distribution in BNNs and how to use these factors to improve the performance of the network.\tThis paper addresses the perplexing issue of cold posterior having better predictive performance than the ideal Bayesian posterior in Bayesian deep learning (Wenzel et al., 2020), and offers a possible explanation in terms of a mis-specified likelihood function that deviates from the true generative process of the data. By considering the data curation process and augmenting the likelihood model accordingly, the effect of cold posterior is shown to diminish significantly, and the ideal posterior is again optimal. Empirical results on both a toy problem and image classification support the theory.""543\tThis paper discusses the behavior of Bayesian neural networks (BNNs) and suggests that the posterior distribution can be sharper or\u9a7f faster when the temperature (or \\\"temperature\\\" in the stochastic gradient Langevin dynamics (SGLD) framework) is smaller than 1. This is achieved by multiplying the log-posterior by a factor of 1/T. The paper also discusses the use of variational inference (VI) and suggests that the KL term can be tempered to obtain good performance in BNNs. The results of this approach are not always emphasized in the literature, making it challenging to understand the importance of this factor. The paper also\u6307\u51fa that the sharper or\u9a7f faster posterior distribution can be achieved by using a \\\"right\\\" prior, which may not always be the case in practice.\tThe authors propose the idea that cold posteriors in Bayesian neural networks could be caused by the likelihood instead of the prior. They argue theoretically that the curation process of popular benchmark data sets would lead to a different weighting of the likelihood in the posterior. They show in some experiments that the cold posterior effect can be reduced when accounting for this.""544\tThis paper discusses the recent development of non-autoregressive machine translation (NAR) systems that can speed up decoder training and inference while maintaining high translation quality. NAR systems generate translations by using a decoder that assumes conditional independence between output tokens, which can lead to problems with translation quality. Gu et al. (2018) presented a paper that describes the architecture and performance of a NAR system. They also discuss the challenges of using NAR in practice and offer suggestions for improving the quality of the generated translations.\tTraditional NTM is done with a large encoder and large autoregressive (AR) decoder. Due to the sequential nature of the AR decoder, inference can be slow due to lack of parallelism (unless done at very large batch sizes). Non-Autoregressive (NAR) models have been proposed to alleviate this problem, but all NAR approaches trade off some translation quality for speed gains. In this paper, the authors claim that an alternative to NAR is to speed up standard AR decoding by reallocating network weights and layers to the (easily parallelizable) encoder, and making the decoder a single layer. They claim that this matches the speed of NAR models while keeping the performance of traditional AR models, making it a better choice in the design space than any NAR models. Comparisons are made to CMLM and DisCo NAR models to justify these claims with experimental evidence.""545\tThis paper discusses the recent development of non-autoregressive machine translation (NAR) and its impact on machine translation quality. NAR is a machine translation system that uses parallel decoders to speed up translation inference while still achieving high accuracy. However, NAR can suffer from the problem of conditional independence between output tokens, which can prevent the model from properly capturing the highly multimodal distribution of target translations. Gu et al. (2018) propose a solution to this problem by using a combination of features and a loss function to encourage conditional independence between output tokens. This solution is shown to improve translation quality compared to traditional autoregressive machine translation systems.\tThe authors advocate for fair comparison between autoregressive (AR) and non-autoregressive models (NAR) in non-autoregressive machine translation (NAT) research. They highlight three main aspects where the comparison has not been fair so far in the literature - suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. They perform extensive comparisons between AR and NAR models in these 3 aspects and report interesting results. ""546\tThis paper discusses the state-of-the-art in machine translation and how recent work has developed ways to speed up decoding and improve translation quality with non-autoregressive machine translation (NAR). NAR involves parallelizing the decoder, which allows for partial parallelization of the process and faster inference. However, the paper highlights the trade-offs between speed and quality in NAR and suggests that further research is needed to optimize the system for both objectives. The paper also discusses the challenges of capturing the multimodal distribution of target translations and suggests that additional features like context information and prior knowledge can help improve the quality of the translations.\tThe paper proposes deep encoder and shallow decoder models for auto-regressive NMT. They compare rigorously to NAR models. They also study three factors: layer allocation, speed measurement and knowledge distillation. They include that with a 12E-D1 model they obtain significant speed-up and can outperform the standard 6-6 AR model and almost always beat the NAR model in terms of quality. They also show that NAR models need deep decoders because they need to handle reordering.""547\tThis paper provides an overview of deep neural networks (DNNs) and their behavior when it comes to model capacity and generalization. The paper explores the relationship between DNN model complexity and their performance, and observes that DNNs often have large model capacity, but also generalization well. This behavior violate the conventional VC dimension or Rademacher complexity theory, which inspire new designs of network architectures and reconsideration of their optimization and generalization. The paper also explores the model-wise double descent phenomenon, where as DNN model complexity increases, it shows a classical U-shaped curve and then enters a second descent. This behavior has been observed on many machine learning models, and multiple studies provide theoretical evidence of this phenomenon. The paper provides an overview of the research on DNNs and their behavior, and discusses the potential implications of this work for machine learning and other fields.\tThe paper under review studies the epoch wise double descent phenomena empirically. The epoch wise double descent phenomena is the observation that the risk of a large neural network trained with SGD first decreases, then increases, and finally decreases again as a function of the epochs or SGD steps. In addition, it proposes a quantity called ``optimization variance (OV)'', and it demonstrate that OV correlates with the test error. Based on this observation, it proposes to early stop when the OV reaches a minimum.""548\tThis paper discusses the problem of model complexity in deep neural networks and its impact on their generalization and optimization. It explains that deep neural networks often have large model capacity, but also generalize well, which violated the conventional VC dimension or Rademacher complexity theory. This inspired new designs of network architectures and reconsideration of their optimization and generalization. Model-wise double descent, where the test error first shows a classical U-shaped curve and then enters a second descent, has been observed on many machine learning models. Theoretical evidence of this phenomenon is provided by multiple studies. The paper concludes by discussing the potential implications of this phenomenon for deep learning and other machine learning algorithms.\tHaving a stopping rule without the validation set is intriguing, especially for datasets with a low number of samples. The authors propose a rule that doesn't require the validation dataset, i.e. it is solely based on training data. It introduces the notion of optimization variance which is different from the variance of gradients. ""549\tThis paper provides an overview of deep neural networks (DNNs) and their behavior in terms of model capacity and generalization. It\u8ba8\u8bba\u4e86\u4f20\u7edf VC \u7ef4\u6216 Rademacher \u590d\u6742\u6027\u7406\u8bba\u6240\u63cf\u8ff0\u7684\u5f02\u5e38\u60c5\u51b5\uff0c\u5373DNN \u7684\u6a21\u578b\u80fd\u529b\u65e2\u5177\u6709\u5de8\u5927\u7684\u6a21\u578b capacity\uff0c\u53c8\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u6fc0\u53d1\u4e86\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5e76\u5bf9DNN \u7684 Optimization \u548c generalization \u8fdb\u884c\u4e86\u91cd\u65b0\u8003\u8651\u3002\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0\u4e86DNN \u6a21\u578b\u5728\u968f\u7740\u6a21\u578b\u590d\u6742\u6027\u7684\u589e\u52a0\u800c\u8868\u73b0\u51fa\u7684\u201c\u6a21\u578b\u53cc descent\u201d\u73b0\u8c61\uff0c\u5373\u5148\u5448\u73b0\u51fa U \u578b\u66f2\u7ebf\uff0c\u7136\u540e\u8fdb\u5165\u7b2c\u4e8c\u4e2a\u4e0b\u964d\u9636\u6bb5\u3002\u8fd9\u4e00\u53d1\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9DNN \u7684\u4f18\u5316\u548c\u6cdb\u5316\u7684\u7406\u89e3\u3002\tThe paper studies the trajectory of the test error as a function of training time focusing on Epoch-Wise Double-Descent.  Similar to \"Rethinking Bias-Variance Trade-off for Generalization of Neural Networks\" by Yang et. al., the paper shows that if one decomposes the test error to bias and variance terms, Double Descent occurs as a function of train time as a result of unimodality of the variance term (while the bias term decreases monotonically).  The paper also introduces a quantity they name optimization variance (OV) and that correlates with the test error (while being only a function of the train set) and can be useful for early stopping.""550\tThis paper discusses the meta-learning literature and the most interesting bi-level optimization-based meta-learning methods designed for fast- adapting few-shot learning. The paper starts by introducing the concept of meta-learning and its potential benefits, including fast generalization and ease of implementation. It then discusses model- and metric-based meta-learning methods, including model-agnostic meta-learning (MAML) and its applications. The paper also covers optimizationbased meta-learning methods such as Ravi & Larochelle and Finn et al.'s method, and their applications in few-shot learning. Finally, the paper ends with a summary and future directions.\tThis paper investigates the adversarial robustness of model agnostic meta-learning (MAML). Adversarial robustness can be added to MAML in two places, meta-update stage and and fine-tune stage. It shows that robustifying the meta-update stage via fast attack generation method is sufficient to achieve fast robustness adaptation without losing generalization and computation efficiency in general. The paper also demonstrates that unlabeled data can help using contrastive representation learning to improve generalization and robustness. ""551\tThis paper discusses the meta-learning concept and several existing meta-learning methods, including model-agnostic meta-learning (MAML), which is a bi-level optimization-based meta-learning method designed for fast- adapting few-shot learning. The paper highlights the benefits of meta-learning, including fast generalization and ability to adapt to unseen tasks. The paper also discusses some of the recent advances in meta-learning, such as the use of neural networks and large-scale datasets. The paper ends by discussing some potential applications of meta-learning in fields such as computer vision and natural language processing.\tIt is an interesting paper empirically addressing adversarial robustness of model agnostic meta learning (MAML). The paper investigates where to incorporate robust regularization in MAML in order to improve adversarial robustness, and based on that *efficient* robust MAML methods are proposed. Interestingly, contrastive learning is incorporated and derive a more robust MAML model. ""552\tThis paper discusses a range of meta-learning techniques, including model-agnostic meta-learning (MAML), which is a bi-level optimization-based meta-learning method designed for fast- adapting few-shot learning. The paper explains the background and motivation for meta-learning, the different types of meta-learning methods, and the applications of meta-learning to real-world problems. It also discusses the recent advances in MAML and how these techniques have changed the way we approach meta-learning.\tThis paper explores a way to promote the adversarial robustness in Model-agnostic meta-learning (MAML). It conducts extensive experiments to show regularizing adversarial robustness at meta-update level is sufficient to offer fast and effective robustness adaptation on few-shot test tasks. However, it lacks the theoretical analysis for this conclusion. Also, the experiments only are conducted on few-shot image classification task on miniImageNet dataset. This makes this conclusion lack sufficient credibility.""553\tThis paper studies the learning-to-learn approach to training efficientoptimizers using optimization algorithms. The approach involves using metagradient descent to learn a newoptimizer, based on a meta-objective based on the trajectory that theoptimizer generates. However, there were few theoretical guarantees on how to avoid metagradient explosion/vanishing problems or how totrain anoptimizerwith good generalization performance. This paper studies this issue on a simple problem oftuning the stepsizefor quadratic loss, andshows that while there is a way to design themeta-objective so that themetagradient remain polynomially bounded, Computing themeta-gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. The paper also suggests to compute themeta-objective on a separate validation set instead of the original training set. Finally, the paper verify its results by running experiments andshowing that the trainedoptimizer has better generalization performance than the original ones.\tMeta-gradient descent is an approach to step-size adaptation in which the step-size is adapted by considering how it influences the loss function over time. Intuitively, one can think of the trajectory of parameters $(w_s)_{s=1}^t$ as being a function of the step-size $\\eta$, and try to control the loss indirectly through the step-size's influence on the weight trajectory. This paper provides guarantees for this class of algorithms when applied to a quadradic loss function. It is shown that the meta-objective $\\ell_t(\\eta)=\\frac{1}{2}w_t(\\eta)^\\top H w_t(\\eta)$ contains no bad local solutions, but can suffer from vanishing/exploding gradients. It is then shown that this can be remedied simply considering the logarithm of this meta-objective, but that this too will have issues with numerical stability if approached with back-propagation. Finally, results related to the generalization ability of these methods are presented.""554\tThis paper studies the learning-to-learn approach to training efficient optimizers using optimization algorithms. The approach involves using metagradient descent to learn a new optimizer, based on a meta-objective based on the trajectory that the optimizer generates. However, there were few theoretical guarantees on how to avoid metagradient explosion/vanishing problems or how to train an optimizer with good generalization performance. This paper presents results that show that while there is a way to design the meta-objective so that the metagradient remain polynomially bounded, computing the meta-gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. The paper also discusses when it is necessary to compute the meta-objective on a separate validation set instead of the original training set. Finally, the paper verify our results using empirical results.\tThis paper considers algorithms that attempt to learn learning rates for gradient descent by gradient descent. Analysis is provided for a few specific quadratic losses showing that the gradient with respect to the learning rate may explode or vanish, and taking the logarithm is suggested to mitigate this. Further results suggest that implementing the gradient of the log comes with interesting numerical difficulties as *intermediate results* might explode or vanish even if the final answer does not.""555\tThis paper studies the learning-to-learn approach to training efficientoptimizers using optimization algorithms. The main idea is to use metagradient descent to learn a newoptimizer by training it on a meta-objective based on the trajectory that theoptimizer generates. However, there were few theoretical guarantees on how to avoid metagradient explosion/vanishing problems or how totrain anoptimizer with good generalization performance. The paper demonstrates that there is a way to design themeta-objective so that the metagradient remain polynomially bounded, butcomputing themeta-gradient directly using backpropagation leads to numerical issues that look similar to gradient explosion/vanishing problems. The paper also studies the necessary conditions for when it is necessary to compute themeta-objective on a separate validation set instead of the original training set. Finally, the paper verify its results through empirical verification.\tThis paper presents novel theoretical results on learning a step size for vanilla GD and SGD by unrolling the optimization steps and back-propagating, taking into account the simple problem minimizing quadratic functions and mean-square errors. The authors could demonstrate the occurrence of already-detected phenomena for learned optimizers, such as gradient explosion/vanishing and over-fitting, in the particular studied case. A few experiments illustrate what the developed theory predicts. ""556\tThis paper discusses the performance of Convolutional Neural Networks (CNNs) in solving various spatial and graph data processing problems. The paper starts by introducing CNNs and highlighting their capabilities in image classification, semantic segmentation, and machine translation. It also mentions that CNNs can effectively reuse the convolution kernel and use the given input to train optimal parameters for different types of data. The paper then discuss the challenges faced when using CNNs for non-Euclidean spatial data and graph problems, such as the need for additional data features and the computational complexity of training the network. It also highlights the recent developments in CNN-based graph processing, such as link prediction and node classification. The paper ends by providing a summary of the main findings and future directions for CNN-based data processing.\tIn this paper, a graph view-consistent learning framework (GVCLN) is proposed. Specifically, two view learners are used to give predictions for the input. Then, a consistency loss is employed to force the two viewers giving the same predictions. Moreover, a co-training scheme is proposed to alleviate the label sparsity problem.""557\tThis paper discusses the performance of convolutional neural networks (CNNs) in solving spatial and non-Euclidean data problems. The paper highlights the fact that CNNs are effective at utilizing existing data structures and training parameters in solving these types of problems. The original data for the spatial problems (e.g. image classification and semantic segmentation) all had a grid-like data structure, while the non-Euclidean data (e.g. social networks, telecommunication networks, biological networks, and brain connection structures) was represented in the form of graphs. The paper discusses the different graph problems that can be solved using CNNs, such as link prediction, graph classification, and node classification. Additionally, the paper highlights the challenges that arise when using CNNs on non-Euclidean data and suggests some potential solutions.\tThis paper proposes a view-consistent framework to address the issues of expensive labels. In particular, this work first uses graph neural networks and graph attention networks to construct two different latent features of the same data. Then, it uses the same classification neural networks to produce the node classification outcomes. Finally, it uses the classification outcome to construct a so-called \"view loss\". In addition, it uses an incremental strategy to gradually included pseudo labels until some termination conditions are satisfied. ""558\tThis paper discusses the use of convolutional neural networks (CNNs) to solve non-Euclidean spatial data problems, specifically social networks, telecommunication networks, biological networks, and brain connection structures. It also discusses the different types of graph problems that can be solved using CNNs, such as link prediction, graph classification, and node classification. The paper provides an overview of the challenges and opportunities in using CNNs for non-Euclidean spatial data and suggests possible future directions.\tThis paper adopts a multi-view learning approach for graph representation learning where some labels are assumed to be available. It uses graph convolution network (GCN) and graph attention network (GAT) to create two different views of the same graph and then define a loss function to force the output due to the two views to be consistent. The low label rate scenario is considered and pseudo labels are created to define an additional loss function to better enforce consistency. Three datasets are used for performance evaluation.""559\tThis paper discusses the use of neural networks with symmetries in the context of machine learning. It explains how adding a bias to neural networks can achieve exceptional performance on tasks such as classification, and how building in symmetries, such as translation invariance and rotational symmetries, has been successful in achieving this. It also discusses the use of energy conversation in the context of Hamiltonian Neural Networks (HNNs), which is an approach that uses the energy functional, rather than the phase-space coordinates, to predict the dynamics of a system. The paper ends by\u603b\u7ed3\u51fa the main findings and future directions of the research.\tThis submission explores the question of identifying conserved quantities in Hamiltonian dynamics for physical systems by attempting to learn canonical transformations. The approach closely resembles previous work on \"Hamiltonian neural networks\" but the loss is augmented with a term enforcing the invariance of the dynamics under the transformation and with a term that ensures the resulting transformed coordinates satisfy the constraints of the algebraic relations that emerge from the Poisson bracket. Together these two additions allow the authors to train a network that performs a change of coordinates which is subsequently optimized to bring it closer to a canonical transformation. Perhaps the main observation is that some of the cyclic coordinates identified by the network have a clear relation to the underlying conserved quantities. ""560\tThis paper discusses the use of neural networks with built-in biases and symmetries in order to achieve exceptional performance in tasks such as classification. The paper begins by discussing the importance of building biases into neural networks, particularly in tasks such as classification where the input data often has a certain distribution. It also highlights the success of using symmetries such as translation invariance and rotational symmetries in building neural networks that are more flexible and able to learn more complex relationships between the input data and the output labels. The paper then discusses how to incorporate these biases and symmetries into neural networks, and how the use of Hamiltonian Neural Networks ( HNNs) can improve the performance of these networks. The paper ends by\u603b\u7ed3 the main findings and highlighting the potential applications of these methods in various fields.\t.** The authors propose using a neural network to learn a canonical transformation of the data coordinates before learning a Hamiltonian. This is a novel contribution in that previous work has shown how to learn Hamiltonians with neural networks but it has not shown how to learn the proper canonical transformation. In the course of learning this canonical transformation, they show how to project out other symmetries (linear and angular momentum) and hence improve upon HNNs while also learning these other symmetries to a good approximation.""561\tThis paper discusses the use of neural networks with biases and symmetries in the context of computer vision tasks. The authors provide examples of how to build in translation invariance, rotational symmetries, and energy conversation into neural networks to achieve exceptional performance on tasks such as classification and object detection. The paper also discusses the challenges and limitations of these techniques, including the need for careful design and implementation, as well as the need for additional data and computational resources. Finally, the paper provides an overview of the recent advances and future directions in this field.\tThis paper presents the results of a NN trained to learn symmetries in physics, specifically, to learn and preserve quantities that are preserved (e.g., energy, angular momentum). The input is a sequence generated from a Hamiltonian dynamics. Results of experiments on 2 and 3 body problems and a harmonic oscillator are presented. The training networks are small, shallow feedforward networks. There is some customization of the training networks to incorporate \"cyclic\" coordinates. Results indicated empirical conservation up to small error of physically conserved quantities. The paper is fairly easy to read, with much relevant background provided.""562\tThis paper discusses the applications of graph neural networks (GNNs) in various fields, including molecular design, computer vision, combinatorial optimization, and recommender systems. The paper also highlights the existence of a canonical GNN architecture and the recent focus on using tabular data, such as node embeddings or bag-of-words representations, in GNNs. The paper also discusses the challenges and opportunities in using GNNs for real-world applications.\tThe paper proposes a GNN model by incorporating gradient boosting. In the proposed BGNN, the input feature on the graph is learned by the gradient boosting model. The processed feature then becomes a new feature for a GNN model following the gradient boosting. Several experiments demonstrate the improvement of the performance for a tabular feature and graph-structured datasets. The running time of Res-GNN/BGNN is shown to have a significant reduction as compared to the plain GNN methods.""563\tThis paper discusses the applications of graph neural networks (GNNs) in various fields, including molecular design, computer vision, combinatorial optimization, and recommender systems. The paper also highlights the existence of a canonical GNN architecture and the importance of using tabular data with rich semantics in real-world AI applications. The paper also discusses recent research in using GNNs with sparse data, particularly for tasks where there is a high amount of tabular data and rich semantics among nodes in the graph. The paper concludes by highlighting the future directions and challenges for GNN research.\tThis paper aims to learn from graphs with tabular node features. Existing methods are only designed to handle either tabular data, such as gradient boosting decision tree (GBDT), or graph-structured data, such as graph neural networks (GNNs). This paper naturally extends GBDT to deal with graph-structured data and train it together with GNN in end-to-end fashion.""564\tThis paper discusses the use of graph neural networks (GNNs) for learning on graph-structured data and their applications in various fields. The paper provides an overview of GNNs, including their structure, encoding methods, and limitations. It also highlights the main driving force for progress in GNN research, which is the existence of a canonical GNN architecture that efficiently encodes the original input data into expressive representations. The paper also discusses the recent focus on using tabular data, such as node embeddings or bag-of-words representations, rather than homogeneous node embeddings or bag-of-words representations. The paper also discusses the limitations of GNNs and how they can be overcome. Finally, the paper concludes by highlighting the potential of GNNs in the real-world AI applications, such as recommendation systems and molecular design.\tReview: This paper proposes a fusion of GBDT and graph neural network that works on graphs with heterogeneous tabular features. Previous approaches are computationally heavy and do not consider graph-structured data and suffer from lack of relational bias imposed in GNNs. The proposed method is a new ensemble tree method which alternates between functional gradient step in GBDT (which train on the current latent features) and SGD training of graph neural network (to generate the latent features which are fed into the subsequent trees).""565\tThis paper presents an overview of meta-learning, a powerful paradigm for learning to adapt to unseen tasks. Meta-learning is a form of deep learning that does not train one model for each individual task, but rather learns a \u201c prior\u201d model from multiple existing tasks, which allows it to quickly adapt to unseen new tasks. The paper discusses the high-level methodology in meta-learning, the commonly used practices in meta-learning algorithms, and the applications of meta-learning in real problems. The paper also presents a case study of meta-learning in machine translation, where the success of the algorithm is shown in handling small number of training examples and low resources.\tIn this paper, the authors study the theoretical properties of meta-learning. In particular, the train-validation split to tackle the linear centroid meta-learning problem is investigated using statistical asymptotic theory. First, the authors proved that the train-validation method has statistical consistency, while the train-train method has a statistical bias to the centroid. Under the noise-free setting, however, both methods have statistical consistency. Furthermore, the train-train method is superior to the train-validation method in the sense of the asymptotic MSE. Based on the asymptotic analysis the optimal ratio of the data splitting for the train-validation method was also derived. The theoretical findings are confirmed by some numerical experiments. ""566\tThis paper discusses the concept of meta-learning, a powerful paradigm for learning to adapt to unseen tasks, and its applications in real-world problems such as few-shot image classification, hyper-parameter optimization, machine translation, and short event sequence modeling. The paper provides an overview of meta-learning's high-level methodology, which involves learning a prior model from multiple existing tasks and adapting to new unseen tasks by relates the prior experience to the new task. The paper also discusses the common practices in meta-learning algorithms, such as sample splitting and the use of learning rates. The paper concludes by highlighting the potential of meta-learning for improving the performance of complex machine learning systems.\tIn meta-learning, a common practice is to do a train/validation split of the data within each that, so that optimization of meta-parameters is performed on validation, not training, losses. In this paper, the author argue that this split is important for correcting model misspecification, but if the model is correctly specified, not doing a split might actually lead to better learning rates. They provide theoretical justification under a simple linear model, and some experiments on synthetic and real data.""567\tThis paper provides an overview of meta-learning, a powerful paradigm for learning to adapt to unseen tasks, which is also known as \u201clearning to learn.\u201d The paper explains how meta-learning works, how it has been applied to real-world problems, and some of the latest advances in the field. The paper also discusses the common practices in meta-learning algorithms, such as sample splitting and how to implement them in Python.\tThe authors verify the importance of train-validation split in meta-learning theoretically, which is commonly used in the meta-learning paradigms. By analyzing the linear centroid meta-learning problem, the authors show that the splitting method converges to the optimal prior as expected, whereas the non-splitting method does not in general, without structural assumptions on the data. The authors validate the theories empirically through both simulations and real meta-learning tasks.""568\tThis paper discusses the topic of learning with noisy labels, which is an important issue in machine learning because of the widespread use of large datasets with label noise. The paper categorizes existing methods for learning with noisy labels into two categories: algorithms that result in Statistically consistent or inconsistent classifiers. Zhang and Sabuncu (2018) and Kremer et al. (2018) are examples of methods in the first category that design classifier-consistent algorithms. Liu & Tao (2016) and Scott (2015) are examples of methods in the second category that design inconsistent algorithms. Goldberger & Ben-Reuven (2017) and Patrini et al. (2017) are examples of methods in the first category that are not yet available. Thekumparampil et al. (2018) and Yu et al. (2019) are examples of methods in the second category that are available.\tThis paper proposes momentum of memorization as a way to distinguish hard examples needed for efficient learning from noisy examples which decrease classification accuracy. The method finds confident, hard examples and updates them dynamically during model training. This is done by iteratively selecting examples with labels that agree with model predictions and then training on only the confident data. Results show improved accuracy on standard image classification datasets with both synthetic and real world label noise.""569\tThis paper discusses the topic of learning with noisy labels and the existing methods for reducing the effect of this type of label noise. The paper starts by introducing the problem of growing training datasets while accurately labeling them is often expensive or even infeasible.label noise is a common issue that can have a negative impact on the performance of learning algorithms. The paper then focuses on existing methods for learning with noisy labels, which can be divided into two categories: algorithms that result in statistically consistent or inconsistent classifiers. The first category of methods intent to design classifier-consistent algorithms. These methods include Zhang & Sabuncu (2018), Kremer et al. (2018), Liu & Tao (2016), Scott (2015), Natarajan et al. (2013), Goldberger & Ben-Reuven (2017), Patrini et al. (2017), Thekumparampil et al. (2018), and Yu et al. (2019). The second category of methods include algorithms that result in inconsistent classifiers. These methods include Kremer et al. (2018), Liu & Tao (2016), Scott (2015), Natarajan et al. (2013), Goldberger & Ben-Reuven (2017), Patrini et al. (2017), Thekumparampil et al. (2018), and Yu et al. (2019). The paper\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411.\tThis paper propose a novel and effective method called Me-Momentum to cope with noisy labels. The algorithm borrows the idea of momentum from physics and tries to identify hard examples. The authors alternately update the hard examples and improve the classifier to achieve the robustness to noisy labels. Experiments and comparisons with recent state-of-art methods are provided to verify the effectiveness of Me-Momentum. ""570\tThis paper discusses the issue of label noise in machine learning and proposes two categories of methods for reducing the effect of label noise on learning algorithms. The first category of methods designs algorithms that result in classifier consistent, or consistent with statistical results, classifiers. The second category of methods focuses on developing methods that can achieve better performance with label noise. The paper also discusses the potential limitations of these methods and the challenges of implementing them in practice.\tThe authors introduce an interesting approach to handling hard \"confident\" samples in learning with label noises. At the heart of the proposed approach is an interactive method that jointly refines the classifier and the samples. The confident samples are initialized by utilizing the memorization effect of deep networks. Then, a classifier is learned from such samples. ""571\tThis paper discusses data poisoning attacks, which are a type of malicious behavior that can occur in the training phase of a machine learning system. Data poisoning attacks are committed by carefully modifying, adding, or removing some training examples from a dataset, leading to the corruption of the training model and incorrect predictions for testing examples. These attacks are a serious security concern in critical application domains such as autonomous driving, cybersecurity, and healthcare analytics. Unlike adversarial examples, which add perturbation to each testing example to induce misclassification, data poisoning attacks modify the training model in a way that leads to its incorrect predictions. This paper provides an overview of data poisoning attacks, describes the potential consequences of these attacks, and discusses some of the recent research in the field.\tFirst, the paper identifies k-Nearest Neighbor (kNN) and radius Nearest Neighbor (rNN) to be naturally effective baseline certified defenses against data poisoning attack. It is easy to see that kNN and rNN are resistant to poison attacks, since to flip the prediction of a test example, one would need to insert/delete enough examples to change the majority vote. Second, the paper proposes a joint certificate that further improves certified accuracy for rNN. Specifically, it uses the fact that for any given poison removal budget, it can only decrease the vote for a single label. Even though the idea is simple, the experimental result is quite impressive significantly outperforming the previous more sophisticated certified defense methods.""572\tThis paper discusses data poisoning attacks, which are a type of malicious behavior that can affect the training phase of a machine learning model. The attacks are caused by carefully modifying or adding examples to the training dataset in such a way that the model becomes incorrect for testing examples. Data poisoning attacks are a serious security concern in many critical application domains, such as autonomous driving, cybersecurity, and healthcare analytics. Unlike adversarial attacks, which add adversarial examples to the training dataset to cause misclassification, data poisoning attacks modify the training examples in a way that corruptions the model's ability to accurately classify testing examples. This can have serious consequences for the accuracy and security of machine learning models. The paper provides an overview of the types of data poisoning attacks and discusses some potential\u9632\u8303\u63aa\u65bd.\tThe paper studies robustness of k-NN and r-NN against data poisoning attacks. The main message of the paper is that k-NN and r-NN are automatically resilient against attacks. Furthermore, by grouping test examples based their predicted labels. Data points with different predictions are grouped together, and then better certification guarantee can be derived. Experimental results demonstrate that k-NN and r-NN are indeed self-robust against data poisoning attacks.""573\tThis paper presents an overview of data poisoning attacks, which are a type of malicious input attack on a machine learning system that aim to modify the training dataset of the system in order to cause it to predict incorrect labels for testing examples. Data poisoning attacks are a serious security threat to machine learning in critical application domains such as autonomous driving, cybersecurity, and healthcare analytics. Unlike adversarial examples, which add perturbation to each testing example to induce misclassification, data poisoning attacks modify the training dataset in a careful and targeted manner, leading to the corruption of the training phase of the machine learning system and the misprediction of labels for testing examples. The paper provides an overview of the types of data poisoning attacks, the common techniques used to perform them, and the security measures that can be used to prevent them.\tThis paper studies to train a certifiable robust model against data poisoning attacks using nearest neighbors. The paper studies the voting mechanism in the nearest neighbor models, and presents a relationship between the poisoning instances and the difference between the majority votes and the second majority votes. Such a relationship will result in a guarantee on the lower bound of a training model's accuracy, which is referred to as Certified Accuracy (CA). The theoretical results are neat. The experiments are conducted on MNIST and CIFAR, and results show better CA than previous approaches of DPA and Bagging.""574\tThis paper discusses the issue of training large neural networks being time-consuming and suggests various strategies to reduce the training time while retaining accuracy. The most popular training algorithms for deep learning are Stochastic Gradient Descent (SGD) and its variants such as RMSProp or Adam. The paper suggests that the size of the minibatches plays a crucial role in the network's accuracy, generalization capability and converge time. It also discusses the importance of early stopping and the use of batch\u4f11\u606f\u4e00\u4e0b (batch\u4f11\u606f\u4e00\u4e0b\uff1b\u629b\u94f3\u4f11\u606f\uff1b\u94f3\u624b\u4f11\u606f) to avoid overfitting. The paper also suggests that using data-parallel training (data-parallel training; dpt) or distributed training ( distributed training; dt) can improve the training time and accuracy.\tIn this paper, the authors study an important problem on how the choice of batch size (i.e., the number of sampled nodes) affects the training efficiency and accuracy of graph neural networks (GNN). Focusing on the layer-wise and graph-wise sampling for training, the authors theoretically characterize the impact of batch sizes on the efficiency (measured by a product of computation time and variance) of the algorithms. Especially, in order to better capture the randomness of two-consecutive layers, the authors investigate a different estimator rather than the one truly used in the training of GNN. The resulting theory suggests a choice of the batch size to be n/\\hat d, where n is the total number of nodes and \\hat d is the average degree of the graph. The authors empirically show that compared to the training of NN, the training of GNN requires a much larger batch size to achieve an efficient training. In addition, the experiments show that the best batch size is much smaller than the full batch that is widely adopted in the training of GNN.""575\tThis paper discusses the challenges of training large neural networks and proposes several strategies to reduce training time while retaining accuracy. The most popular training algorithms for deep learning are Stochastic Gradient Descent ( SGD ) and its variants such as RMSProp or Adam. The size of the minibatches plays a crucial role in the network's accuracy, generalization capability and converge time. The paper also discusses the challenges of training deep networks with large datasets and proposes strategies for handling these challenges, such as using data pre-processing, parallel training, and using large-scale neural networks.\tIn this paper, the authors studied the problem of batch size selection in graph neural networks. Since scaling up the batch size is the most efficient way to increase parallelism, this topic seems to be very important for making full use of modern computer architectures like GPUs or TPUs. The authors conduct a detailed analysis on the impact of batch size in graph neural networks. After a series of discussions, the authors conclude that an ideal batch size should be n/d where n is the total number of nodes and d is the average node degree. The authors suggested that this batch size should be able to give the best convergence/generalization performance for graph neural networks. Overall, the paper is properly written. In terms of presentation, the structure is clear and the idea is easy to understand.""576\tThis paper discusses the challenges of training large neural networks and proposes several strategies to reduce training time while retaining accuracy. The most popular training algorithms for deep learning are Stochastic Gradient Descent (SGD) and its variants like RMSProp and Adam. The paper highlights the importance of the size of the minibatches in the accuracy, generalization capability and convergence time of the network. It also discusses the impact of other factors such as the number of layers, and the choice of\u6fc0\u6d3b\u51fd\u6570 on the training time. The paper proposes some strategies for selecting the size of the minibatches, choosing the\u6fc0\u6d3b\u51fd\u6570\uff0c and other parameters of the network. The paper also discusses the potential limitations of these strategies and suggests some additional research directions.\tThe goal of the paper is to propose a principled strategy to select batch size for training graph neural networks with SGD. Training (using GNNs) real world graphs with a large number of nodes/ edges may not always fit in CPU /GPU memory, hence constructing mini-batches is important. Specifically, the authors propose a strategy for the task of node classification - where they aim to select batch size based on number of nodes and average degree in a graph and show that their proposed guidelines have benefits in terms of training time as well as accuracy. The authors propose a metric - pseudo precision rate which is dependent on the computation cost and the variance of the gradients and derive a lower bound for this metric which factors into account the batch size.""577\tThis paper provides an overview ofClassifiers based on Neural Networks (NNs) are widely deployed in many real-world applications. However, when applied to safety-critical domains, such as healthcare, finance, and self-driving, NNs may not provide reliable or accurate predictions due to the lack of safety guarantees. To estimate the trustworthiness of a classifier prediction, the paper discusses various confidence-related scores, such as the maximum class probability, entropy of the softmax outputs, or difference between the highest and second highest activation outputs, but\u6307\u51fa\u8fd9\u4e9b scores are unreliable and may even be misleading. The paper also suggests alternative methods for assessing the trustworthiness of a classifier prediction, such as safety checks or additional data analysis.\tThis paper introduces RED, a new methodology to produce reliable confidence scores to detect missclassification errors in neural networks. The idea is to combine kernels based on both input and output spaces (as in RIO) to define a (sparse) GP that estimates the residual between the correctness of the original prediction and the maximum class probability. The authors show enhanced performance against other related methods and the ability of RED to detect OOD and adversarial data through the variance of the confidence score. ""578\tThis paper provides an overview of the use of Neural Networks (NNs) in real-world applications, including their widely deployment and good prediction accuracy. However, the paper also highlights the issue of lack of safety guarantees when NNs are applied to safety-critical domains such as healthcare, finance, and self-driving. The paper proposes to use a combination of classifier's inherent confidence-related score, such as the maximum class probability, entropy of the softmax outputs, or difference between the highest and second highest activation outputs, to estimate the trustworthiness of a classifier prediction. However, it also points out that these scores are unreliable and may even be misleading, as high-confidence but\u9519\u8bef\u7684\u9884\u6d4b\u7ed3\u679c. The paper also discusses the potential limitations of using NNs in safety-critical domains and proposes alternative approaches for estimating the trustworthiness of a classifier prediction in these cases.\tIn this paper, their goal is to improve calibration and accuracy by augmenting a classification model with a GP. They base their model off RIO (ICLR 2020) which targets regression problems and tries to predict the residual between predicted value and true value. They propose a model, RED, which instead tries to predict the residual between the predicted confidence score for the true class and 1 \u2014 the true class target confidence score using a GP. They show strong improvements over the methods they compare to for 125 UCI datasets and CIFAR-10 dataset.""579\tThis paper discusses the issue of trustworthiness of classifier predictions, particularly when they are based on Neural Networks, and proposes a new score, the difference between the highest and second highest activation outputs, as a more reliable measure of classifier confidence. It also discusses the limitations of the other confidence-related scores proposed in the literature, such as the maximum class probability and entropy of the softmax outputs. The paper concludes by suggesting ways to overcome these limitations and improve the trustworthiness of classifier predictions.\tThis paper solves an interesting problem of predicting uncertainty in NN without re-raining/modifying the existing NN. The authors propose a framework to calculate a confidence score for detecting misclassification errors by calibrating the NN classifier\u2019s confidence scores and estimates uncertainty around the calibrated scores using Gaussian processes. This framework is called RED (Residual i/o Error Detection). ""580\tThis paper discusses information retrieval and its role in natural language processing tasks, such as question answering and fact checking.  traditionally, information retrieval systems were based on hand-crafted sparse representations of text documents, such as TF-IDF or BM25. However, recent methods based on dense vectors and machine learning have shown promising results. Deep neural networks based on pre-training, such as BERT, have been used to encode documents into fixed-size representations, which are then querying using approximate nearest neighbor algorithms. This paper also discusses some of the challenges and future directions in information retrieval.\tMany Information Retrieval systems rely on two components: a retriever that identifies a small set of \"support\" documents from a large corpus, followed by a reader that re-scores these support documents more finely. For retrievers, metrics like BM25 were once common but they are increasingly replaced by machine learned components. However, most datasets do not provide direct supervision information for the retriever.""581\tThis paper provides an overview of the importance of information retrieval in natural language processing and highlights the recent advances in using machine learning and dense vectors in building information retrieval systems. The paper discusses traditional methods of information retrieval, such as hand-crafted sparse representations, and newer methods based on dense vectors and machine learning, such as deep neural networks based on pre-training, such as BERT. The paper also highlights the challenges and opportunities in building effective information retrieval systems, including the need for efficient querying and the need for a well-defined retrieval task.\tThe paper targets an important problem in open-domain QA - the training of the retriever for the purpose of determining a segment that may contain the answer. In the most traditional setting, the retriever is just a traditional IR system such as BM25. In some existing work, the retriever has been trained to locate the documents containing the answer (e.g. inverse cloze task, or DPR). This paper goes in the same direction. The difference is that it uses the attention weights as relevance signals to train the retriever, instead of the inclusion of the answer in the passage.""582\tThis paper provides an overview of information retrieval, including the traditional methods used to represent text documents and the recent advances in machine learning-based information retrieval. It discusses the importance of information retrieval in natural language processing tasks such as question answering and fact checking, and highlights the use of deep neural networks based on pre-training, such as BERT, for encoding documents into fixed-size representations and querying them using approximate nearest neighbor search.\tThe authors propose a training technique for information retrieval models in the context of (open domain) question answering. Assuming the existence of some reader model, the idea is to use internal information of that model as a training signal for a retriever. Specifically, they use the attention activations over the input documents as synthetic labels for the retriever.""583\tThis paper discusses the importance of imposing safety constraints on reinforcement learning (RL) systems in order to ensure their deployment in real-world environments. Controllers derived mathematically typically rely on a full a priori analysis of agent behavior to guarantee safe operation. This approach allows for verification of safety properties, but is difficult to understand and verify due to the requirement for exploration in an unknown environment. The paper also discusses the challenges of learning RL controllers and the importance of incorporating safety checks during the learning process.\tThe paper proposes a constrained reinforcement learning (RL) formulation relying on constraints written in a formal language. The proposed formulation is based on constrained Markov decision processes where the constraint is represented as a deterministic finite automaton that rejects any trajectory violating the constraint. The proposed solution relies on transforming the automaton's sparse binary cost into an approximate dense cost and augmenting that with the reward objective. The paper presents a series of results from simulations in Safety Gym, MuJoCo, and Atari environments.""584\tThis paper discusses the importance of imposing safety constraints on reinforcement learning (RL) systems in order to ensure their deployment in real-world environments. RL controllers that are derived mathematically rely on a full a priori analysis of agent behavior to guarantee safe operation. This approach allows for verification of safety properties and satisfaction of software contracts, but it restrictions the controllers to pre-defined, analytical operational limits. RL controllers are free to learn control trajectories that better suit their tasks and goals, but understanding and verifying their safety properties is challenging. The paper also discusses the importance of exploring new environments in reinforcement learning, as this requires exploration and learning from the environment, which can be a hazard in itself.\tThe paper builds on the constrained MDP framework (Altman, 1999), by considering the special-case where the cost functions are defined in terms of states from a parser of a formal language. In the experiments the work uses deterministic finite automata (DFA) but in principle other more expressive classes could be used. Using a formal language to specify constraints may simplify model checking (although this is left to future work). ""585\tThis paper discusses the importance of imposing safety constraints on reinforcement learning (RL) systems in order to ensure their safe deployment in real-world environments. RL controllers are typically derived mathematically and rely on a full a priori analysis of the agent's behavior to guarantee safe operation within a predefined envelope of safety. This approach allows for verification of safety properties and satisfaction of software contracts, but it is challenging to understand and verify the safety properties of RL controllers in practice. The paper highlights the need for exploration in an unknown environment as a particular hazard of learning RL controllers.\tThe authors propose to use formal languages, specifically DFAs, as a mechanism to specify constraints in a constrained MDP setting. This has the benefit of being able to rely on a large body of existing work on identification, safety verification, etc. The strategy relies on decomposing the constraint into a translation, recogniser & cost assignment function that connect the MDP to the DFA. The mentioned cost can then be combined with existing solution for solving cMDPs, such as reward shaping and Lagrangian methods. The key observation is that adding the recogniser state to the observations of the policy can result in significant gains in both performance and constraint satisfaction. A range of results are presented across different environment suites and hyper parameters.""586\tThis paper discusses the importance of understanding andExplaining binary classification models, especially in real-world applications such as medical records and scientific research. The paper emphasizes that for a model to be useful in these applications, users need to be able to understand the logic underlying its predictions. To this end, the paper presents several measures that can be used to evaluate the comprehensibility of a binary classification model, including the accuracy of its predictions and the ability of users to understand the model's decision trees. Additionally, the paper provides examples of real-world applications where comprehensibility of binary classification models is of importance, and discusses the challenges and opportunities that arise when trying to evaluate and improve the performance of these models.\tThe authors presented in this submission a nice novel idea of building a tree ensemble in a cascading style so that any positive predictions are decided and explained by the first tree predicting them positively. The reviewer finds this idea very interesting and clearly elaborated in this paper. However, more theoretical and empirical justification is crucially necessary in order to make the claims in the submission convincing. The issues listed here are some questions that the reviewer believes should have been discussed or answered in the paper.""587\tThis paper discusses the importance of understanding and explaining binary classification models, especially in the medical and scientific domains. The paper emphasizes that model comprehensibility is essential for making binary classification models useful in real-world applications, especially in the medical and scientific fields. The paper provides an overview of binary classification, explains the importance of accuracy in evaluating model performance, and discusses how to achieve model comprehensibility. Additionally, the paper presents some recent advances in binary classification models and their applications.\tThis paper introduces the Cascading Decision Tree, a novel variant of decision trees with permits to extract short explanations for a class of interest. The idea is to realize a cascade of small decision trees: at a certain level, the tree is built using all points except the positive ones correctly classified by trees in previous levels. The method has been tested using three standard datasets and a novel application.""588\tBinary classification is a process of classification of input data into two classes based on some classification criteria. It is a widely used algorithm in many real-world applications, such as determining whether a patient has a disease by analyzing their comprehensive medical record. However, for a model to be useful in real-world applications, it is imperative that users are able to understand and explain the logic underlying its predictions. In some real-world applications, especially in the medical and scientific domains, model comprehensibility is of the utmost importance. Users need to understand the classification model to scientifically explain the reasons behind the classification or rely on the model itself to discover the possibilities. This paper presents an overview of binary classification and discusses some of the most important criteria for evaluating model performance, as well as the importance of model comprehensibility in real-world applications.\tThis paper introduces a new type of classification model called the \"cascading decision tree.\" The cascading decision tree is a rule-based classifier designed to have an overlapping hierarchical structure between its nodes to produce succinct explanations. The paper introduces these models, presents an induction algorithm to learn them from data, and includes an empirical evaluation on three UCI datasets as well as a propietary dataset. The submission includes code.""589\tThis paper discusses the empirical observation that increasing the number of parameters in a deep neural network leads to improved performance. The paper also explores the consequences of this effect for model optimization and generalization. The vast majority of these studies find that increasing the width of the network, rather than increasing the number of parameters, leads to improved performance. The paper also discusses the use of neural Tangent Kernels and other techniques to analyze the effect of network width on the dynamics of neural network optimization.\tThis paper analyzed the influence of neural network width on the network performances while fixing the total number of parameters. Specifically, the authors introduced several ways to change the model width without increasing the number of parameters, and showed experimentally that for widened networks with a random static mask on weights to keep the number of parameters, increasing the width can improve the performances of the models until the network become very sparse and hard to train. They author theoretically showed that for a not so sparse one-hidden-layer neural network, increasing width decreases the distance to the Gaussian Process kernel corresponding to the infinith-width limit, which partially supports their experimental findings.""590\tThis paper discusses the empirical observation that increasing the number of parameters in a deep neural network leads to improved performance. The paper explores the consequences of this effect for model optimization and generalization. The vast majority of studies in this area have shown that increasing the width of the network, also known as the number of layers, leads to an increase in the number of parameters, and therefore, improves performance. The paper also discusses the use of neural Tangent Kernels and other techniques for optimizing deep neural networks.\tIn this paper, the authors analyze the enhancements brought by widening networks with the number of parameters fixed. From the experimental side, they conduct various experiments to compare the methods of widening the networks and demonstrate different ratios of widening different networks on diverse datasets. From the theoretical side, the authors relate the training dynamics of neural networks to kernel-based learning, in  the infinite-width limit. As a consequence, the authors claim that wider networks indeed improve the performance of algorithms under certain conditions.""591\tThis paper discusses the empirical observation that increasing the number of parameters in a deep neural network leads to improved performance. The paper explores the consequences of this effect for model optimization and generalization. The vast majority of these studies (95%) increase the number of parameters by increasing the network width. The paper also discusses the use of neural Tangent Kernels and other techniques to optimize the network parameters and improve its performance. Additionally, the paper discusses the potential impact of this phenomenon on deep learning applications in various domains.\tThis manuscript provides an intriguing discussion on the different roles that the width and parameter size could play in a neural network. While these two aspects are traditionally treated as -if not identical- correlated, the authors managed to develop a couple of configurations to decouple and analyze both separately. Especially the wide and sparse approach could be a new way to design neural networks that are supposed to be small and expressive at the same time. ""592\tThis paper discusses the performance of pre-trained language models (PLMs) in various NLP tasks and the limitations of these models in understanding world knowledge about entities and relations. It also discusses the use of knowledge graphs (KGs) as a technique to solve the sparsity problem in text modeling and improve the performance of PLMs. The paper provides an overview of the pre-trained language models, their selfsupervised training method, and the performance they achieve in various NLP tasks. It also highlights the limitations of these models, such as their struggle to grasp world knowledge about entities and relations, and the use of KGs as a technique to improve the performance of PLMs.\tThis paper presents an approach to jointly pre-train language models and representations for knowledge graphs. In particular, natural language texts (English Wikipedia) are used to train context representations, while knowledge graphs (Wikidata) train entity representations (and both depend on each other). Experiments show that the approach outperforms baseline methods on several natural language understanding tasks: few-shot relation classification, knowledge graph question answering, and entity classification. ""593\tThis paper provides an overview of pre-trained language models (PLMs), which leverage large-scale unlabeled corpora to conduct selfsupervised training and achieve remarkable performance in various NLP tasks. The paper also discusses the limitations of PLMs, including their struggle to grasp world knowledge about entities and relations, and the use of knowledge graphs to solve the sparsity problem in text modeling. It also highlights the potential applications of PLMs in language understanding, such as\u95ee\u7b54\u7cfb\u7edf (QAS) and\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP)\u4e2d\u7684\u63a8\u7406 (RLW)\u3002\tThis work proposes a method for joint pre-training of knowledge graph and text data which embeds KG entities and relations into shared latent semantic space as entity embeddings from text. The proposed model JAKET consists of two main parts: a language module and a knowledge module. The model is pre-trained on a collection of tasks: entity category prediction, relation type prediction, masked token prediction and masked entity prediction. The proposed framework enables fine-tuning on knowledge graphs which are unseen during pre-training.""594\tThis paper provides an overview of pre-trained language models (PLMs) and their applications in natural language processing (NLP). It highlights the remarkable performance of PLMs in various NLP tasks, but also discusses the limitations of these models in grasping world knowledge about entities and relations, which is important in language understanding. The paper also discusses the use of knowledge graphs (KGs) as a substitute for text in trainingPLMs and their potential applications in various NLP tasks.\tThis paper proposed a new language modeling pretraining method that leverages the knowledge graph information. Specifically, the paper replaces the entity embedding in one hidden layer of BERT context embedding, with the corresponding graph attention embedding that is obtained from the knowledge graph. The pretraining tasks contain not only the language related tasks (like predicting masked tokens), but also the knowledge graph tasks like entity classification or relation type prediction. Experiments on few-shot learning tasks, question answering and entity classification show better performance over other pretraining counterparts. ""595\tThis paper discusses the history of autoregressive models and the importance of the left-to-right autoregressive order in various application domains. It also discusses recent interest in non-left-to-right autoregressive orders and their potential applications. The paper begins by introducing the autoregressive model, which is a type of statistical model that can be used to predict the future values of a sequence of numbers. It then discusses early papers that studied autoregressive models and the interest in designing algorithms that did not require a gold-standard autoregressive order to be known upfront. However, these papers were overshadowed by developments in natural language processing, which demonstrated the power of the left-to-right autoregressive order. The left-to-right autoregressive order has been essential for application domains such as image captioning, machine translation, and image synthesis. However, interest in non-left-to-right autoregressive orders issurfacering, and evidence suggests that they may have potential applications in other fields. The paper ends by\u603b\u7ed3\u4e00\u4e0b the key points and highlighting the importance of the left-to-right autoregressive order in modern application domains.\tThis paper proposes to model the generation order as latent variables for sequence generation tasks, by optimizing the ELBO involving a proposed process of Variational Order Inference (VOI). To alleviate the difficulty of optimizing discrete latent variables, the authors propose to cast it as a one-step Markov Decision problem and optimize it using the policy gradient. The authors also introduce the recent developed Gumbel-matching techniques to derive the close-form of the posterior distribution.""596\tThis paper discusses the history of autoregressive models and their use in various applications. It reviews early papers that studied autoregressive models and their importance in natural language processing, as well as papers that explore non-left-to-right autoregressive orders. The paper also highlights the recent interest in using autoregressive models in other applications, such as imagecaptioning, machine translation, and image synthesis.\tThis paper aims to decode both content and ordering of language models and proposes Variational Order Inference (VOI). The authors introduce a latent sequence variable z = (z_1, .. ,z_n) in which z_t is defined as the absolute position of the value generated. The authors model the posterior distribution of z as a Gumbel-Matching distribution which is relaxed as a Gumbel-Sinkorn distribution. To training the encoder and decoder networks, the ELBO is maximized using the policy gradient with baseline. The experimental results on Django and MS-COCO 2017 dataset show the proposed VOI outperforms the Transformer-InDIGO, as well as suggests that learned orders depend on content and best-first generation order.""597\tThis paper provides an overview of the history of autoregressive models, including their early interest in designing algorithms that did not require a gold-standard autoregressive order to be known upfront by researchers, but which were overshadowed by developments in natural language processing. The paper then discusses the essential applications of left-to-right autoregressive orders, such as image captioning, machine translation, and image synthesis, and the recent interest in non-left-to-right autoregressive orders. Additionally, the paper provides evidence of the effectiveness of various autoregressive orders in different applications.\tThis paper designed a new generative model by capturing the auto-regressive order as latent variables for sequence generation task. Based on combinatorical optimization techniques, the authors derived an policy gradient algorithm to optimize the variational lower bound. Empirical results on image caption and code generation showed that this method is superior than both fixed-order generation and previous adaptive-order method transformer-InDIGO. The authors further analyzed the learned orders on global and local level on COCO2017 dataset, demonstrating that the arrangement tend to follows the best-first strategy.""598\tThis paper discusses the scaling issue of graph convolutional networks (GCNs) for large-scale graphs. GCNs have shown great success in many graph-related applications, but they face a problem when working with large graphs. The dependency of the nodes in the graph makes it difficult to calculate the representation of each node in the mini-batch, which grows exponentially with the number of layers. To alleviate this issue, sampling-based methods, such as node-wise sampling, layer-wise sampling, and subgraph sampling, are used. These methods can help to scale GCNs for large graphs.\tThis paper studies the convergence of stochastic training methods for graph neural networks. Here, this paper views GNN as a compositional optimization problem. Then, to reduce the variance incurred by the neighbor sampling, this paper uses SPIDER to reduce the variance to accelerate the convergence speed. It provides theoretical convergence analysis for SPIDER used on GNNs, showing that the proposed method has a better convergence rate compared with the traditional gradient descent method. At last, this paper conducts experiments to verify the proposed algorithm. ""599\tThis paper discusses the scaling issue of graph convolutional networks (GCNs) for large-scale graphs and proposes several methods to alleviate this issue, including node-wise sampling, layer-wise sampling, and subgraph sampling. The paper also reviews the existing literature on GCNs and their applications.\tNode sampling is a crucial point in making GCNs efficient. While several sampling methods have been proposed previously, the theorectical convergence analysis is still lacking. This paper finds that the convergence speed is related to not only the function approximation error but also the layer-gradient error. Based on this finding, the authors suggest to take historical hidden features and historical gradients to do doubly variance reduction. Experiments are done on 5 datasets for 7 baseline sampling-based GCNs.""600\tThis paper discusses the design and analysis of a graph convolutional network (GCN) for large-scale graphs. The GCN is a powerful network that has been used for many graph-related applications, including semi-supervised node classification, supervised graph classification, protein interface prediction, and knowledge graph processing. The paper highlights the challenges of scaling GCNs for large-scale graphs and discusses various sampling-based methods that can alleviate these challenges. The resulting GCN is then used in a real-world application, where it is used to process a large graph and perform node classification. The paper provides an overview of the GCN and the sampling-based methods, and discusses their advantages and limitations.\tThis paper presents a novel variance reduction method which can adapt to any sampling-based GCN methods (inductive GCNs). The paper draws the idea from VRGCN that integrates the historical latent representations of nodes computed with full Laplacian to approximate the that computed with sampled sparse Laplacian. The variance reduction is implemented on both node embedding approximation, as well as layer-wise gradient computation in back-propagation. The resulting algorithms lead to faster convergence rate.""601\tThis paper discusses the issue of training a deep neural network for image manipulation tasks, specifically addressing the need for large amounts of data from the same distribution as the target image. The paper demonstrates that single target images can be sufficient for performing complex image manipulation, and finds that extensive augmentation is the key for enabling single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself, allowing for general image changes by modifying the primitive input representations. The paper provides a novel augmentation method that enables single image training and discusses potential future directions for image manipulation research.\tThis paper proposed a single image-based manipulation method (DeepSIM) using conditional a generative model. The authors addressed this problem by proposing to learn the mapping between a set of primitive representation, which consists of edges and segmentation masks, and an image. They also adopted a thin-plate-splines (TPS) transformation as augmentation which enables the model to robustly manipulate an image by editing primitives.""602\tThis paper discusses the use of deep neural networks for image manipulation tasks. The authors propose that training a conditional adversarial generator on a single target image is sufficient for performing complex image manipulation. The key to enabling single image training is extensive augmentation of the input image, which they provide a novel method for doing. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself, allowing for general image changes by modifying the primitive input representation. The method is shown to be effective for manipulating complex images and can be used in a variety of applications.\tThis work proposes a method to design conditional generative models based on a single image. In particular, while some recent models have enabled one to sample (unconditionally) images from a generative model learned from a single image (like SinGAN), this work explores a way of conditioning the generation on a primitive, which can be user-specified. As a result, one can produce realistic modifications to a given image by modifying - or sketching - some primitive.""603\tThis paper presents a study on image manipulation using deep neural networks. The paper argues that deep neural networks are effective for performing image manipulation tasks, even when training on single target images is difficult due to the lack of dataset. The paper proposes a novel augmentation method that enables single image training by making extensive use of additional input data. The network is trained on a conditional adversarial generator, which allows for the manipulation of primitive representations of the image. The study demonstrates that simple training on a single target image is sufficient for performing complex image manipulation tasks.\tThis paper provides an augmentation method to enable single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. During manipulation, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network.""604\tThis paper discusses the importance of using machine learning techniques to solve graph tasks and the Maximum Common Subgraph (MCS) task, which is an important but particularly difficult graph task. The paper provides background information on graph processing and discusses the various types of graph tasks that can be solved using machine learning, including MCS. The paper also discusses the current state of the art in MCS detection and the challenges that arise when trying to solve this task using machine learning. Finally, the paper concludes by highlighting the potential applications of MCS detection in various domains, such as software analysis, graph database systems, cloud computing platforms, and drug design.\tGiven two input graphs G1,G2 the maximum common subgraph detection problem asks to find an induced subgraph of both G1 and G2, with as many vertices as possible. In the recent years, there have been papers that introduce different heuristics for guiding the search of this subgraph within branch & bound algorithms. The main contribution of this paper is a combination of graph neural network embeddings and RL to guide the search more efficiently.  The function used to guide the deep Q-network is given in Equation (3). The paper performs a set of experiments on synthetic and real world pairs of graphs, where it is shown that it performs well in practice. The supplementary material provides more details on the experiments. ""605\tThis paper discusses the importance of using machine learning to solve graph tasks and the problem of finding the largest subgraph that is common in both input graphs, also known as Maximum Common Subgraph (MCS). MCS is an important and particularly hard task that has a wide range of applications, including software analysis, graph database systems, cloud computing platforms, and drug design. The paper presents an overview of the challenges and opportunities in solving MCS and discusses some recent advances in the field. The paper also provides a summary of the paper and the main findings.\tThe motivation of this paper is clear and interesting, as it\u2019s important to explore the maximum common subgraph in biochemical domain. In this paper, the authors conduct a lot of experiments to demonstrate the effectiveness of the proposed method. Despite of this, the presentation of this paper requires improvement because many important details are missing, which makes it hard to follow. The time-complexity analysis might also be crucial to demonstrate the superiority of the proposed method over other baselines in terms of searching time. ""606\tThis paper discusses the importance of using machine learning techniques to solve graph tasks and the maximum common subgraph (MCS) problem, which is a particularly challenging task in that it encodes the degree of similarity between two graphs. The paper also highlights the various applications of MCS in various domains such as software analysis, graph database systems, cloud computing platforms, and drug design. It also discusses the challenges and limitations of solving MCS using traditional approaches and the recent research in using\u6df1\u5ea6\u5b66\u4e60 techniques to solve the problem.\tThe paper deals with the problem of Maximum Common Subgraph (MCS) detection, following a learning-based approach. In particular, it introduces GLSEARCH, a model that leverages representations learned by GNNs in a reinforcement learning framework to allow for efficient search. The proposed model has been experimentally evaluated on both artificial and real-world graphs, and its performance has been compared against traditional and learning-based baselines.""607\tThis paper discusses the use of point clouds in 3D modeling and representation. It explains that many practical 3D sensing systems produce unstructured point clouds, which can be difficult to represent accurately using traditional methods. The paper\u8ba8\u8bba\u4e86\u4e24\u4e2a\u65b9\u6cd5\u6765\u62bd\u8c61point clouds into polyhedral models\uff1a\u4e00\u662f\u627e\u5230\u5e73\u9762Surface\u5e76\u76f8\u4ea4\u4ee5\u627e\u5230\u8fb9\u7f18\u548c\u89d2\uff0c\u5982Schnabel\u7b49(2007)\u3001Fang\u7b49(2018)\u3001Coudron\u7b49(2018)\uff1b\u4e8c\u662f\u76f4\u63a5\u627e\u5230sausal\u89d2\u548c\u6216\u8fb9\u7f18\uff0c\u5982Jung\u7b49(201\tThis paper introduces a supervised neural network predicting a wireframe structure from a 3D point cloud. The network takes a raw unordered 3D point cloud as input, processes it using FCGF architecture, and predicts three types of information: vertex existence in each patch, vertex location, and edge existence for each pair of vertices. In the experiments, the network is evaluated with two datasets, a subset of the ABC dataset and a set of 3D models from Google 3D warehouse. Also, it is compared with the baseline methods using four evaluation metrics, which are created to assess the accuracy of the predicted vertices, edges, and the overall wireframe graph structure. The results demonstrate the outperformance of the proposed method quantitatively and qualitatively.""608\tThis paper discusses the use of polyhedral models to represent 3D point clouds in a more efficient and intuitive way. Many practical 3D sensing systems produce unstructured point clouds, which can be difficult to represent in a compact and meaningful way. The paper\u63a2\u8ba8\u4e86\u4e24\u79cd\u5c06point cloud\u62bd\u8c61\u6210polyhedral\u6a21\u578b\u7684\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u627e\u5230\u5e73\u9762Surfaces\u5e76\u76f8\u4ea4\u4ee5\u627e\u5230\u8fb9\u7f18\u548c\u89d2\uff0c\u5982 Schnabel et al. (2007); Fang et al. (2018); Coudron et al. (2018)\uff1b\u53e6\u4e00\u79cd\u662f\u76f4\u63a5\u627e\u5230saliency corner\u548c/\u6216\u8fb9\u7f18\uff0c\u5982 Jung et al. (2013)\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u3002\u901a\u8fc7\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u7528\u6237\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u626b\u63cf\u5bf9\u8c61\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u66f4\u8f7b\u677e\u5730\u4e0e\u5bf9\u8c61\u8fdb\u884c\u4ea4\u4e92\u3002\tThis paper presents a deep architecture to extract a wireframe model from a 3D point cloud. This is a problem of high interest, and the author claim that the approach they present is the first one to address this task, which is true to the best of my knowledge. Since both the approach and the evaluation are sound, this alone seem to warrant publication. There are however several weaknesses in the paper, and my accept recommendation is conditional to clear answer on each of them:""609\tThis paper discusses the problem of representing 3D geometry in a compact and intuitive way from a point cloud. The paper\u5148\u4ecb\u7ecd\u8bb8\u591a\u5b9e\u96453D\u4f20\u611f\u7cfb\u7edf\uff0c\u5982 stereoCamera \u6216\u6fc0\u5149\u626b\u63cf\u4eea\uff0c\u751f\u6210 unstructured 3D \u70b9\u4e91\u3002\u8fd9\u4e9b\u70b9\u4e91\u7684\u9009\u62e9\u53ea\u662f\u6700\u5c0f\u627f\u8bfa\u6027\u7684\u8868\u793a\uff0c\u53ef\u4ee5\u4f7f\u7528\u8f83\u4f4e\u5c42\u6b21\u7684\u4fe1\u53f7\u5904\u7406 reliablely \u751f\u6210\u3002\u5927\u591a\u6570\u7528\u6237\u66f4\u559c\u6b22\u63cf\u8ff0\u626b\u63cf\u5bf9\u8c61 geometry \u7684\u66f4\u9ad8\u6548\u548c\u66f4\u76f4\u89c2\u7684\u8868\u793a\uff0c\u5e94\u8be5\u63cf\u8ff0\u4e3a compact \u96c6\u5408 geometric  primitives \u53ca\u5176\u62d3\u6251\u5173\u7cfb\u3002 roughly speaking, there are two ways to abstract a point cloud into a polyhedral model: either find the planar surfaces and intersect them to find the edges and corners (e.g. , Schnabel et al. 2007 ; Fang et al. 2018 ; Coudron et al. 2018), or directly find the salient corner and/or edges (e.g. , Jung et al. 2019).\tThis paper introduces PC2WF, a neural network that turns 3D point clouds into a wireframe model. PC2WF encodes each point into a feature vector and uses them to predict the candidate corners. After that, line proposals are generated by connecting pairs of corners, and the point features along each line are pooled into its confidence value. By pruning the proposed lines, PC2WF generates the final wireframe represesntation.""610\tThis paper discusses the performance of stochastic gradient descent (SGD) and its competitors, adaptive gradient methods, in terms of optimization efficiency and deep learning applications. It also examines the relationship between SGD and other optimization methods, such as gradient projection and Hessian-free optimization. The paper provides a detailed analysis of the performance of SGD under different smoothness and convexity conditions, and shows that it can achieve the minimax optimal convergence rate for both convex and nonconvex optimization problems. However, despite the theoretical minimax optimality of SGD, adaptive gradient methods have become the methods of choice for training deep neural networks, and have received a surge of attention recently. The paper also discusses some of the challenges and opportunities in using adaptive gradient methods in deep learning.\tThis paper studies gradient-based stochastic optimization algorithms which incorporate (estimates of) the noise statistics in the adaptive stepsize design. Starting from the standard analysis of SGD with adaptive steps (Thm 1) the authors show in Cor 1 how, using a second-moment-dependent learning rate, one can \u201caccelerate\u201d (see comment later) the convergence of SGD. Next, the authors show (Thm 3) that one can recover a similar result by estimating the second moment using an exponential moving average.""611\tThis paper provides an overview of the performance of stochastic gradient descent (SGD) in machine learning, highlighting its theoretical optimality for both convex and nonconvex optimization problems under certain conditions. The paper also discusses the recent attention on adaptive gradient methods, which have become the preferred approach for training deep neural networks. The paper also discusses the comparison between SGD and other optimization methods such as the gradient-based learning rate method (GLR) and the fixed-step size method (FSM). The paper\u6700\u540e presents a summary of the main findings and future directions.\tThis paper studies the problem of stochastic optimization where the gradient noise process is non-stationary. Based on a general convergence results based on a general sequence for the second moments of the stochastic gradient norms and a general stepsize sequence, the authors propose to use an online estimation procedure for the gradient norm second moments, in order to mimic the behavior of the ``idealized'' stepsize sequence. Finite-time convergence rates are established for the algorithms with adaptive stepsize, leading to an acceleration effect in certain regimes for the non-stationarity.""612\tThis paper discusses the performance of stochastic gradient descent (SGD) in machine learning, specifically highlighting its ability to achieve the minimax optimal convergence rate for both convex and nonconvex optimization problems. It also discusses the theoretical minimax optimality of SGD, but emphasizes the importance of controlling gradient noise in adaptive gradient methods for deep neural networks. The paper also discusses the practical benefits of using these methods, such as their computational efficiency and the ability to train complex models with large amounts of data.\tThe objective of the paper is to provide a theoretical justification for the value of using adaptive learning steps. The paper presents two results. The first is essentially of theoretical interest, and assumes that the noise level indicators defined in Eq. (1) [but which are difficult to understand at this level of the paper] are known. The second is more practical: it shows that a variant of the RMSprop algorithm achieves the same results as the \"theoretical\" algorithm.""613\tThis paper discusses the issue of how to improve translation performance using neural machine translation (NMT) models, where the explicit word alignment model, which was an essential intermediary result in the training of most statistical machine translation (SMT) models, seems becoming increasingly obsolete. The paper suggests that the attention mechanism of NMT systems can take over the word alignment model of SMT systems and improve translation performance with the guidance of known word alignment. The paper also discusses the limitations of the word alignment information extracted from the attention mechanism and suggests alternative ways to improve translation performance.\tThis paper proposes to integrate word alignment obtained from SMT into an NMT system. This is an exciting topic not only because it can help interpretability, but also because the same mechanism could be used e.g. for imposing a specific terminology in translations, something that was relatively easy to do with SMT but is much harder to achieve with NMT. The proposed method involves computing word-word alignment using existing SMT models (GIZA and FastAlign in the experiments) and integrating that information in the decoder of a transformer-based NMT model. Experiments on English-Romanian and English Korean show small improvement over a standard baseline.""614\tThis paper discusses the current state of neural machine translation (NMT) and how explicit word alignment, a step in the training process for statistical machine translation (SMT) models, may become obsolete in the context of NMT. The paper suggests that the attention mechanism of NMT systems takes over word alignment information from SMT systems, but the resulting word alignment information is far from gold alignment and performs much worse than automatic word aligner such as FastAlign orGIZA++. The paper also discusses the use of known word alignment in the NMT system to improve translation performance. By providing guidance on good enough known word alignment, the paper suggests that it may be possible to replace some words in the source sentence with semantically corresponding words in the target language\uff0c\u4ece\u800c improving translation performance.\tThis work proposes to incorporate word alignment information as a word substitution model. Basic idea is to jointly train a separate encoder using a cross entropy loss which predicts a source input sequence with words substituted by aligned target words. The learned representation is combined with a Transformer either by simple summation, gating or joint attention mechanism. Experimental results on Romanian/English and Korean/English tasks show very marginal gains over the baseline Transformer.""615\tThis paper discusses the issue of how to use prior word alignment in neural machine translation (NMT) systems to improve translation performance. NMT models have become the dominant approach to machine translation, but the explicit word alignment model, which is an intermediary result from the training of most statistical machine translation (SMT) models, seems becoming increasingly obsolete. Prior research suggests that the attention mechanism of NMT systems takes over the word alignment model of SMT systems, but the information extracted from the attention mechanism is far from gold alignment and even performs much worse than automatic word aligner such as FastAlign orGIZA++. This paper focuses on the use of known word alignment in the NMT system to improve translation performance by guidance of good enough known word alignment. It suggests that using prior word alignment in the NMT system can improve translation performance by replacing some words in the source sentence with semantically corresponding words in the target language.\tThe paper presents a strategy to integrate prior word alignments into NMT models. It is not clear the motivation for this in the NMT context, especially why the prior alignments are crucial information that is necessary to be given a-priori to the Transformer. Besides this, the description of the method and the discussion of related work is given, SMT methods are briefly mentioned but the usage of the idea in previous work, also SMT literature is necessary.""616\tThis paper discusses the challenges in understanding and developing efficient RL algorithms. It also proposes a new benchmark for RL that can help to clarify the workings of the algorithms and improve their performance. The paper argues that the lack of a unified benchmark for all RL problems is a major obstacle to understanding and improving RL algorithms. It also highlights the need for more research in understanding the underlying principles of RL and how to apply them to different environments. The paper\u6700\u540e proposes a new benchmark that can help to clarify the workings of RL algorithms and improve their performance.\tThe paper describes a new benchmark for evaluating reinforcement learning techniques (MDP-playground). It can be seen as a toolbox allowing to generate different MDPs with different characteristics. Each MDP will then be used to probe a particular ability of learning algorithms, resulting in a comparison of methods over multiple dimensions. Proposed dimensions are reward sparsity, stochasticity, delayed reward, etc....  In addition to this toolbox, the authors also evaluate some of the classical algorithms in the domain. ""617\tThis paper discusses the current state of RL and the challenges it faces in environments. It argues that the lack of a unified benchmark for RL and the diversity of benchmark types present a problem for understanding the low-level workings of RL algorithms. It proposes a new benchmark that can be used to distill difficulties for Markov Decision Process (MDP)-based environments and allows for more precise experiments.\tThis paper proposes a suite of benchmark tasks designed to test (and possibly debug) reinforcement learning algorithms. Deemed the MDP Playground, these environments are applicable to both discrete and continuous RL agents and allow tuning of various dimensions of complexity - reward delays, reward sparsity, stochasticity, etc. The authors demonstrate their framework by evaluating the performance of many well-known RL agents across a variety of these playground environments. Additionally they conduct similar experiments on Atari and Mujoco tasks and observe similar trends in agent performance when injecting noise, reward delays, and varying action max values. Finally, the MDP Playground is very quick to run and facilitates fast experimentation.""618\tThis paper provides an overview of RL (iridates learning) and its challenges. RL algorithms have successfully completed diverse tasks such as helicopterAerobatics, game-playing, and continuous control, but many of the underlying workings of RL are not well understood. This is caused by the lack of a unified benchmark for all RL problems. The paper proposes a benchmark that distills difficulties for MDPs and can be generalised across RL problems, which allows for more precise experiments.\tThis paper presents \"MDP Playground\", a family of procedurally generated MDPs that can be used to benchmark certain dimension of difficulty considered by the authors to be challenging to current RL algorithms.  The paper presents the effects of the various perturbations to the MDP on state-of-the-art learning algorithms and discusses particular dimension of interest in the paper.  A full and exhaustive analysis of results is presented in the appendix.  The \"MDP Playground\" is slated to be open-sourced so that the community can benchmark against it.""619\tThis paper discusses the concept of calibration in supervised machine learning and its importance for evaluating the accuracy of a learned predictive model. It explains how a model's confidence in its predictions matches the correctness of these predictions can be used as a measure of calibration, and provides examples of different types of models and their corresponding calibration measures. The paper also discusses the limitations of modern deep neural networks and their poor calibration, which raises questions on their reliability as predictive models.\tThe manuscript discusses the side-effects (or drawbacks) of Isotonic regression and proposes an alternative approach for calibration in regression problems. The authors demonstrate the limitiation of Isotonoc regression such as nonsmooth PDFs and truncation of support under some constructions of the calibration dataset (line 2 in page 4) on a simple linear regression problem (last paragraph before section 4). On the light of these observations, they propose quantile regression (QR) which does not require an additional dataset.""620\tThe paper\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u7684 calibration\u6982\u5ff5\u3002\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c calibration \u662f\u6307\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u81ea\u5df1\u9884\u6d4b\u7ed3\u679c\u7684\u7cbe\u5ea6\u548c\u6b63\u786e\u6027\u7684\u4fe1\u5fc3\u7a0b\u5ea6\u3002\u4e00\u4e2a calibration \u597d\u7684\u6a21\u578b\uff0c\u4f1a\u5728\u9884\u6d4b\u7ed3\u679c\u7684 0.9 \u7f6e\u4fe1\u533a\u95f4\u4e2d\u5305\u542b 90% \u7684 true \u7ed3\u679c\u3002\u8be5 paper \u8ba8\u8bba\u4e86 calibration \u7684\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684 calibration \u95ee\u9898\u3002\u5176\u4e2d\uff0c\u8be5 paper \u5f15\u7528\u4e86 Brier\u3001 Murphy\u3001Gneiti \u7b49\u4eba\u5728 meteorology \u4e2d\u63d0\u51fa\u7684 calibration \u6982\u5ff5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u5982\u4f55\u5e94\u7528\u8fd9\u4e9b\u6982\u5ff5\u3002\u6700\u540e\uff0c\u8be5 paper \u5f97\u51fa\u7ed3\u8bba\uff0c\u5373\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684 calibration \u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u7f6e\u4fe1\u533a\u95f4\u6765\u8861\u91cf\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684 calibration \u95ee\u9898\u3002\tThe paper discusses a new calibration mechanism for regression models which produce better model prediction and uncertainty estimates. In Section 3, the paper first discusses some properties and drawbacks of the approach based on isotonic regression in Kuleshov et al., 2018 which uses a post-hoc calibration dataset after model fitting for calibration. Section 4 discusses a new approach based on regularization to achieve implicit regression calibration during model training instead of using a post-hoc processing approach. Section 5 is devoted to experimental results supporting the theory outlined in Sections 3 and 4.""621\tThis paper discusses the concept of calibration in supervised machine learning. It defines what it means for a learned predictive model to be calibration, and provides examples of how to evaluate the calibration of a model. The paper also discusses some of the known issues with modern deep neural networks, including their poor calibration. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c including improving the calibration of deep neural networks and developing new machine learning algorithms that are more calibration-friendly.\tThe authors consider the problem of learning quantile calibrated regressions model. A probabilistic regression model is a model that, given an input, outputs a distribution over possible scalar values. A quantile calibrated model is one is such that, for all quantiles $p$, the probability over $X, Y$ that the model predicts $Y$ is in the $p$th quantile is equal to $p$. Machine learning models are not usually calibrated after standard training, so the authors consider a regularization approach to improve the calibration of the model during training.""622\tThis paper discusses the problem of learning representations of spatial environments, perceived through RGB-D and inertial sensors, such as in mobile robots, vehicles, or drones. The paper presents a deepSequential generative model and demonstrates how it can be used to learn such representations from realistic spatial data and dynamics. Existing spatial generative representations are limited to simulated 2D and 2.5D environments. The paper also discusses the challenges and potential applications of learning such representations, including the need for efficient data processing and the ability to\u5e94\u5bf9\u590d\u6742\u7684 spatial dynamics.\tThis paper describes a Deep Variational Bayes Filter (DBVF) for Deep-Learning based SLAM in 3D environments. It builds upon similar work for 2D environments in [Mirchev et. al. 19], and learns a full 3D RGBD occupancy map and a sequence of 6 DoF poses (localization) using raw stereoscopic camera data. Differentiable ray-casting and an attention model is described to access the learnt global map to give a local map and an expected observation - using an emission model from the current pose and local map. A transition model describing the evolution of the dynamics of the agent is also learnt. A variational approximation of the actual posterior (of the sequence of poses and the map, given the sequence of observations) is learnt by optimizing the standard ELBO equation from Variational Bayes. Such deep generative models, once learnt (in an unsupervised way) for an environment, allows one to hallucinate a sequence of poses and observations, given the learnt map and control inputs. This allows downstream robotic control tasks like environment exploration and path planning to be integrated into the model. Experiments on a simulated dataset with a flying drone in a subway and living room environments demonstrate good SLAM performance (that approach traditional methods): bird's eye view projections of the 6 DoF poses and the emitted maps closely match the ground truth poses and the occupancy grid. This is a well-written paper that represents a good step up from [Mirchev et. al. 19] to formulate a DVBF with realistic RGBD data streams. The authors mention that the computational times for this method is still far from conventional SLAM techniques - an actual quantification of the time taken during inference would be useful.""623\tThis paper discusses the problem of learning representations of spatial environments, perceived through RGB-D and inertial sensors, such as in mobile robots, vehicles, and drones. It presents a deepSequential generative model for state estimation in such environments, and discusses the challenges of training such models from realistic spatial data and dynamics. The paper also highlights the potential of such models to serve as world models or environment simulators, and demonstrates how they can be trained using a variety of inference techniques.\tThis paper addresses the problem of dense RGB-D SLAM. The key idea is to formulate the problem as a deep state-space model and infer the state of the latent variables (i.e. pose and geometry) using variational inference. The experiments demonstrate that the method performs well in a challenging quadcopter dataset. However, the advantages of the approach are not demonstrated. ""624\tThis paper discusses the problem of learning representations of spatial environments, perceived through RGB-D and inertial sensors, such as in mobile robots, vehicles, and drones. The paper presents a deepSequential generative model for such environments, which can serve as a world model or environment simulator. The model is trained using realistic spatial data and dynamics, and has shown impressive performance on a variety of simulated control tasks. However, the existing spatial generative representations are limited to simulated 2D and 2.5D environments. The paper also discusses the state estimation problem in spatial environments, and proposes a solution for that problem.\tThis paper presents a novel learning-based visual-inertial odometry algorithm. The algorithm simultaneously reconstructs the world map as well as the states of the agent from the stereo RGBD sensors.  The world is modeled as an occupancy grid with color. A graphical model with attention mechanism and ray casting is used to model how the world and the agent state renders the RGBD sensor data. ELBO is used to optimize the model. The technical details look sound to me.""625\tThis paper discusses the use of interactive game environments for developing agents that can learn grounded representations of language for autonomous decision making. The key objective in these learning setups is for the agent to utilize feedback from the environment to acquire linguistic representations (e.g. word vectors) that are optimized for the task. Research has shown that using word vector representations can capture the meaning of words in context and help agents navigate new environments with previously unseen entities or dynamics. The paper also discusses recent methods for grounded language description, including using transition and reward dynamics to guide learning.\tThis paper is studying the problem of learning to interpret manuals / textual information about the task, with the goal of faster learning and generalization in RL. Unlike recent prior work (Narasimhan et al 2018, Zhong et al 2020) where the entities in the environment are already partly grounded to text (e.g. by representing them as their textual description), the agent here needs to learn the mapping between entities and corresponding text. In order to study this problem, the authors use a new environment and dataset of natural language descriptions, as well as propose a self-attention model that matches entities to relevant sentences. The proposed model performed comparable to a partly grounded policy (as in Narasimhan et al 2018).""626\tThis paper provides an overview of the current state of the art in developing language-based agents for autonomous decision making using interactive game environments. The paper discusses the key objectives of these learning setups, including utilizing feedback from the environment to acquire linguistic representations that are optimized for the task, and exploring the use of transition and reward dynamics to guide learning. The paper also presents recent research in this area, including methods for grounded representation learning and the use of word vector techniques to capture the meaning of words in context. Finally, the paper concludes by highlighting the potential applications of language-based agents in fields such as autonomous driving and virtual reality.\tThe paper considers the task of training an agent to act following a manual expressed in natural language. The manual describes the roles and the behaviors of the entities in the environment. Each entity can be the goal, the message or the enemy and it can also be either fleeing, chasing or not moving. The agent has to bring the message to the goal while avoiding the enemy. A model called EMMA is proposed to change entity representations based on the manual, thereby making the agent aware of the entities\u2019 roles. It is shown that EMMA is more effective than simple baselines.""627\tThis paper discusses the use of interactive game environments for developing agents that learn grounded representations of language for autonomous decision making. The key objective in these learning setups is for the agent to utilize feedback from the environment to acquire linguistic representations (e.g. word vectors) that are optimized for the task. The paper provides an overview of the literature on this topic, including the use of game environments for language learning, the importance of using word vectors, and the recent research ongrounding language descriptions to the transition and reward dynamics of an environment. The paper also discusses the challenges and potential limitations of this approach, such as the need for large-scale data collection and the need for more advanced machine learning algorithms.\tNatural language grounding is an interesting research direction and has attracted many researchers in recent years. Previous work mainly considered grounding the text to image objects. This paper considers collaboratively to learn \u201centity\u201d representations and natural language explanations with a reinforcement learning framework. Specifically, a multi-modal attention network is proposed to model the interaction between the entity representation and the text descriptions. The entire framework is trained over multiple games in a multi-task manner. Experiments are conducted on a newly designed benchmark. The proposed RL framework achieves reasonable performance in domain games (training & test are from the same games) and also has a strong zero-shot generalization to unseen games and \"entities\" (thanks to the parameter sharing and multi-task learning). Besides, the newly released dataset may facilitate future research in natural language grounding.""628\tThis paper discusses the use of model-free deep reinforcement learning (RL) in a wide range of problem domains, including teaching computers to control robots and playing sophisticated strategy games. It also discusses the state-of-the-art policy gradient algorithms that use neural networks as function approximators in the actor-Critic framework. The paper\u8ba8\u8bba\u4e86\u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u7b97\u6cd5\u7684 conceptual framework \u548c\u5728\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u7684\u6700\u5927 gain \u4e4b\u95f4\u5b58\u5728 discrepancies\uff0c\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb value \u51fd\u6570\u7684\u5b66\u4e60\u3002\u8fd8\u8ba8\u8bba\u4e86\u9650\u5236\u51fd\u6570\u8fd1\u4f3c\u5668\u7c7b\u7684\u4e0d\u540c\u5047\u8bbe\uff0c\u7136\u540e\u63d0\u51faCritic\u6846\u67b6\u6765\u6539\u8fdb\u5b66\u4e60\u3002\tThis paper presents AVEC, a new critic loss for model-free actor-critic Reinforcement Learning algorithms. The AVEC loss can be used with any actor-critic algorithm, with PPO, TRPO and SAC being evaluated in the paper. The loss builds on the mean-squared-error, and adds a term that minimizes $E_s [f_{\\\\phi}(s) - \\\\hat{V}^{\\\\pi_{\\\\theta_k}}(s) ]$. The addition of that extra term is motivated by recent research on the stability of actor-critic algorithms, and the benefits obtained by the AVEC loss are empirically demonstrated in numerous environments, with AVEC+PPO, AVEC+SAC and AVEC+TRPO.""629\tThis paper discusses model-free deep reinforcement learning (RL) and its applications in a wide range of problem domains, including teaching computers to control robots, playing sophisticated strategy games, and playing games with human players. The paper also discusses the state-of-the-art policy gradient algorithms that use neural networks as function approximators in the actor-Critic framework. While these methods have shown great performance in continuous control tasks, there are discrepancies between the concepts and practice in order to improve the learning of value functions. Research on this topic has often restricted the class of function approximators and proposed new critic formulations to improve the performance of the algorithms.\tThe paper explores an alternative loss function for fitting critic in Reinforcement Learning. Instead of using the standard mean squared loss between critic predictions and value estimates, the authors propose to use a loss function that also incorporates a variance term. The authors dub the approach AVEC. The authors combine their approach with popular RL algorithms such as SAC and PPO and evaluated on the standard benchmarks for continuous control.""630\tThis paper discusses the use of model-free deep reinforcement learning (RL) in a wide range of problem domains, including teaching computers to control robots and playing sophisticated strategy games. The paper also discusses the state-of-the-art policy gradient algorithms that use neural networks as function approximators in the actor-Critic framework. While these methods have shown great performance in continuous control tasks, there are discrepancies between what motivates the conceptual framework of these algorithms and what is implemented in practice to obtain maximum gains. The paper also discusses some of the challenges and limitations of using model-free RL in practice.\tThe paper proposes a simple and elegant idea for changing the value function objectives in deep RL and demonstrates reasonable empirical evidence of it's potential usefulness.  The authors also provide a clearly articulated intuitive motivation and provide experiments to support the proposal.  The idea complements several other algorithms and is therefore quite widely applicable (and easy to try). The analysis of the experiments is also quite interesting and clearly presented. ""631\tThe paper discusses the state of deep learning and overparameterized neural networks. It presents some interesting findings and insights, such as the observation that overparameterized networks can achieve impressive generalization performance despite having large amounts of parameters. The paper also discusses the challenges of training overparameterized networks and the recent research on how to improve their generalization performance. The paper also suggests some potential applications of overparameterized neural networks in fields such as machine translation and natural language processing.\tThis paper aims to empirically explore the depth dependence of overparameterized networks. The authors study fully-connected networks trained on a synthetic dataset consisting of random Gaussian inputs, with the label a simple function of the input. In one case, for \"local\" labels, the label is the parity of a product of a subset of the components. In the other case, for \"global\" labels, the label is a sum of such products of subsets with coverage over all the components. Broadly, the authors find that deeper MLPs are better able to learn the local labels, but shallower MLPs are better able to learn the global labels. Finally, the authors compare these results to the infinite width NTK and show that the NTK does not at all capture the behavior of the finite networks.""632\tThis paper discusses the phenomenon of overfitting in deep learning and the ways to address it. It also analyzes the relationship between overfitting and the number of parameters in a deep network. The paper provides evidence that suggests that the generalization performance of an overparameterized network can be improved by adjusting the hyperparameters of the model. Additionally, it suggests that using prior knowledge or additional features can help prevent overfitting.\tThis paper analyzes overparametrized networks evaluating how depth and width affect the generalization performance of the network. A set of experiments is designed in which labels are determined either by local or global interactions among the features, and generalization is observed for different values of width and depth of the network. NTK is also considered as a limit case of a network with infinite width.""633\tThis paper discusses the overparameterization of deep learning neural networks and the ways in which they can achieve impressive generalization performance. It also examines the implications of overparameterization for the design of future deep learning algorithms. The paper begins by discussing the success of deep learning in various AI tasks and the limitations of classical learning theory. It then explains how overparameterization can occur in deep learning and how it can lead to impressive generalization performance. The paper then examines the learning dynamics and the generalization power of heavily overparameterized wide neural networks. It is reported that training of an overparameterized network easily achieves zero training error with high accuracy. The paper also discusses the potential implications of overparameterization for the design of future deep learning algorithms.\tRecent theoretical study on the training of neural networks has introduced an important kernel function called neural tangent kernel. This paper studies the training of deep ReLU networks and compares it with the training directly using NTK by conducting experiments on synthetic data. Based on the experimental results, the authors conclude that deeper networks perform better on certain datasets whose labels are more \u201clocal\u201d, while shallower networks are better at more \u201cglobal\u201d labels. Moreover, the authors observed that finite-width networks have better generalization than NTK. ""634\tThis paper discusses a result that demonstrates the ability of representation learning to overcome the \u03a9(1T) barrier for few-shot learning under the i.i.d. task assumption. The result is obtained for the linear representation class and the dimension of the representation is k(d), where d is the ambient input dimension and k is the dimension of the representation. The paper also discusses a further extension of the result to handle a general representation function class and demonstrates the advantage of representation learning in several different settings. Finally, the paper shows that representation learning can fully utilize all n1T samples from source tasks in high-dimensional linear regression and neural networks.\tThis paper studies the benefit of few-shot learning for sample complexity, when all the tasks (both source and target task) share the same underline representation. Under some assumptions on the data and tasks, this paper improves the previous result based on the iid task assumption and shows that they can utilize all source data. The considered models include linear model (both low dimensional and high dimensional representation) and two-layer neural network.""635\tThis paper discusses a result that allows for the efficient use of all n1T samples from a source task in linear representation learning under the i.i.d. task assumption. The result is that all samples from the source tasks can be pooled together for representation learning, even if the representation is high-dimensional and has a limited capacity. The paper also discusses a possible extension to the result to handle a general representation function class and demonstrates the advantage of representation learning in both high-dimensional linear regression and neural networks.\tThe paper aims at justifying the success of few shot learning methods that work based on finding a shared representation among a number of tasks. A serious theoretical challenge is that, even if we assume such a representation exists (and belongs to a predefined class of functions with controlled capacity), we would still need to assume something that connects the source tasks with the target task. Previous work has considered \"i.i.d. tasks\", however, the obtained bounds were not natural in the sense that we don't have the usual decrease in the error as we increase the size of the training set of the source tasks. Under a different set of assumptions, the authors show that, in a sense, one can \"fully\" exploit the training data from the source tasks. ""636\tThis paper discusses a result that bypass the \u03a9 ( 1T ) barrier for few-shot learning under the i.i.d. task assumption. The result is for the linear representation class, where d is the ambient input dimension and k ( d ) is the dimension of the representation. The paper also extends the result to a general representation function class and obtains a similar result. The paper then discusses the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm). The paper demonstrates the advantage of representation learning in both high-dimensional linear regression and neural networks, and show that representation learning can fully utilize all n1T samples from source tasks.\tThis paper presents some new theoretical insights into a two-layer (linear or non-linear) network based meta-learning framework for dimension reduction and few-shot linear regression. In the considered problem setting, the hidden layer for feature extraction is assumed to be shared across the training and test tasks, and the output layer is optimized in a task-specific way with quadratic loss. For well-specified low-dimensional linear representation learning models, statistical analysis shows that when the tasks are sufficiently divergent, the excess risk of the target task estimator has a near-optimal rate of convergence, up to a near-optimal statistical error of meta-training. The corresponding results for well-specified high-dimensional linear representation and neural networks have also been derived under additional regularization conditions.""637\tThis paper explores the issue of how to evaluate the robustness of deep neural networks (DNNs) against adversarial examples. The reliability of DNNs has been undermine by small changes to inputs that deceive the network, making it difficult to recover the network\\'s performance. To address this issue, the paper proposes analyzing input features, such as the input values, to identify those that are robust to adversarial examples. The paper also discusses existing approaches to evaluating network robustness and\u6307\u51fa\u4e86\u5728\u8bc4\u4ef7DNN\u7684 robustness\u65f6\u5b58\u5728\u7684\u6f5c\u5728\u969c\u788d\uff0c\u5373DNN\u5bf9\u8ddd\u79bb-based neighborhood\u7684\u9c81\u68d2\u6027\u53ef\u80fd\u5b58\u5728\u7740\u56fa\u6709\u7684\u9650\u5236\u3002 The paper also includes a discussion of potential future directions in this field.\tThe present paper proposes to consider features derived from PCA for the purposes of adversarial attack and defense. They argue that, based on these features, they can verify larger neighborhoods and provide stronger attacks. The neighborhoods are based on the ERAN verifier and the attacks are in comparison to a recent attack called AutoZoom. Defense performance is measured in number of images in the neighborhood, and attack performance in terms of L2 distance.""638\tThis paper explores the problem of evaluating the robustness of deep neural networks (DNNs) to adversarial examples. The reliability of DNNs has been undermine by small perturbations to inputs that deceive the network, and a key step in recovering this reliability is identifying input features that are robust to adversarial examples. Existing work focuses on evaluating the robustness of DNNs to input values, the lowest-level features, using various approaches such as analyzing the distance between inputs and identifying provably robust neighborhoods. However, the diameter of these robust neighborhoods is often very small, which suggests that there may be an inherent barrier to evaluating the robustness of DNNs using distance-based methods. This paper explores this issue and proposes a new approach to evaluating the robustness of DNNs that overcome this barrier.\tThe authors study the problem of adversarial robustness, aiming to find regions of the input space for which a classifier is robust. Instead of the standard approach of defining a neighborhood around each data point based on some $\\ell_p$-norm, they use PCA to identify directions along which the model is robust or brittle. They then use these methods to identify large regions of input space for which models are robust and, in a complementary direction, to craft imperceptible adversarial examples with few model queries.""639\tThis paper discusses the problem of adversarial examples in deep neural networks and how to approach it. The paper argues that existing work focuses on evaluating the robustness of deep neural networks by analyzing input values, rather than identifying input features that are robust. This can lead to a lack of sufficient understanding of the true level of network robustness. To overcome this, the paper proposes a new approach to evaluating network robustness, which analyzes the distance-based neighborhood of input values. The paper also discusses some of the limitations of this approach and how they may affect the ability to recover network reliability.\tThe overall quality of the paper is good. This paper proposed a feature perturbation procedure, as a comparison to the commonly used perturbation to the original input data. Given access to a feature mapping and a black-box classifier, the proposed procedure is able to select the most robust/weak features. This then can be used for two important tasks: to determine a robust neighborhood for a data point using the robust features and to design adversarial examples using the weak features. For the first task, the feature-based robust neighborhood proposed by this paper is shown by experiments to contain far more points than the traditional input-based neighborhood. For the second task, the feature-based adversarial examples require less query to the black-box classifier and have less distortion from the original data points compared with other competitive methods, and thus are more human-imperceptible. These characteristics make the procedure appealing.""640\tThis paper proposes a method to avoid human assumptions on task similarities by training a separate policy for each task. The obvious solution is to train a separate policy, but this leads to a large amount of experience being required to learn the desired behaviors. Instead, the paper proposes to use a method called implicit meta-learning, which allows for implicit relations between tasks by learning from past performances. This approach can lead to more impressive results without the need for explicit modeling of task similarities. The paper also discusses the potential limitations of this approach and\u63d0\u51fa further research directions.\tIn this paper, the authors present a new multi-task reinforcement learning (RL) algorithm. Since In general, the relationships between tasks is unknown a-priori, directly applying classical multi-task learning approaches that assume all tasks are related, could suffer from negative transfer. The authors propose to cluster tasks into disjoint groups: The proposed algorithm iterates through steps of assigning tasks to specific policies and training each policy only based on the respective assigned tasks (clusters). In the experiments, the authors compare their algorithm with two single-task learning baselines (SP: a single policy for all tasks and PPT: a policy per task) and a recent multi-task RL algorithm of Eramo et al. 2020 on Pendulum, Bipedal Walker, and Atari problems.""641\tThis paper proposes a method to avoid human assumptions on task similarities by training a single policy that acts across all tasks. An obvious solution is to train a separate policy for each task, but this leads to a large amount of experience being required to learn the desired behaviors. The paper suggests using a technique called meta learning to implicit relations between tasks, which can help improve the performance of the single policy. Additionally, the paper proposes a new concept called task-task similarity, which defines a relationship between tasks that can help avoid human assumptions on task similarities. The method is shown to improve the performance of a neural network in a dataset, indicating the potential applications of the approach in other settings.\tThis paper proposes a multi-task RL algorithm that leverages unsupervised task clustering. The authors propose to initialize a number of policies, cluster each task based on its performance on different policies, and train each policy with data coming from tasks within the cluster. The paper shows that such kind of an EM style clustering can lead to better performance than single-task training and be more sample efficient more training each task independently on both tabular settings, continuous control experiments, and Atari. ""642\tThis paper proposes a new approach to avoiding human assumptions on task similarities when training multiple policies for a given task. The approach is to train a single policy that learns to achieve the goals of all the tasks by implicit learning from a shared set of meta-parameters. This approach is easier and faster than training separate policies for each task, and can also lead to more general and accurate learning outcomes. The paper provides an example of how the approach can be implemented in a real-world setting, and discusses its potential applications in various fields.\tThe authors propose to approach multi-task RL problems in which tasks may differ considerably in transition functions/dynamics and reward functions as well as in the action space through task clustering. Specifically, tasks are modeled as belonging to separate task clusters defined by the return obtainable by individual policies i. All policies are evaluated on a single task and the relative cumulative discounted rewards over some iterations determines the assignment of tasks to policies. Simulations show the advantage in terms of number of training iterations on several tasks compared to a selection to other related algorithms. ""643\tThis paper explores the use of unsupervised representation learning to extract informative low-dimensional representations from real-world time series data. The paper presents a review of the existing literature on unsupervised representation learning and its applications in time series settings. It argues that time series data is high dimensional, complex, and has unique properties that make it challenging for traditional supervised learning methods. However, unsupervised representation learning can extract informative low-dimensional representations that are more generalizable and robust, without the need for explicit supervision. The paper provides a overview of the key concepts and algorithms in unsupervised representation learning, and\u8ba8\u8bbas the limitations and potential future directions of this research.\tThis paper proposes a self-supervised encoder-discriminator based framework for embedding the multivariate time series into a compact fixed dimensional representation. The approach dubbed Temporal Neighborhood Coding (TNC) leverages the concept of a neighborhood in time (with stationary properties), and learns time series representations by ensuring the distribution of neighboring signals is distinguishable from the distribution of non-neighboring signals, in the encoding space. Empirical evidence is provided that such embedding of time series results in clusters of higher quality, as well that use of such obtained representations for supervised tasks outperforms few competitor (unsupervised) approaches. ""644\tThis paper discusses the use of unsupervised representation learning to extract informative low-dimensional representations from real-world time series data. The paper highlights the benefits of this approach, including more generalizable and robust representations that are less specialized for solving a single supervised task. Unsupervised representation learning has been well studied in domains such as vision and natural language processing, but has been underexplored in the time series setting. The paper provides an overview of the literature and discusses existing frameworks for time series analysis, including the use of machine learning algorithms and techniques. The paper also discusses potential future directions and limitations of the approach.\tThe authors propose a novel unsupervised encoding scheme for time series. Utilizing a statistical test for non-stationarity, the authors derive a Temporal Neighborhood Coding (TNC) scheme and combine it with ideas from Positive-Unlabeled (PU) learning to learn informative hidden representations of time series windows. The representations are evaluated in terms of how well they can be clustered and how much they influence classification performance on three data sets. The supreme performance was demonstrated when comparing to the state of the art methods and a $k$NN (for classification) baseline. Furthermore, the authors illustrate how the learnt representations remain interpretable as long as the encoding network is reasonably small. ""645\tThis paper explores the use of unsupervised representation learning to extract informative low-dimensional representations from real-world time series data. The paper highlights the benefits of this approach, including its ability to generalize and be robust to sparse labeling, without the need for explicit supervision. Unsupervised representation learning has been well studied in domains such as vision and natural language processing, but has been underExplore in the time series setting. The paper provides an overview of the literature and previous work in this area, and discusses the potential future directions of this field.\tThe paper proposes an unsupervised representation (embedding) learning method for time-series. While unsupervised representation learning has been extensively studied and shown good performance in fields like NLP and vision, it is relatively new to the time-series community. This paper, in contrast to recent work (CPC and Triplet-Loss), has the following differences:""646\tThis paper presents an alternative design for neural networks, which\u5b83\u5c06\u6574\u4e2a\u6a21\u578b\u62c6\u5206\u4e3a\u591a\u4e2a\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u53ef\u4ee5\u72ec\u7acb\u5730\u8d21\u732e\u7279\u5f81\u5e76\u4e0e\u5176\u4ed6\u6a21\u5757\u4ea4\u4e92\u3002\u8fd9\u79cd\u8bbe\u8ba1\u53ef\u4ee5\u5141\u8bb8\u6a21\u578b\u5728\u4e0d\u65ad\u5b66\u4e60\u5e76\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002\u8be5\u8bbe\u8ba1\u8fd8\u5141\u8bb8\u6a21\u5757\u4e4b\u95f4\u8fdb\u884c\u91cd\u7528\uff0c\u5373\u4f7f\u67d0\u4e9b\u6a21\u5757\u5177\u6709\u590d\u6742\u7684\u4ea4\u4e92\u548c\u53cd\u9988\u673a\u5236\u3002\u8be5\u8bbe\u8ba1\u5728 Transfer Learning \u548c\u77e5\u8bc6\u8fc1\u79fb\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6bd4\u4f20\u7edf\u7684\u6574\u6a21\u578b\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3002\tThis paper presents a method of composing stacked neural network blocks by linearly combining module \"template\" weights.  The work extends \"isometric networks\", in which each block in the stack has the same operational structure, by parameterizing each block using a mixture of the weights from a bank of K blocks (\"templates\").  In multitask learning experiments, the mixtures naturally learn to share common components between tasks, while learning task-specific components where needed.  Further experiments describe behavior of the system applied to transfer learning and domain adaptation scenarios.""647\tThis paper proposes a simple alternative design for neural networks, which allows for module reuse and parameter sharing across layers. The design breaks up monolithic models into interacting modular components, rather than localization them to particular processing stages. The paper discusses the limitations of traditional designs that prevent parameter sharing across layers, and suggests a simple alternative that allows for this. It also discusses the potential benefits of this design, such as allowing for continually growing the model and preventing catastrophic forgetting.\tThe authors propose a method to design network architectures as a combination of network components. Their idea consists in imposing an architecture that is a sequence of identical network blocks, and learn a series of templates, which provide an instantiation of model weights for one such network block. Model weights are then estimated for a specific task as a linear combination of a set of template weights. ""648\tThis paper discusses the limitations of traditional designs for neural networks, which break up monolithic models into interacting modular components. The paper proposes a simple alternative design that represents an entire model as a mixture of modules, which allows for parameter sharing across layers and allows for the use of Recurrent components and feedback loops in complex decision making processes. The paper also discusses the advantages and limitations of this design.\tThis paper considers \u201cmodular multi-task learning\u201d where parameters in each layer/task are generated as a (layer/task-specific) linear (mixture) combination of a common pool of parameters. Exploring this idea, several observations are made: (1) single task isometric model performance on ImageNet can be improved, (2) Multi-task learning is supported and parameter sharing (selecting same mixture components) emerges in early layers, with specialisation emerging in later layers. With multi-domain learning, the opposite effect is achieved with domain-wise specificity arising in earlier layers, and sharing in later layers. (3) Parameter-efficient transfer learning is supported by fine-tuning the task-specific weights for new tasks. (4) Parameter-efficient domain adaptation is supported by optimising the task-specific weights for new domains. ""649\tThe variational autoencoder (VAE) framework is a popular approach to achieve generative modeling in the field of machine learning. It involves training a model to approximate the true posterior of observation data, which is achieved by a joint training of an encoder and decoder. The latent space of the VAE is assumed to follow a prior distribution, which in this case is a Gaussian distribution. The generation of a new data sample is achieved by sampling the latent space and passing it through the decoder. The variance of the decoder output is typically modeled as an isotropic matrix \u03c32xI with a scalar parameter \u03c32x \u2265 0. This setup is also known as the Gaussian VAE. The VAE has been shown to be effective for various tasks, such as image generation, language modeling, and time series forecasting.\tThe paper considers Gaussian VAEs and their tendency to suffer from posterior collapse. In particular, the authors analyse the impact of the usually fixed covariance $\\sigma_x$ of the decoder Gaussian on the learned encoder variance. They show that the former can be seen as a regulariser for the latter and therefore impacts the \"smoothness\" of the encoder. The authors hypothesize that a large value of $\\sigma_x$ causes posterior collapse as a consequence.""650\tThe variational autoencoder (VAE) framework is a popular approach to achieve generative modeling in the field of machine learning. In this framework, a model that approximates the true posterior of observation data is learned by a joint training of encoder and decoder, which creates a stochastic mapping between the observation data and the learned deep latent space. The latent space is assumed to follow a prior distribution, and the generation of a new data sample can be done by sampling the latent space and passing the sample through the decoder. It is common to assume that both the prior on the latent space and the posterior of the observation data follow a Gaussian distribution. The setup known as the Gaussian VAE is commonly used, and the decoder output is usually modeled as an isotropic matrix with a scalar parameter \u03c32x\u22650. This paper presents an overview of the VAE framework and its applications to various fields, including image and speech generation, as well as deep learning.\tThis paper studies the Gaussian VAE and figures out that the decoder variance regularizes the VAE and affects the model smoothness, and an inappropriate estimation of this parameter would raise posterior collapse, which is supported by theoretical analysis and empirical demonstrations. Hence, this paper then proposes an ELBO with adaptive decoder variance to avoid oversmoothing the model. Overall, the idea is interesting and provides some new insights for our community. The major concerns regarding this paper are listed as below.""651\tThe variational autoencoder (VAE) framework is a popular approach to achieve generative modeling in the field of machine learning. It involves training a model to generate new data samples by mapping the observation data to a deep latent space and then sampling from the latent space. The prior on the latent space is assumed to follow a Gaussian distribution, and the decoder output is typically modeled as an isotropic matrix with a scalar parameter \u03c32x. The main goal of the VAE is to achieve a high degree of flexibility and generate data samples that are difficult to generate using traditional means, such as drawing from a known distribution. The VAE has been widely used in fields such as image generation, natural language generation, and speech\u751f\u6210.\tThis paper analyses the \"posterior collapse\" phenomenon observed in training latent variable models (in particular Variational Auto-Encoders), and propose a new training objective to remedy the problem. The theoretical analysis of the authors suggests that the posterior collapse is induced by an inappropriate choice of variance in the decoder distribution. The new objective they propose, the AR-ELBO, jointly optimises this variance along with the usual network parameters. The authors demonstrate that their objective yields relatively good results on image modelling, compared to other standard VAE methods.""652\tThis paper discusses the development of Variational Autoencoders (VAEs), a generative model that is inspired by the autoencoder. It covers the history of the model, the different variations that have been proposed, and the advantages and limitations of each. The paper also discusses how to handle missing data and heterogeneous data types in VAEs. The paper concludes by discussing future directions and potential applications of VAEs.\tThis paper proposed a deep generative model based on the non i.i.d. VAE framework in an unsupervised version. The model which combines a mixture prior in the local latent space with global latent space has three advantages: First, the latent space can capture interpretable features. Second, the model performs domain alignment. Third, the model can discriminate among their global posterior representations. Although this paper has mild improvement on the basic VAE structure, the model displays a good interpretability power, and the setup of the latent variables are illustrated reasonably in the paper.""653\tThis paper discusses the development of Variational Autoencoders (VAEs), a machine learning model that is used to learn embeddings of input data. The paper discusses a number of different variations of the VAE, including those that use latent mixture models priors, time-series models, and handle heterogeneous data types and missing data. The paper also discusses the benefits and challenges of Dropout, a technique that is commonly used in machine learning to prevent overfitting. The paper\u6700\u540e argues that inferring higher level dependencies can improve current approaches to machine learning.\tThis article introduces a VAE-based method for separating local variation factors from global variation factors in the data in an unsupervised manner. It achieves so by designing a graphical model with a mix of example-local and batch-shared variables, and training it using the ELBO. The article provide an detailed experimental analysis on MNIST and CelebA, and experimental evidence that all parts of the model (notably the discrete d variable) are relevant.""654\tThis paper discusses the history and development of Variational Autoencoders (VAEs), a generative model that has evolved into a vast amount of variants over the past few years. The paper covers the original proposal by Kingma & Welling (2013), the extensions to time-series models ( Chung et al., 2015), the use of deep hierarchical variational families ( Ranganath et al., 2016), and the handling of heterogeneous data types and missing data ( Nazabal et al., 2020). The paper also discusses the challenges and opportunities for improving the performance of VAEs, such as the issue of overfitting and the need for additional priors or constraints to prevent it.\tThis paper presents a novel deep generative model based on noni.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. The proposed model combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound. The proposed model is being evaluated in two tasks: (1) disentanglement, and (2) domain alignment.""655\tThis paper provides an overview of the key challenges and recent advances in computer vision, particularly in the field of low-dimensional vector representation learning. The paper discusses the importance of encoding visual information from pixel space into a lower-dimensional vector, the rich set of algorithms and architectures that have been developed to achieve this, and the common practice of explicitly training networks to map visual inputs to curated label spaces. It also highlights the importance of weakly supervised and self-supervised representation learning approaches, which can mitigate the need for supervision and achieve more efficient and effective learning. Finally, the paper provides an overview of recent advances in low-dimensional vector representation learning, including contrastive learning, transfer learning, and other approaches.\tThis paper proposes to improve upon unsupervised representation learning for various downstream vision tasks by leveraging human motion and attention (gaze) information. The authors collect a large spatio-temporal dataset with gaze and body motion labels for this task. They train a network to jointly predict the visual focus of attention in scenes and body motion besides visual instance recognition via an NCE loss to learn good visual representations. They show large improvements in accuracy of many different visual recognition downstream tasks with their approach versus the SOTA MOCO approach, which uses visual information only.""656\tThis paper provides an overview of the current state-of-the-art in deep learning for computer vision, specifically highlighting the use of encoding visual information from pixel space to a lower-dimensional vector as the core element of many modern deep learning-based solutions. The paper also discusses the common practice of explicitly training the networks to map visual inputs to curated label space, as well as recent approaches to mitigate the need for supervision, such as weakly supervised and self-supervised representation learning. The most successful approaches are contrastive learning-based methods such as (Chen et al., 2020c) and are becoming increasingly popular due to their ability to learn complex encodings of visual information and their ability to perform tasks such as object detection and  object recognition.\tThe main aim of the paper is to make use of human interaction/motion to learn a visual  representation that can be re-used for classic visual tasks such as depth estimation. The authors claim that by encoding interaction and attention cues in the self-supervised representation, the method can outperform visual-only state-of-the-art methods. To study the interaction element, the authors attach sensors like Inertial Movement Units (IMUs) to the limbs of subjects and monitor their reaction to visual events in daily life. The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants which include synchronized streams of images, body part movements, and gaze information.""657\tThis paper discusses the use of deep learning for computer vision, specifically highlighting the importance of encoding visual information from pixel space to a lower-dimensional vector. The paper provides an overview of the key algorithms and architectures that have been developed to enable this encoding, including explicit training to map visual inputs to curated label spaces and the use of weakly supervised and self-supervised representation learning approaches. The paper also discusses the challenges and limitations of using deep learning for computer vision, including the need for large datasets and the need for efficient training algorithms. Finally, the paper provides a summary of the main findings of the paper.\tThe paper uses a combination of visual, human gaze and human motion sensors to build representations that perform better on downstream tasks such as action recognition, physics prediction and depth estimation than representations extracted from solely visual input. The paper announces the release a new data set of aligned visual images, eye gaze fixations and IMU motion readings from test subjects walking around an environment. Representations are computed using three different forms of information simultaneously. Given a visual input, the system tries to predict the location of eye gaze in image frame coordinates, whether each of 6 groups of motion detectors are active or not (head, torso, legs, etc.) and the result of a more traditional auxiliary visual pretext task. In this work, the paper uses \u201cinstance discrimination\u201d where representations of augmented versions of a specific image are pushed close together in latent space and far away from augmentations of other images. Tests on diverse benchmarks show that the gaze and motion prediction improve over visual pretext tasks alone and that there is a small benefit to using both together, but it is not additive. The paper also shows the benefit of gaze and motion is present for two different visual auxiliary tasks.""658\tThis paper presents an overview of theSequential Learning Problem, a balancing act between retaining knowledge and adapting to new experiences that is central to lifelong learning. The paper argues that understanding the underlying mechanisms that influence this process is crucial for developing effective learning strategies. The paper also discusses the challenges that arise when training large models from scratch, such as the increased cost of training models like GPT-3. Finally, the paper provides an overview of existing literature onSequential Learning Problem and suggests areas for future research.\tThis work investigates the intriguing phenomenon where pretraining on one task hurts the finetuning performance of another. Besides being interesting in general, this phenomenon has practical relevance as pretraining becomes increasingly popular with large-scale models. Here, the authors present a clean case for \u201cnegative pretraining effect\u201d on images, and propose three ways to mitigate it.""659\tThis paper provides an overview of the lifelong learning process and the challenges it faces in constantly adapting to new problem settings. The paper discusses the balance between retaining knowledge and adapting to new experiences referred to as the stability-plasticity dilemma. The paper also discusses the existing literature onSequential learning and its role in training models to maximize their performance on a target task. The paper also highlights the increasing cost of training large models from scratch and the need for new research to understand the underlying mechanisms that influence the process ofSequential learning.\tThis paper conducts an empirical study to examine the well-known negative transfer phenomenon (termed as a negative pretraining effect in this work) in neural networks. In particular, a network trained on a sequence of tasks performs inferior to a network trained from scratch on the intended target task. The main idea of the paper is to study this phenomenon by formulating and intervening on different constituents of the sequential learning process - (1) changing the learning rate across tasks, (2) number and type of tasks encountered in the learning process, and (3) resetting the model biases when going from one task to another. The paper conducts experiments on four visual classification datasets (CIFAR-10, FashionMNIST, MNIST, SVHN) and report their findings for sequential training of ResNet-18 architecture. They show that increasing the learning rate after training on the first task can alleviate the negative pretraining effect. They further showcase how different task discretization and resetting model biases help to reduce the effect. ""660\tThis paper discusses theSequential Learning Problem, which is the process of training a model on a sequence of tasks to maximize its performance on a target task. The paper explores the balance between retaining knowledge and adapting to new experiences that is required for successful learning. It also examines the existing literature onSequential Learning and the challenges that arise when training large models from scratch. The paper\u6700\u540e proposes a new approach to understanding theSequential Learning Problem and suggests ways to improve the performance of models trained on a sequence of tasks.\tThe authors study what they call a negative pretraining effect = models pretrained on task 1 and tuned on task 2 sometimes underperform compared to just training on task 2 from scratch. This is an important factor in many forms of life long learning, multi-task learning and curriculum learning. They investigate 3 potential remedies / setups: a) using different learning rates for task 1 and task 2, b) changing from task 1 to task 2 more smoothly, and c) resetting network biases at different stages of the process. The perform with a single ResNet18 architecture on MNIST, Fashion MNIST, SVHN and CIFAR-10.""661\tThis paper studies the issue of adversarial examples in deep neural networks. It highlights the vulnerability of deep neural networks to adversarial inputs, which are minute input perturbations designed to mislead the network to yield incorrect predictions. The paper presents a number of studies that have been conducted to improve the robustness of deep neural networks against adversarial examples. Adversarial training and randomized smoothing are two of the few methods that have been shown to survive harsh verifications and improve the robustness of deep neural networks against adversarialdversarial examples. The paper also discusses the potential implications of adversarial examples and suggests ways to address them in future research.\tThis paper aims to improve adversarial robustness of the classifiers in a different perspective than the existing works. Usually, the networks are trained using adversarial examples to improve robustness (adversarial training). This work extend this line of thought and make an input robust to adversarial attacks. Instead of updating the network, they make updates to the input to gain robustness. In other words, this work explore the existence of safe spots near the input samples that are robust against adversarial attacks. Results on CIFAR-10 and ImageNet reveals that there exists such safe spots which are resistant to adversarial perturbations and improve adversarial robustness when combined with adversarial training (the authors term it as safe-spot aware adversarial training). Based on this approach, the authors also propose out-of-distribution detection method that outperforms previous works.""662\tThis paper discusses the problem of adversarial examples in deep neural networks and proposes two methods to improve the robustness of these networks:dversarial training and randomized smoothing. Adversarial examples are minute input perturbations designed to mislead networks to yield incorrect predictions. Despite the success of deep neural networks on various AI tasks, these examples have shown up in the literature as a major challenge for network design. The paper also discusses some of the recent methods for improving the robustness of deep neural networks against adversarial examples, including adversarial training and randomized smoothing.\tThis paper proposes a new adversarial framework where the defender could preemptively modify classifier inputs to find safe spots that are robust to adversarial attacks. They then introduce a novel bi-level optimization algorithm that can find safe spots on over 90% of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets and show that they can be used to improve both the empirical and certified robustness on smoothed classifiers. Besides, they propose a new training scheme based on their conjecture about safe spots for out-of-distribution detection which achieves state-of-the-art results on near-distribution outliers. ""663\tThis paper discusses the problem of adversarial examples in deep neural networks and the various methods that have been proposed to improve the robustness of these networks against them. It also examines the impact of these attacks on the performance of deep neural networks on various artificial intelligence tasks. The paper concludes by discussing the limitations of these methods and highlighting the need for further research in this area.\tThe authors argue that there are some safe \"spots\" in the data space that are less prone to adversarial attacks. The authors propose a technique to identify such \"safe spots\". They then leverage them for robust training and observe higher robust accuracy than baseline. Finally, they leverage this observation to identify out of distribution data. ""664\tThis paper discusses the challenges and solutions for modeling the dynamics of point clouds in modern robotic and automatic driving systems. Real-time depth sensors, such as LiDAR, are used to capture the geometric information of scenes accurately while being robust to different lighting conditions. However, dynamic point clouds, which are irregular and unordered in the spatial dimension, are difficult to model using existing 3D convolutions on grid based videos. To overcome this challenge, the paper proposes converting point clouds to a more suitable format, which is a 3D point cloud with a consistent spatial structure. This can be done by using a technique called point cloud fusion, which combines multiple point clouds into a single, more accurate representation. The paper also discusses the challenges and limitations of point cloud fusion, including the need for additional data and the problem of point flow in and out over time.\tThis paper introduces a new convolutional approach to directly process raw spatiotemporal (ST) point cloud data. The proposed point spatio-temporal (PST) convolution operates on \"point tubes\" and decouples space and time through a shared spatial convolution at each timestep, followed by a temporal convolution. It also introduces a transposed PST to enable point-wise predictions in an encoder-decoder framework (PSTNet). The presented experiments demonstrate the effectiveness of these convolutions by using PSTNet for action recognition and semantic segmentation on point cloud sequences, showing improvement over relevant recent work.""665\tThe paper discusses the challenges and opportunities in modeling the dynamics of point clouds, particularly in robotic and automatic driving systems. It highlights the importance of converting dynamic point clouds into a consistent format, which can be used for training and evaluation. The paper proposes a novel approach to this task, called point cloud convolutions, which can effectively model the dynamics of point clouds while maintaining their discriminatory power. The paper also discusses the limitations of existing methods for modeling dynamic point clouds and proposes some future directions for this research area.\tThe paper introduces point spatio-temporal convolutions, which are used for the feature extraction of point cloud sequences. A trainable kernel is used which is applied locally as a continuous convolution. An important aspect is that the temporal dimension is processed separately, with an additional convolution, instead of simply using a 4D convolution. The authors claim that in this way the network will achieve a better understanding of the dynamics of the input.""666\tThe paper discusses the challenges of modeling the dynamics of 3D point clouds in robotic and automatic driving systems. It explains that real-time depth sensors, such as LiDAR, generate 3D point clouds that represent the geometry of a scene. However, these point clouds are often used in combination with RGB images to enhance the discriminativeness of the point clouds. While these point clouds are often irregular and unordered in the spatial dimension, points are not consistent and even flow in and out over time. Therefore, existing 3D convolutions on grid based videos are not suitable for modeling the dynamics of point clouds. To overcome this challenge, the paper proposes a new approach to modeling dynamic point clouds, which involves converting the point cloud to a more regular representation and then using 3D convolutions to model the dynamics. The resulting model is able to capture the changes in the point cloud over time and can be used to optimize the parameters of a robotic system for dynamic\u573a\u666f modeling.\tThis paper aims to process the point cloud data in a convolution manner. The authors propose the PST convolution and deconvolution operations to handle the different tasks such as the classification and segmentation on point cloud. The extensive experiments verify the effectiveness of the proposed method and achieve state-of-the-art results on multiple benchmark datasets. Overall, the paper is well-written and organized. ""667\tThis paper provides an overview of the current state of text to speech (TTS) technology, including its history, challenges, and applications. It discusses the recent advances in TTS models, including those using machine learning, and the use of multi- speaker corpora to improve the quality and naturalness of the synthesized voice. The paper also highlights the increasing interest in custom voice technology, which can be used in various application scenarios such as personal assistant, news broadcast, and audio navigation, and is widely supported in commercial products.\tAdaSpeech is a paper on practical TTS custom voice adaptation with the aim of reducing the amount of adapted parameters per voice to allow cloud serving of a large number of custom voices while maintaining high adaptation quality and similarity. The novel piece that enables this is the conditioning of layernorm in the model on the speaker embedding. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. Descriptions of the model, experiments, and analysis of results are well done.""668\tThis paper provides an overview of the recent advances in text to speech (TTS) technology, including the use of custom voices in different application scenarios such as personal assistant, news broadcast, and audio navigation. It also highlights the challenges and future directions of TTS, such as the need for more advanced models and the development of more flexible and realistic speech synthesis algorithms.\tThe authors propose an interesting text-to-speech adaptation method for high quality and efficient customization of new voice. The proposed method consists of two-stage modeling : multi-phonetic-level acoustic condition modeling and conditional layer normalization. In the first stage modeling, the authors proposed a new phoneme-level acoustic condition modeling in addition to the speaker and utterance-level approaches. In the second stage modeling, they employ conditional layer normalization for efficient adaptation.""669\tThis paper discusses the topic of text to speech (TTS) and its applications. It explains the background of the TTS industry and the challenges in developing effective TTS models. The paper also covers the recent advances in TTS technology, including the use of multi-speaker corpora and custom voices, and discusses the future directions of the field.\tIn this paper, the authors present AdaSpeech, a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. The model is based on the TTS model in FastSpeech 2, with several additional components. The authors show that AdaSpeech has improved results over other baselines. They also provide an interesting ablation study.""670\tThis paper discusses the issue of training sparse networks to achieve the same performance as dense neural architectures. While recent work suggests that initialization is the key, focusing on it alone may not be sufficient. The paper takes a broader view of training sparse networks and consider various choices made during training that might disadvantage them. It measures the gradient flow across different networks and datasets and shows that the default choices of optimizers, activation functions, and regularizers used for dense networks can disadvantage sparse networks. Based on these findings, the paper shows that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. The paper suggests that initialization is only one piece of the puzzle and a wider view of tailoring optimization to sparse networks yields promising results.\tThis paper proposes effective gradient flow (EGF), which is a layer-wise normalized gradient flow. Compared to (unnormalized) gradient flow, the paper shows that the proposed EGF is (slightly) better correlated with metrics like test loss and test accuracy (see Table 1). Given that this claim is supported with experimental results, the paper would become much stronger if a larger number and a more diverse set of data-sets were used (in addition to CIFAR-10 and CIFAR-100, which are two very similar image-data-sets) as to show that the claim holds more generally. Apart from that, given that the correlations are (only) about 0.4 in Table 1, it seems that only some aspects are explained by EGF. ""671\tThis paper discusses the issue of training sparse neural networks to the same performance as dense neural architectures. Research has shown that training sparse networks can be difficult, andInitialization is often the key. However, focusing solely onInitialization alone has not been effective in achieving this goal. The paper takes a broader view of training sparse networks and considers various choices made during training that might disadvantage them. It measures the gradient flow across different networks and datasets, andshows that the default choices ofoptimizers, activation functions, and regularizers used for dense networks can disadvantage sparse networks. Based on these findings, the paper suggests thatreminding the architecture design and the training regime of sparse networks can improve gradient flow. The paper provides promising results and suggests that a broader view of training sparse networks is necessary to achieve better performance.\tRecently, initialization has been found critical to the model accuracy attained by sparse training [1]. In this paper, the authors studies the impact of factors other than initialization on the model accuracy attained by sparse training relative to dense training (under the same model parameter count). At the core of this paper, the authors argue that the effective gradient flow (grad norm from only activate model weight dimensions) is an effective indicator on the model accuracy attained by sparse training. Firstly, the authors show that the effective gradient flow attains higher correlation with model accuracy than the norm of full gradient (including gradients on sparsified weight dimensions) in sparse training. Secondly, the paper empirically demonstrate that in sparse training, 1) weight decay and data augmentation can hurt model accuracy, 2) batch normalization plays significantly role for model accuracy in sparse training and 3) non-saturating activations boost the magnitude of effective gradient flow and consequently improve model accuracy. ""672\tThis paper discusses the issue of training sparse networks to achieve the same performance as dense neural architectures. Research has suggested that initialization is the key, but focusing on it alone has not been effective. The paper takes a broader view of training sparse networks and considers various choices made during training that might disadvantage them. It measures the gradient flow across different networks and datasets and shows that the default choices of optimizers, activation functions, and regularizers used for dense networks can disadvantage sparse networks. The paper suggests that a broader view of tailored optimization to sparse networks is needed to improve gradient flow.\tThis paper looks at how choices in optimization affect how well you can train sparse networks. The authors come up with a new measure of effective gradient flow, which is important for good performance. They also compare sparse vs. dense networks across various optimizers, hyperparameters and activation functions, and find that batch norm and certain activation functions are beneficial for sparse networks""673\tThis paper discusses the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). LPA propagates node label information across the edges of a graph while GCN propagates and transforms node feature information. The paper investigates the two aspects of the relationship: (1) feature/label smoothing and (2) feature/label influence. Based on this analysis, the paper proposes an end-to-end model that combines LPA and GCN for node classification. In the unified model, edge weights are learnable and LPA serves as regularization to assist GCN in learning proper edge weights.\tThe manuscript proposes a unified model which combines label propagation algorithm (LPA) and graph convolution network (GCN). The main idea is to optimize edge weights (after making edge weights trainable) by maximizing the intra-class feature influence. Introducing the theorems on the relationship between feature and label influence, and LPA\u2019s prediction, the authors propose the unified objective function (a summation of the GCN loss and LPA loss) which combines both methods.   ""674\tThis paper studies the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) for node classification. It analyzes the two aspects of this relationship: (1) feature/label smoothing and (2) feature/label influence. The paper proposes an end-to-end model that unified LPA and GCN, where edge weights are learnable and LPA serves as regularization to assist GCN in learning proper edge weights. The analysis shows that LPA can help to smooth out the feature information of a node and improve the overall performance of GCN. Additionally, the paper demonstrates that the feature/label influence of a node can be used to inform the choice of edge weights in GCN, providing a more accurate classification result.\tThis paper addresses the problem that edges in a graph could be noisy, containing erroneous edges. With the assumption of GCN that \u2018labels/features are correlated over the edges of the graph\u2019, it is desired that weights of inter-class edges are large, and those of intra-class edges are small. Hence, these noisy edges could impair GCN\u2019s performance.""675\tThis paper discusses the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) for node classification. The authors analyze the two algorithms in terms of their ability to smooth and influence the feature and label information of nodes in a graph. They propose an end-to-end model that combines LPA and GCN to achieve improved performance for node classification. In the proposed model, learnable edge weights are used to regularization and assist in the learning of proper edge weights for the GCN. The paper also discusses potential applications of the proposed model.\tThis paper aims to combine the label propagation and graph convolutional network with the modeling of their latent relationships. In the developed model, the node label is utilized to infer the edge weights between different nodes. From the evaluation results in Section 4, the performance improvement between the proposed method GCN-LPA and GDC is marginal, which can hardly demonstrate the advantage of the unified model (with GCN and LPA) over the graph diffusion network (without the restriction of information aggregation over neighboring nodes).""676\tThis paper presents a study on the generalization error of supervised classification algorithms. The main goal of the study is to investigate how complexity measures on the classifier function space, such as the Rademacher Complexity Rm (F) and the VC dimension ( Blumer et al. (1989)), can be used to minimize the generalization gap of a classifier and avoid overfitting. The study also explores the relationship between these measures and the performance of the classifier on the training data. The results of the study suggest that the Rademacher Complexity Rm (F) and the VC dimension can be used to\u8bc4\u4f30 the generalization performance of a classifier, and that these measures have a strong relationship with the performance of the classifier on the training data. The paper also provides some insights on how to choose the appropriate complexity measure for a particular classification problem and the potential applications of these measures in the field of supervised classification.\tThis paper studied a novel perspective on generalization error bounds, by introducing the \"label generating function \"(LGF). Several new complexity measures (correlated Rademacher complexity, co-complexity, invariance co-complexity, dissociation co-complexity, Rademacher smoothness) were proposed. The properties of the measures and generalization error bound with respect to these complexity measures are studied.  ""677\tThis paper discusses the importance of complexity measures in the context of supervised classification and their ability to minimize generalization error. It provides an overview of the existing literature on this topic and presents a discussion of two commonly used complexity measures: the Rademacher Complexity Rm (F) and the VC dimension V C (F). The paper explains how these measures are used to evaluate the generalization ability of a classifier and how they can be used to avoid overfitting. The paper also highlights the limitations of using simple complexity measures such as Rm (F) and VC dimension, and provides an example of how a more complex measure, the Complexity of Classifier (COC), can be used to evaluate the generalization ability of a classifier.\tThe paper provides an interesting perspective to view generalization error for the machine learning model. In particular, it proposes to investigate the constraint on the label generating function space. They propose a concept of co-complexity analogous to the entropy-ish concept which measures complexities between two function spaces. This co-complexity can be decomposed into two parts which measure the categorization ability of the classifier in generator and extent level in classifier for the invariance transformation in the generator.""678\tThis paper provides an overview of the literature on the relationship between complexity measures and generalization error in the context of supervised classification. The paper\u8ba8\u8bba\u4e86\u4e0d\u540c\u7c7b\u578b\u7684 complexity measure\uff0c\u4f8b\u5982 Rademacher Complexity Rm (F) \u548c VC Dimension (F)\uff0c\u5982\u4f55\u4e0e generalization error \u76f8\u5173\u3002\u8fd8\u8ba8\u8bba\u4e86\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684\u4e00\u4e9b complexity measure\uff0c\u4f8b\u5982 Rm (F),V C (F)\uff0c\u5e76\u63d0\u51fa\u4e86\u5b83\u4eec\u5bf9 generalization \u6027\u80fd\u7684\u5f71\u54cd\u3002 The paper found that the complexity measures such as Rm (F) and VC Dimension (F) often directly control the generalization gap of a classifier, and that these measures are useful in finding the best complexity for a given problem. Additionally, the paper discusses the commonly used complexity measures in machine learning, such as Rm (F),V C (F), and how they affect the generalization performance of a classifier.\tThis paper aims to propose a new complexity measure, called co-complexity, to control classifiers' generalization gap. This new measure acts like a joint-entropy and leads to tighter bounds on the generalization error in this setting. The main idea is to extend the classical complexity measure of Barlett & Mendelson (2003) by introducing a new function space: the generator space is defined as the function space of all possible LGFs satisfying ad hoc constraints. Thus, the authors claimed to be able to measure the extent to which the classifier's function space obeys the invariance transformations in the data and measure the extent to which the classifier can differentiate between separate categories in the data.""679\tThis paper discusses the problem of deep learning models' memory footprint and computation cost, which arise from the large number of parameters and training data required to achieve high accuracy on a wide range of tasks. The paper presents several methods to reduce the memory footprint and computation cost of deep learning models, including neural architecture search, quantization, and network pruning. These methods are designed to compress and accelerate well-trained networks while still allowing for efficient fine-tuning on new data. The paper also discusses the trade-offs between memory footprint and accuracy, and the challenges and limitations of these methods.\tThis paper proposes BRECQ which is a new Post Training Quantization (PTQ) method. The goal of the paper is to push the limit of PTQ to low bit precision (INT2). They try to address this by considering both inter and intra-layer sensitivity to find the best update to the model parameters so that the output from a block is minimally changed/perturbed. Furthermore, the authors also consider mixed precision quantization setting.""680\tThis paper discusses the issue of high memory and computation requirements in deep learning, specifically the use of quantization and network pruning techniques to reduce the size and computational complexity of large, redundancy deep learning models. The paper discusses the advantages and limitations of these techniques, as well as the challenges of implementing them on real-world devices. The paper also highlights the need for further research in this area to better understand and address the issues involved in using these techniques.\tThis paper explores the post-training inference quantization. Based on second-order quantization error analysis, it proposes to reconstruct quantized model in a block level to achieve SOTA accuracy for INT2 weight quantization, which distinguish this paper from previous reported layer-wise reconstruction approach. The proposed approach is intuitive and supported by extensive experiments across a wide range of image classification and object detection tasks.""681\tThe paper discusses the problem of deep learning models with large memory footprints and high computational costs, which are issues that have\u5236\u7ea6 the development of many deep learning tasks. The paper provides several solutions, including neural architecture search, quantization, and network pruning, to address these issues. The paper also explores the trade-offs between memory footprint and computational cost, and\u6307\u51fa that not all training data are always ready-to-use. The paper concludes by highlighting the importance of finding a balance between the two factors in order to achieve efficient and effective deep learning.\tI couldn't follow the method described in the paper. The authors are basically trying to address post-training quantization by perturbing the the weights of a trained DNN. The goal is to perturb the weights so that the quantized DNN will behave similar to the original full-precision DNN. The authors draw a link between this optimization problem and optimizing for the \"reconstruction\" of the output activations of a block (see Equation 7). The technique BRECQ, shown in Algorithm 1, is basically to optimize the perturbation of the weights for the right hand side of Equation 7 for each block of a DNN.""682\tThis paper provides an overview of the importance of data calibration in neural networks and the current challenges in achieving it. The paper discusses the need for data curation in neural networks, the evaluation criteria used in previous studies, and the importance of  calibration in real-world applications such as medicine, self-driving cars, and scientific analysis. The paper also discusses the challenges of achieving data calibration, including the need for more efficient data processing and the need for better machine learning algorithms. The paper\u6700\u540e provides a summary of the key findings and recommendations for future research in data calibration.\t\tThis paper discussed how data properties (e.g., label noise, label imbalance, data size) affects calibration error. The author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and iNaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) Calibration error under different combinations of data augmentations. The experimental results show that poor calibration performance accompanies with large noisy label rate, large imbalance ratio and small dataset size. For the reason of small dataset size causing poor calibration error, this paper provided the theoretical proof. ""683\tThis paper discusses the importance of data curation in neural network deployment and the challenges faced when working with large amounts of labeled data. The paper provides an overview of the need for curated data in neural networks, including the importance of dataset properties such as distributional shift and bias in crowd-sourced computer vision datasets. The paper also discusses the evaluation criteria used to evaluate neural networks, including downstream prediction accuracy, but also  calibration, the extent to which model certainty reflects the actual correctness likelihood. The paper\u6700\u540e\u6307\u51fa\uff0c in real-world applications, neural networks need to be able to handle both accuracy and calibration, and\u63d0\u51fa\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\tThis work is an empirical survey of the calibration problem with convnets. The authors use several existing benchmark datasets and create synthetic class-imbalance for datasets that are initially balanced. They then extend the well-known results on higher prediction error of minority class, to its calibration error. The work investigates several existing methods that alleviate prediction error in imbalanced datasets and examine their effect on calibration error. At last, the effect of dataset size and data augmentation on calibration error is reported. Later on, the effect of random label noise is also examined. The observations, although not surprising, have not been reported before ""684\tThis paper discusses the importance of evaluating neural networks for accuracy and calibration in real-world applications. It explains that neural networks often require large amounts of labeled data to perform well, making data curation a crucial but costly aspect of deployment. The paper discusses several studies that have been conducted on dataset properties such as distributional shift and the bias in crowd-sourced computer vision datasets. It also explains the importance of evaluation criteria such as calibration in real-world applications, especially in sensitive real-world applications such as medicine, self-driving cars, and scientific analysis. The paper also highlights the need to balance the trade-off between accuracy and calibration in order to achieve the best performance for a particular application.\tIn this work, authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation. In the field of applied AI to real-life problem, we face all the time decision-makings on what is the most effective strategy in the pipeline (eg. sampling, noise, labeling) and this paper present some evidence for those decisions.""685\tThis paper studies how agents can communicate effectively in a 3D environment using joint actuating rather than symbolic channels. The paper explores the benefits of this newmodality, including the ability to generalize to novel partners and the ability to communicate with a non-uniform distribution of intents. The paper also analyzes the specific difficulties associated with finding effective communication protocols in practice. Finally, the paper proposes and evaluating initial training improvements to help agents learn more effective communication protocols.\tThe paper investigates emergent gesture-based communication in Embodied Multi-Agent Populations. A noticeable feature of the paper is that it investigates emergent communication in the case of non-uniform distribution of intents and costly communication (i.e. agents are penalized for effort). The authors find that in certain scenarios, these conditions may lead to communication generalization of learned communication strategies to new, previously unseen agents.""686\tThis paper discusses the importance of effective communication in multi-agent settings and the current state of research in emergent communication. The paper proposes studying agents that learn to communicate through actuating their joints in a 3D environment, which allows for a newmodality of communication that can generalization to novel partners. The paper also discusses the limitations of traditional communication settings and the challenges of finding protocols that generalize beyond the training partners. The paper proposes and evaluated initial training improvements to enhance the effectiveness of communication among agents.\tThe paper deals with agents that communicate non-verbally via actuating their joints in a 3D environment. The authors show that the agents should be able to learn protocols that can generalize to novel partners. Furthermore, the authors find that the current training approaches are brittle, and they propose and evaluate approaches to address this challenge.""687\tThis paper discusses the importance of effective communication in multi-agent settings and how emergent communication can be used to enable information exchange and cooperation between agents. The paper explores the current state of emergent communication research and how it has focused on the use of discrete cheap-talk channels. It also discusses a newModality of communication that is learned by agents via actuating their joints in a 3D environment. The paper shows that under realistic assumptions and a non-uniform distribution of intents, these agents can find protocols that generalize to novel partners and that initial training improvements can be used to enhance the effectiveness of these protocols.\tThe authors study the zero-shot emergent non-verbal communication in this paper. Different from most papers on emergent communication. this paper uses the motion of three-joint agents. The agents meet partners that they have never seen in the training phase, presenting the challenge of the universal protocol. To make a universal protocol possible, the authors study intents sampled from Zipf distribution and energy regulation. The authors conducted experiments on tasks with 2, 5, 10 intents. The results show that providing latent energy feature is essential for zero-shot coordination. To achieve better than chance accuracy on tasks with 10 intents, a torque curriculum is needed. ""688\tThis paper presents a new method for uncertainty quantification inSequential regression, particularly for deep recurrent networks. Existing approaches often make restrictive assumptions such as stationarity, yet perform poorly in practice in the presence of real world non-stationary signals and drift. The proposed method makes no assumptions about stationarity and outperforms competitive baselines on both drift and non drift scenarios. This work helps in makingSequential regression more effective and practical for use in real-world applications and is a powerful new addition to the modeling toolbox forSequential uncertainty quantification in general.\tIn this paper, the authors propose a technique for uncertainty estimation in regression with neural networks. The basic idea is to use an auxiliary \"meta model\" that (in the authors' best performing setting) has access to the base model and is trained jointly with it. The purpose of the meta model is to predict the error characteristics of the base model, which of course naturally leads to error bars. In order to account for the fact that the models errors on the train set are unlikely to be representative, the authors make use of a validation set on which the meta model is trained further. ""689\tThis paper discusses a new method for generating high-quality uncertainty estimates forSequential regression, particularly deep recurrent networks. Existing approaches often make restrictive assumptions, such as stationarity, yet perform poorly in practice, particularly in presence of real-world non-stationary signals and drift. The paper presents a flexible method that generates symmetric and asymmetric uncertainty estimates, making no assumptions about stationarity, and outperforms competitive baselines on both drift and non- drift scenarios. This work helps makeSequential regression more effective and practical for use in real-world applications, and is a powerful new addition to the modeling toolbox forSequential uncertaintyQuantification in general.\tIn this work, the authors present a meta-modeling approach to provide predictions with uncertainty estimates in a sequential task. They develop a white box, black box, and joint modeling method that allows them to apply their method to a variety of scenarios. These methods differ based on the amount of information provided to a meta-learner which has the goal of predicting errors $\\hat{z} \\in \\mathbb{R}^{D \\times M} $. The authors also incorporate the ability to make asymmetric uncertainty bounds. They apply this method and many baeslines to two datasets: MITV and SPE9PR.""690\tThis paper presents a method for generating high-quality uncertainty estimates forSequential regression, particularly deep recurrent networks, which remains a challenging and open problem. Existing approaches often make restrictive assumptions, such as stationarity, yet perform poorly in practice, particularly in presence of real-world non-stationary signals and drift. The paper proposes a flexible method that can generate symmetric and asymmetric uncertainty estimates, making no assumptions about stationarity, and outperforms competitive baselines on both drift and non drift scenarios. This work helps makeSequential regression more effective and practical for use in real-world applications, and is a powerful new addition to the modeling toolbox forSequential uncertainty quantification in general.\tThis paper describes a method to generate symmetric and asymmetric uncertainty estimates. The method is proposed to work for the non-stationarity processes found in real-world applications. The paper introduces a meta-modelling concept as an approach to achieve high-quality uncertainty quantification in deep neural networks for sequential regression tasks. The paper also introduces metrics for evaluating the proposed approach. A proposed meta-modelling approach is related to the work of Chen et al. (2019) which is mainly used for classification task in a non-sequential setting, however, the proposed method is mainly for the sequential setting. ""691\tThis paper provides an overview of the main challenges and problems in comparing data distributions on metric spaces, which are a fundamental aspect of machine learning. The paper covers the following topics:\n\n1.  Introduction: This section provides an overview of the paper and explains the main goals and contributions of the paper.\n\n2.  Metric measure spaces: This section defines metric measure spaces and discusses the properties and limitations of these spaces, such as the fact that they are not compact, and that the measure \u03bc is not always smooth.\n\n3.  Data distributions: This section provides an overview of the different types of data distributions that can be\u5b58\u5728\u4e8e metric measure spaces, such as point clouds, sequences of points, or functions between metric spaces.\n\n4.   comparing data distributions: This section discusses the basic problems and challenges in comparing data distributions on different metric spaces, such as the issue of differentiability and the need for suitable metrics and measures to compare the quality of the data.\n\n5.  \u89e3\u51b3\u65b9\u6cd5\uff1a This section presents some possible solutions to the problem of comparing data distributions on different metric spaces, such as the use of metrics that are similar in different spaces, or the use of machine learning techniques such as support vector machines or deep learning.\n\n6. \u603b\u7ed3\uff1a This section summarizes the main points and contributions of the paper, and provides a final overview of the challenges and possible solutions to comparing data distributions on metric spaces.\n\n7. \u53c2\u8003\u6587\u732e\uff1a This section provides a list of references that are used in the paper, including books, articles, and research papers on metric measure spaces and comparing data distributions.\tThe paper introduces a novel unbalanced Gromov-Wasserstein type problem. The Gromov-Wasserstein distance is very useful in practice for comparing probability distributions that do not lie in the same metric spaces. It has recently found several successful applications in ML for computational chemistry, graphs comparisons or NLP. Following previous works on unbalanced optimal transport (i.e. soft constraints over marginals enforcement of the coupling matrix), and the rationale that disposing of unbalanced versions of transport problems can alleviate in some ways presence of outliers or noise in the distributions, the authors propose two \u2018unbalanced\u2019 variants of the Gromov-Wasserstein (GW) problem, that allow comparison of metric spaces with arbitrary positive measures up to isometries (I.e. rigid transformations). ""692\tThis paper provides an overview of the challenges and opportunities in comparing data distributions on different metric spaces, a fundamental problem in machine learning. The paper covers the background information on metric measure spaces, the basic concepts of metric spaces and measure spaces, and the problems that can be solved by comparing data distributions on these spaces. It also discusses some of the recent advances in this area, such as the use of machine learning algorithms and deep learning models to compare data distributions on different metric spaces. The paper concludes by highlighting the potential applications of comparing data distributions on different metric spaces in various fields, such as natural language processing, quantum chemistry, and computer vision.\tThe authors consider the Gromov Wasserstein (GW) problem for metric measure spaces having different masses (i.e., Unbalanced GW). Similar to the ideas of unbalanced optimal transport (UOT), they proposed to use a quadratic divergence to relax the marginal constraints (instead of divergence as in UOT). When divergence is KL, the authors derive a GPU-friendly algorithm for the UGW  which relies on the unbalanced Sinkhorn algorithm. Additionally, the authors also propose a variant of UGW, namely Conic Gromov-Wasserstein (CGW) to address the different masses of metric measure spaces. The authors propose that CGW has nice properties (Theorem 1). However, there is no algorithm to solve the CGW yet.""693\tThis paper discusses the problem of comparing data distributions on metric spaces, which is a fundamental problem in machine learning. Metric measure spaces are a mathematical framework that models data as a distribution in a space with a distance function and a measure that captures the distribution's spread. This paper provides an overview of the different types of metric spaces that can be used in machine learning, and discusses how to compare data distributions on these spaces. Additionally, it discusses some common machine learning algorithms for comparing data distributions on metric spaces, such as the R-squared algorithm and the Mann-Whitney U-test. The paper also highlights the challenges and opportunities that arise when comparing data distributions on metric spaces, such as the issue of infinite support, and the need for efficient algorithms and data structures.\tIn the paper, the authors propose two versions of Gromov-Wasserstein distance when the weights of measures do not need to sum up to one. The first version, named Unbalanced Gromov-Wasserstein (UGW), is a direct application of unbalanced optimal transport (UOT) to the setting of Gromov-Wassertstein. The second version, named Conic Gromov-Wasserstein (CGW), is an extension of the conic formulation of UOT to the setting of Gromov-Wasserstein. The authors also show that CGW is a distance in the metric-measure spaces and is an lower bound of the UGW. Finally, the authors also provide some experiments with their proposed divergences.""694\tThis paper discusses the role of network pruning in reducing the computational and memory requirements of neural networks while maintaining their performance on downstream tasks. Network pruning is a technique that remove unnecessary units from a neural network while maintaining the performance. Existing pruning methods can be divided into two categories: models that enforce sparsity as model regularization and models that do not have this property. The paper\u8ba8\u8bbas the theoretical basis of these two types of pruning methods and the impact of each on the network's performance. Additionally, the paper presents an analysis of the trade-offs between accuracy and computational complexity for different types of pruning methods. The paper concludes by suggesting some future directions for studying network pruning and its applications in neural networks.\tThe submission deals with eliminating neurons in a network where either a) all the input connections xor b) all the output connections have been pruned. When this is the case, the unpruned a) output or b) input connections are unused and can also be pruned: and the freed parameter budget used for other more useful connections. This is shown to improve the accuracy of pruned networks at a given sparsity ratio, especially for very high levels of sparsity.""695\tThis paper discusses the use of neural networks in computer vision, natural language processing, and speech recognition, and how network pruning can be used to reduce the computational and memory requirements of these models while maintaining their performance. The paper identifies two main categories of pruning methods: those that encourage sparsity (i.e., remove unnecessary units) and those that use regularization to prevent overfitting. The paper provides an overview of the existing literature on network pruning, and discusses the potential benefits and limitations of these methods. Additionally, the paper discusses the challenges of using network pruning on low-end devices and suggests future directions for research in this area.\tThe paper proposes to remove dead neurons and their connected parameters through a very simple check while reviving pruned (salient) parameters up to the prespecified sparsity level, such that the sparse network obtained could perform better. The main (and perhaps the single major) contribution of this work is in its demonstration that such a simple method is indeed effective for different pruning methods on various network architectures and datasets. The proposed method (AAP) can perhaps be considered as a generic post-processing step that could be equipped to any pruning method leaving dead neurons.""696\tThis paper discusses the role of network pruning in reducing the computational and memory requirements of neural networks and improving their deployment on low-end devices. Network pruning is a technique used to remove unnecessary units from a neural network while maintaining the performance with minimal accuracy loss. Existing pruning methods can be divided into two categories: model regularization and active learning. Model regularization techniques like sparsity-inducing regularization achieve model regularization by making the model contain only those units that contribute to the final output with a high probability. Active learning techniques like weight decay and learning rate decay also achieve regularization by adjusting the weights and learning rates of the model to have a lower risk of overfitting. pruned networks can be trained more easily on smaller datasets and have a lower computational complexity, making them more suitable for deployment on low-end devices.\tThis work proposes a novel pruning method, called all-alive pruning (AAP), which is a general technique to remove dead connections from pruned neural networks. AAP is broadly applicable to various saliency-based pruning methods and model architectures. AAP equipped with existing pruning methods consistently improves the accuracy of original methods on three benchmark datasets.""697\tThis paper presents an overview of semantic image editing, a technique for manipulating semantic attributes of images while maintaining their original identity. The paper discusses the goals of semantic image editing, the different types of methods for achieving these goals, and the challenges and limitations of existing approaches. It also provides an overview of recent advances in the field, including new GAN-based techniques and the use of machine learning to automate the process of semantic image editing.\tIn this paper, the authors propose an image attribute editing method by manipulating the GAN latent vector. Specifically, this paper uses a pre-trained GAN to synthesize images, a pre-trained regressor to get the image attributes, and trains a network T to find meaningful latent-space directions. It then edits image attributes by modifying the input latent vector, described as z' = z + T(z)\u03b5. The experimental results show that the proposed method performs better than other selected methods to some degree.""698\tThis paper discusses the use ofGANs for semantic image editing. It defines semantic image editing as the process of changing the meaning of an image while maintaining its identity. It then presents an overview of existingGAN-based approaches for semantic image editing and categorizes them into two groups: image-space editing methods that directly transform one image to another across domains, and non-image-space editing methods that modify the attributes of an image without changing its appearance. The paper also discusses the challenges and limitations of these approaches, such as the need for large amounts of data and the difficulties of achieving photo-realism. Finally, the paper proposes a future direction for semantic image editing, namely using machine learning to automate the process of semantic editing.\tThis paper presents a new approach for the semantic image editing task by allowing the controllable transformation on the latent space. Authors proposed to integrate an attribute regression network for training the transformation functions. The local transformation T is learned from a simple MLP conditioned on the latent vector z. Two outputs of the regression module for the original latent vector z and the transformed one z+T*epsilon are used to minimize the cross-entropy loss. Experiments validate the effectiveness of the proposed method in terms of manipulation quality.""699\tThis paper provides an overview of semantic image editing, a process of modifying images while preserving their semantic attributes. The paper discusses the challenges of semantic image editing, including maintaining the original image\u2019s identity and providing continuous manipulation of multiple attributes simultaneously. Existing GAN-based approaches for semantic image editing can be categorize into two groups: image-space editing methods that transform one image to another across domains, and colorspace editing methods that modify color attributes. The paper also provides an overview of future directions for semantic image editing, including the use of machine learning algorithms and the development of more advanced editing tools.\tThe paper proposes a new simple, yet powerful and alternative method of editing the semantic attributes of images generated using pre-trained GAN models as well as a pre-trained regressors. The approach allows for the manipulation of single or multiple various image attributes, while preserving the identity of the original image in contrast to the baseline method of Shen et. al 2019. The method focuses on the manipulation of the latent space, in contrast to the popular image space editing methods. ""700\tThis paper focuses on unsupervised sequence generation and its application in various fields such as machine translation, image captioning, and dialogue generation. The most common approach to autoregressive sequence modeling is to maximize the likelihood of each token in the sequence given the previous partial observation, but this approach is prone to exposure bias problem. GANs, which are trained using adversarial networks, are proposed as an alternative to models for sequence generation. The performance of GAN-based models is evaluated using various metrics and compared with existing models in the field. The paper concludes by discussing the future directions and challenges in unsupervised sequence generation.\tThe paper proposes an improvement to sequence generative adversarial networks (GAN) to cope with the common training issues of GANs. For the sake, the paper combines Gumbel-Softmax based GAN, relativistic discrimination  function with  the matching of mean representations of true and generated samples in a latent feature space. This feature statistics alignment allows to leak information from the discriminator to the generator as the used features are extracted from the discriminator network. Experimental evaluations on synthetic and real datasets show the improvement achieved by the proposed method over existing sequence generation networks.""701\tThis paper discusses the use of Generative Adversarial Networks (GANs) for unsupervised sequence generation. It explains the basic concepts of GANs, including its components (a generator and a discriminator), and discusses the challenges that arise when using GANs for sequence modeling. The paper also highlights the benefits of using GANs for sequence generation, such as improved quality and consistency, as well as the ability to generate complex and dynamic sequences. The paper ends with a discussion of potential applications of GANs in sequence generation, including machine translation, image captioning, and dialogue generation.\tThe paper addresses the task of improving GANs for sequence generation and proposed a method based on the relativistic discriminator. The proposed method employs a Feature Statistics Alignment (FSA) paradigm to reduce the gap between real and generated data distributions. It relies on the relativistic discriminator for \"coarse\" differences and FSA for \"fine-grained\" differences between real and generated data distributions. It is evaluated on synthetic and real datasets, and it significantly outperforms the baselines. It also outperforms baselines on human evaluation based on the acceptance, grammaticality, and meaningfulness of the generated sentences. ""702\tThis paper discusses the problem of autoregressive sequence modeling and the use of Generative Adversarial Networks (GANs) for it. The traditional approach to modeling sequence data using maximum likelihood estimation (MLE) is prone to exposure bias, which results in accumulative mismatch as the sequence length increases. GANs, on the other hand, can serve as an alternative to models that are based on MLE. The paper also discusses some of the recent advances in unsupervised sequence generation and their applications.\tThis paper proposes a new GAN-based text generation method that incorporates feature statistics alignment and gumbel-softmax for reparameterization to deal with mode collapse and unstable training. For feature statistics alignment, the authors design two methods such as mean square and mean distance alignments. They evaluate the proposed method on a synthetic dataset, MS COCO caption, and EMNLP2017 WMT news dataset, comparing them with RL-based and non RL-based models. With extensive experiments including ablation studies, the proposed method show promising results.""703\tThis paper discusses the problem of reward progressivity in value-based deep reinforcement learning agents. It explains that in decision making tasks with compounding returns, such as stock market investing, the rewards received by the agent increase in magnitude over time. This property also arises in many games settings, such as the Atari game Video Pinball, where the player can increase a \u201c bonus multiplier\u201d that increases the score paid per bumper hit. The paper suggests that this property may be problematic for value-based deep reinforcement learning agents, as it can lead to temporal difference errors. The paper also provides suggestions for ways to address the problem of reward progressivity, such as using rewards that are more stable or using a different value function.\tThis paper proposes an extension to DQN, more generally applicable to value-based deep RL systems, that encodes the return using a thermometer encoding with exponentially-sized bins. This enables returns of vastly differing magnitudes to be learned without hurting performance. The authors propose an algorithm for learning these encode returns, including the use of a variance scaling term to speed up learning.""704\tThe paper\u63a2\u8ba8\u4e86\u5728\u6d89\u53ca compounding  returns \u7684\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u5982 stock market \u6295\u8d44\uff0c\u5956\u52b1\u901a\u5e38\u968f\u65f6\u95f4\u589e\u52a0\u7684\u73b0\u8c61\uff0c\u8fd9\u79cd\u73b0\u8c61\u4e5f\u51fa\u73b0\u5728\u8bb8\u591a\u6e38\u620f\u4e2d\uff0c\u4f8b\u5982 Atari \u6e38\u620f\u4e2d\u7684\u201c bonus  multiplier\u201d\uff0c\u73a9\u5bb6\u65e0\u6cd5\u91cd\u7f6e\uff0c\u56e0\u6b64\u5956\u52b1\u901a\u5e38\u4f1a\u968f\u65f6\u95f4\u589e\u52a0\u3002\u8be5\u73b0\u8c61\u88ab\u79f0\u4e3a progressive reward \u4efb\u52a1\u3002\u8be5 paper \u7684\u5047\u8bbe\u662f\uff0c\u5bf9\u4e8e value-based \u7684 deep reinforcement learning \u6a21\u578b\u6765\u8bf4\uff0c\u5956\u52b1 progressivity \u53ef\u80fd\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u5176\u7406\u7531\u662f\uff0c\u5728\u65f6\u95f4\u5dee\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u8fd9\u79cd\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u8be5 paper \u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5373\u4f7f\u7528\u4e00\u4e9b\u989d\u5916\u7684\u6280\u672f\u6765\u7ea0\u6b63\u65f6\u95f4\u5dee\u9519\u8bef\u3002\tThis work describes and addresses the issue of _reward progressivity_ in reinforcement learning, where as the task progresses the scale of the reward changes. The authors argue that reward progressivity harms Q-learning when training signals arising from large rewards interfere with those arising from smaller rewards. They propose a form of reward decomposition with an analogous modification to the Q-network output, which together help to ensure that training losses from small and large rewards are similarly scaled. The authors present a handful of experimental results demonstrating that their proposed method outperforms two other reward re-scaling baselines when reward progressivity is an issue and maintains good performance in more standard tasks.""705\tThis paper discusses the problem of reward progressivity in value-based deep reinforcement learning agents. Reward progressivity refers to the property that the rewards received by the agent increase in magnitude over time in decision making tasks such as stock market investing and games like Video Pinball. The paper hypothesises that this can be a problem for value-based deep reinforcement learning agents, as it can lead to temporal difference errors. The paper proposes that agents should instead be trained using a method that is more sensitive to the current state of the environment and the past actions of the agent. This can help to prevent temporal difference errors and improve the performance of the deep reinforcement learning agent.\tIn this paper the authors propose a new RL method, spectral DQN, in which rewards are decomposed into different frequencies. This decomposition allow for the training loss to better balanced on certain tasks - in particular those with progressive rewards. The new method is shown to perform well on specially constructed tasks with extreme reward progressively, as well as on a selection of standard Atari tasks.""706\tThis paper discusses the question of why highly over-parametrized neural networks, which can memorize the entire training set, learn solutions that generalize well even though they can't. Some have speculate that neural networks learn minimal but sufficient representations of the input through implicit regularization of SGD, and that the minimality of the representations relates to generalizability. Follow-up work has disputed the validity of some of these claims when using deterministic deep networks, leading to an ongoing debate on the notion of optimality of representations and how they are learned during training. The paper also discusses the use of information-theoretic quantities in deep learning and the challenges of analyzing the learning process using these quantities.\tBroadly, this work is an attempt to understand how neural networks can form generalizable representations while being severely overparameterized. This work proposes an information theoretic measure, called the \"usable information\", and use it to quantify the amount of relevant information in different layers of a neural network during training. The key idea is that, in order for the information represented in one layer to be \"usable\" by the next layer, it should be decodable by a simple transformation (affine + element-wise nonlinearity).""707\tThis paper discusses the question of why highly over-parametrized neural networks can learn solutions that generalize well even though they can memorize the entire training set. Some have speculate that neural networks learn minimal but sufficient representations of the input through implicit regularization of Stochastic Gradient Descent (SGD), and that the minimality of the representations relates to generalizability. Follow-up work has disputed the validity of some of these claims when using deterministic deep networks, leading to an ongoing debate on the notion of optimality of representations and how they are learned during training. The paper also discusses the use of information-theoretic quantities in deep learning and their potential for analyzing the information content of learned representations.\tThe paper studies how initialization and the implicit regularization of SGD affect the training dynamics of neural networks in terms of minimality and sufficiency of learned representations. The main findings are that 1) SGD with random initialization learns almost minimal and sufficient representations and 2) SGD with an initialization that contains information about irrelevant factors fails to converge to minimal representations, increasing the chance of overfitting. These findings are interesting, useful for understanding neural networks, relevant to the ICLR community, but lack evidence of generality.""708\tThis paper discusses the question of why highly over-parametrized neural networks can learn solutions that generalize well even though they can memorize the entire training set. Some have speculate that neural networks learn minimal but sufficient representations of the input through implicit regularization of Stochastic Gradient Descent ( SGD ), and that the minimality of the representations relates to generalizability. Follow-up work has disputed the validity of some of these claims when using deterministic deep networks, leading to an ongoing debate on the notion of optimality of representations and how they are learned during training. The paper uses information-theoretic quantities to analyze the amount of information that the learned representation contains about the input, and discusses the possible implications of this for the ability of neural networks to generalize well.\tThe authors contribute to the recent research on whether neural network training (in particular, SGD) favors minimal representations, in which irrelevant information is not represented by deeper layers. They do so by implementing a simple neuroscience-inspired task, in which the network is asked to make a decision by combining color and target information. Importantly, the network's output is conditionally independent of the color information, given the direction decision, so the color information is in some sense irrelevant at the later stages. Using this, the authors quantify the 'relevant' and 'irrelevant' information in different layers of the neural network during training. Interestingly, the authors show that minimal representation are uncovered only if the network is started with random initial weights. Information is quantified using a simple decoder network.""709\tThis paper focuses on the nonconvex-strongly-concave min-max problem, where the objective function f ( x , y ) is nonconvex with respect to x for all y \u2208 Rd2 , and \u03bc-strongly concave with respect to y for all x \u2208 Rd1 . This problem is typically encountered in machine learning, and it is challenging to optimize using traditional min-max algorithms. This paper proposes a new approach to this problem, which is known as the mixed-integer linear programming (MILP) approach. The MILP approach involves creating a mixed-integer model that can be optimized using linear programming techniques, which can be more efficient than traditional min-max algorithms for this type of problem. The paper also discusses the challenges and limitations of the MILP approach, and provides some experimental results that demonstrate its effectiveness for solving the nonconvex-strongly-concave min-max problem.\tThis paper studies the nonconvex strongly-concave min-max optimization problem. It improves the analysis of an existing method SREDA to make it allow larger step size and less initialization computation. Besides, it extends the algorithm to the case where the objective function is non-differentiable. The authors claimed it is the first zeroth-order variance-reduced method for the min-max problem. Experiments are conducted to demonstrate the improved algorithm is better than existing methods.""710\tThis paper focuses on the nonconvex-strongly-concave min-max problem, which is a type of optimization problem that can be used to solve a wide range of machine learning problems, including generative adversarial networks (GANs), robust adversarial machine learning, imitation learning, and more. The nonconvex-strongly-concave min-max problem is characterized by the property that the objective function is nonconvex with respect to one of the variables for all values of the other variable, and is \u03bc-strongly concave with respect to the other variable for all values of the first variable. This paper proposes a novel approach to solve the nonConvex-strongly-concave min-max problem using a combination of deep learning and traditional optimization techniques. The proposed approach is shown to be effective in solving a number of real-world machine learning problems.\tThe paper proposes a SREDA-Boost, which builds upon SREDA for nonconvex-strongly-concave minimax problem. The SREDA-Boost algorithm is less restrictive to initialization and has an accuracy-independent and larger step size. Thus it can run substantially faster than SREDA. The main contribution to the first-order optimization story is a new analytical framework that builds upon the previous analysis in SREDA and overcomes the dependence of highly accurate initialization via bounding the tracking error and gradient estimation error separately. It also proposes a zeroth-order variance reduction algorithm for the same optimization problem, which has the largest possible step size so far and also improves the complexity of the state-of-the-art in some cases. Various experiments have validated the superiority. The theoretical analysis and empirical results look good to me.""711\tThis paper focuses on the nonconvex-strongly-concave min-max problem, where the objective function f ( x , y ) is nonconvex with respect to x for all y \u2208 Rd2 , and \u03bc-strongly concave with respect to y for all x \u2208 Rd1 . This problem is of interest in machine learning, as it captures several important machine learning models and problems, such as generative adversarial networks (GANs), robust adversarial machine learning, and imitation learning. The paper proposes a novel approach to solving the nonConvex-strongly-concave min-max problem using a combination of gradient descent and the k-\u6700\u8fd1\u90bb\u7b97\u6cd5 (k-\u8fd1\u90bb\u6cd5) . The proposed approach is shown to be efficient and effective in solving many common min-max problems in machine learning.\tThe paper proposes a variant SREDA-Boost of the variance reduction method SEDRA for solving nonconvex-strongly-concave min-max problem. The first contribution of the paper is to relax the conditions on the initialization  of SEDRA and moreover enable larger stepsizes ($\\epsilon$-independent stepsizes). As SEDRA is already optimal, such modification does not improve the theoretical convergence rate, but it is beneficial from the practical perspective. The second contribution is to adapt the method to zero order oracle, achieving the state-of-the-art convergence rate. ""712\tThis paper discusses an important challenge in developing autonomous agents, specifically how to recognize and learn from a single example of a novel object without the need for multiple examples. The story of the household robot and the casserole highlights this issue, as the robot was unable to recognize the casserole based on a single example alone. The paper suggests various approaches to\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c including using larger datasets, using multiple types of information about the object, or using more advanced machine learning techniques.\tThis paper studies one-shot 2D object detections. The major conclusion is that when keeping the number of training images fixed, increasing the number of training categories can significantly increase the one-shot performance.  This paper demonstrated this conclusion empirically by doing controlled experiments on COCO, Objects365, and LIVS.   Inspired by this observation, this paper improves the state-of-the-art one-shot detection performance on COCO from 22.0 to 27.5 AP50 by training on LIVS.  Other two related conclusions in this paper: - PASCAL VOC is not suitable for evaluating one-shot object detection algorithms. The reason may be that the number of instances per image is low(2.9 ins/img on average). The algorithm only needs to recognize foreground objects rather than objects of target categories.  - Only when the training data is challenging enough, increasing the model size and training time can help improve one-shot performance.   ""713\tThe paper describes a situation in which a household robot is able to recognize over 21,000 objects, but is unable to recognize a specific object due to a one-time error. The paper highlights an important obstacle towards truly autonomous agents\uff0c\u5373\u80fd\u591f\u68c0\u6d4b\u5230\u672a\u5728\u5148\u524d\u793a\u4f8b\u4e2d\u89c1\u8fc7\u7684\u7269\u4f53\u5e76\u57fa\u4e8e(\u6700\u597d\u662f)\u4e00\u4e2a\u793a\u4f8b\u5b66\u4e60\u8bc6\u522b\u5b83\u4eec\u3002\u901a\u8fc7\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u4ee3\u7406\u3002\t- The paper considers the problem of one-shot object detection, meaning, the model is asked to detect unseen categories, based on only one provided a template.  - The main discovery made by the authors is that, to generalise better, the model should be trained with data from as many categories as possible, given the same budget on number of samples for training.  - Architecture-wise, a Siamese Faster R-CNN is adapted. ""714\tThis paper discusses an important challenge in developing truly autonomous agents, which are systems that can recognize and respond to novel, previously unseen objects. The paper provides an example of how this challenge is addressed in the real world by customer service representatives who need to be able to fix a problem with a household robot, which includes recognizing and identifying a single object from a group of similar objects. The paper also discusses the challenges that arise when developing systems that can learn from a single example and how these challenges can be overcome by using various techniques such as deep learning and feedback loops.\tThis paper studies the effect of category number in the one-shot object detection task. In the testing of one-shot detection, there exists a performance gap between the base (training) classes and held-out classes. It is claimed that this performance gap can be largely closed by increasing the number of categories used for training. And the number of categories is more crucial than the number of samples per category. Experiments are conducted on VOC, COCO, Object365, and LVIS using a Siamese-style detector to verify the claims.""715\tThe paper introducing a household robot with deep learning technology that can recognize over 21,000 objects but couldn't recognize the casserole that it was looking for, due to the presence of an olive oil. The story highlights an important challenge for autonomous systems, which is to detect novel, previously unseen objects and learn to recognize them based on a single example (ideally). The paper proposes some solutions to this challenge, such as using additional information about the object, using multiple sensors, or using a larger dataset.\tThis paper shows that the key to reduce the generalization gap between base classes and novel classes is to increase the number of training categories, instead of training samples. The authors did many experiments on four existing datasets (PASCAL, COCO, Objects365, and LVIS) with Siamese Faster R-CNN to verify this point. Experiments show that with more categories in the training set, the generalization gap will be nearly closed. Finally, the author proposes that future data sets should focus on the diversity of categories.""716\tThis paper discusses implicit neural shape functions, such as occupancy fields or signed distance functions, as a promising 3D representation for modeling clean mesh surfaces. It explains that these functions are promising for single-view object reconstruction, but existing approaches require supervision signals from every location in the scene, which is difficult to obtain in real-world scenarios. To overcome this problem, the paper proposes a new solution, differentiable gradient sampling (DGS), which enables backpropagation of the loss on spatial gradients to the feature maps. The DGS solution allows training on large-scale scenes without the need for manually\u6807\u6ce8\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u6570\u636e.\tThis paper presents a method for 3D scene reconstruction from a single image using implicit surface representations such as occupancy or SDF. The authors propose to incorporate loss functions on the spatial gradients to provide dense supervision in the 3D space in the case where 3D labels may be incomplete (e.g. open 3D meshes) or not well-defined everywhere. Experiments are performed on ShapeNet and ScanNet show that the proposed method can achieve competitive performance on single-image scene reconstruction tasks.""717\tThis paper discusses implicit neural shape functions, such as occupancy fields or signed distance functions, as promising 3D representations for modeling clean mesh surfaces. It explains that these functions are promising for single-view object reconstruction, but existing approaches require supervision signals from every location in the scene, which is difficult to obtain in real-world scenarios. To overcome this problem, the paper discusses constraints on the spatial gradient of the implicit field, rather than the value itself, as a source of supervision for single-view reconstruction. The paper also suggests a new solution for differentiably sampling a spatial gradient from a feature map, which enables backpropagation of the loss on spatial gradients to the feature maps. This allows training on large-scale scenes without the need for manually specifying the spatial relationships between features.\tThis paper describes novel loss functions for learning to predict an implicit 3D scene representation from a single image.  They argue that when working with real scan data of scenes (rather than single objects) it is difficult to generate accurate occupancy or signed distance function (SDF) ground truth as would be required for supervised learning.  Instead, they propose to only use occupancy or SDF supervision near the surfaces of objects; elsewhere, they rely on constraints on the gradient of the occupancy or SDF adapted from Gropp et al. 2020.  They perform a thorough evaluation on several benchmark datasets and compare against state-of-the-art competing methods.  They show that they outperform competing methods, even though in some cases their method has access to less supervisory data.    They also perform an ablation study to show the importance of various parts of the loss function.  ""718\tThis paper discusses implicit neural shape functions, such as occupancy fields or signed distance functions, as promising 3D representations for modeling clean mesh surfaces. Existing approaches that use these representations for single-view object reconstruction require supervision signals from every location in the scene, which is difficult to obtain in real-world scenarios. In this paper, the authors propose a new solution for differentiably sampling a spatial gradient from a feature map and backpropagation of the loss on this gradient to the feature maps, allowing training on large-scale scenes. The solution is derived using a novel closed-form differentiable gradient sampling method.\tThis paper presents a new method to learn implicit 3D scene reconstructions from single image input. The main improvement is a closed-form Differentiable Gradient Sampling.  By taking spatial gradient into  consideration, the proposed method can apply back-propagation of the  loss on spatial gradients to feature maps and allow the training for the case of without dense 3D supervision.   ""719\tThis paper discusses implicit neural shape functions, such as occupancy fields or signed distance functions, as 3D representations for modeling clean mesh surfaces. It explains that these functions are promising for this task and that existing approaches that use them for single-view object reconstruction require supervision signals from every location in the scene. However, these signals are difficult to obtain in real-world scenarios where ideal watertight geometric training data is difficult to obtain. To overcome this problem, the paper suggests using constraints on the spatial gradient of the implicit field, rather than the value itself, as a source of supervision. This solution is derived using a novel closed-form Differentiable Gradient Sampling (DGS) solution, which enables backpropagation of the loss on spatial gradients to the feature maps, allowing training on large-scale scenes.\tIn this paper, the authors propose a new method for single view 3D reconstruction.  A conditional (image feature prior) implicit representation framework is proposed to reconstruct 3D scene from a single view. In this paper, the authors propose that feature gradient is essential for watertight reconstruction and propose a differentiable gradient sampling method for the formulation. Experiments have been performed on both synthetic and real datasets. Superior results have been presented.""720\tThis paper discusses the emerging use of foundation models, which are self-supervised learning algorithms that can train on big data, and their transferability to a wide range of downstream tasks. The paper also discusses the increasing scale of these models across domains, including natural language processing, computer vision, and cross-modality representation learning, and the impact that their training has on the financial and environmental costs. The paper also demonstrates the neural scaling law, which states that the number of parameters in a model can grow exponentially as the scale of the problem increases. However, the training of these models can incur high financial costs and even environmental damage due to their massive carbon footprint. The paper also discusses the challenges of training extreme-scale models and suggests that the development of more efficient and resource-effective training frameworks are needed to mitigate the impact of these models on society.\tThis work studies the problem of efficient pretraining of large-scale models for language and vision representations, namely the issue of significant memory requirements for models with billions to trillions of parameters. Authors propose two modifications: first, to reduce the memory load and improve convergence at the initial stage of training, they suggest to train a multilayer model with shared parameters and then unshare them. Second, to maximize GPU utilization with offloading, authors develop a method for granular CPU offloading, which keeps larger chunks of the model in GPU memory. When combined, the proposed methods allow the authors to train a 10 trillion parameter model on 512 GPUs.""721\tThis paper discusses the emerging use of foundation models with self-supervised learning on big data in the field of artificial intelligence, specifically in the areas of natural language processing, computer vision, and cross-modality representation learning. The paper also examines the rapidly increasing scale of these models, with parameters growing from millions to trillions, and the challenges associated with training these models in a distributed manner and under limited resources. The paper highlights the potential benefits of using foundation models, including high transferability and scalability, and suggests that there may be opportunities for cost-effective training of extreme-scale models.\tThe authors propose to train very large neural language models via a \"Sharing-Delinking\" paradigm. The proposed method first trains a model with weights shared across layers. In this way, the model appears to be smaller and it can fit into fewer GPUs. At some point, the authors delink the weights, and continue training the model in the conventional way.  The authors also propose a granular CPU offloading mechanism to save CPU memory.""722\tThis paper discusses the emerging use of foundation models with self-supervised learning on big data as a paradigm for artificial intelligence systems. These models have shown high transferability to a wide range of downstream tasks and multiple modalities, and have grown in scale from millions to trillions of parameters through the advancement of distributed training frameworks and hardware design. However, the training of these models incurs high financial costs and even environmental damage due to the massive carbon footprint. The paper highlights the need for more resources to be allocated to train extreme-scale models and suggests ways to mitigate the costs and environmental impact of training these models.\tThe paper proposes a technique called Pseudo-to-Real (P2R) for reducing the computational and time requirements of training massive (or Giant) models with trillions of parameters. The key idea of P2R is a two phase training approach for Giant models. The first phase involves training a smaller version of the model (a.k.a., Pseudo-Giant) which is obtained by making all layers share parameters. The second phase involves training the Giant model after initializing with Pseudo-Giant weights. The paper further proposes Granular CPU offloading which is to offload some but not all model parameters to CPU memory to reduce GPU memory consumption. Finally, the paper provides some evaluation results to demonstrate P2G.""723\tThis paper discusses the emerging use of foundation models with self-supervised learning on big data as an approach to artificial intelligence systems. These models, which possess high transferability to a wide range of downstream tasks and multiple modalities, have the potential to revolutionize the field of artificial intelligence. The paper also discusses the growing scale of foundation models across domains, from millions to trillions of parameters, and the challenges of training these models with limited resources. The paper also demonstrates the neural scaling law, which suggests that the number of parameters in a model can increase significantly with only a moderate increase in resources.\tThe paper proposed an interesting strategy to reduce the training time for large scale language models consisting of stacking layers with identical structures. Users first trained the models with shared parameters across the layers, then relax the tie constraints so that parameters at different layers are updated differently.  The paper showed some empirical evidence that the proposed strategy converged faster given a limited training time budget and demonstrated the feasibility of training a 10T model.""724\tThis paper discusses energy-based models (EBMs), which are generative models trained using maximum likelihood estimation. However, in certain situations, the trained energy is nonconvex, which makes it difficult to sample the Gibbs distribution associated with it. Using Fenchel duality, the paper derived variational principles dual to EBMs with shallow overparametrized neural network energies, both in the active (featurelearning) and lazy regimes. In the active regime, the dual formulation leads to a faster-than-normal training algorithm that updates the particles in the sample space and the neurons in the parameter space of the energy simultaneously. The paper also presents a variant of the algorithm in which restarts are performed at random samples from the data set, which corresponds to score matching training.\tThis paper derives a Fenchel duality formulation of the maximum likelihood loss of F1-EBMs, which turns the optimization into a min-max problem on probability measures over the sample space. A dual algorithm with mean-field dynamics is proposed to solve this problem. Using the dual aogrithm they also draw a connection between maximum-likelihood training and score matching. Simple numerical experiments on two-layer ReLU network show that the algorithm converges much faster than the original MLE. ""725\tThis paper discusses energy-based models (EBMs), which are generative models trained using maximum likelihood estimation. The training of EBMs becomes challenging in situations where the trained energy is nonconvex, due to the need to sample the Gibbs distribution associated with this energy. Using general Fenchel duality results, the paper derived variational principles dual to maximum likelihood EBMs with shallow overparametrized neural network energies. In the active regime, this dual formulation leads to a training algorithm that updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The paper also discusses a variant of the algorithm in which the particles are sometimes restart at random samples drawn from the data set. The restarts correspond to score matching training.\tThe authors formulated the Fenchel dual problems to maximum likelihood and score matching training applied to energy-based models defined by shallow neural networks. The authors then proposed practical algorithms based on the Hahn decomposition and compared this to various predecessor algorithms. An interesting part is that the learning dynamics can interpolate between maximum likelihood and score matching. There are also rich results in the Appendix. ""726\tThis paper discusses energy-based models (EBMs), which are generative models trained using maximum likelihood estimation. The training of EBMs can be challenging in situations where the trained energy is nonconvex, due to the need to sample the Gibbs distribution associated with this energy. Using Fenchel duality, the paper derived variational principles dual to maximum likelihood EBMs with shallow overparametrized neural network energies, both in the active (featurelearning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm that updates concurrently the particles in the sample space and the neurons in the parameter space of the energy at a faster rate. The paper also discusses a variant of the algorithm in which the particles are sometimes restart at random samples drawn from the data set, and shows that performing these restarts at every iteration step corresponds to score matching training.\tTwo approaches exist for learning a Gibbs measure: either by MLE (minimizing the KL divergence) or by score matching (maximizing the Hyvarinen score). With the same goal to minimizing the KL divergence, the author proposes an alternative training approach by augmenting an auxiliary $\\gamma$ measure and thereby optimizing a more tractable dual problem. The author claims that this new training method has a quicker convergence rate than the primary problem.  ""727\tThis paper studies energy-based models (EBMs), which are generative models trained using maximum likelihood estimation. In general, EBMs are difficult to train because the trained energy is nonconvex, which requires sample from the Gibbs distribution associated with the energy. Using Fenchel duality, the paper derived variational principles dual to EBMs with shallow overparametrized neural network energies, both in the active (featurelearning) and lazy regimes. In the active regime, the dual formulation leads to a faster-than-normal training algorithm by updating the particles in the sample space and the neurons in the parameter space of the energy concurrently. The paper also presents a variant of the algorithm where restarts at random samples from the data set are performed, which corresponds to score matching training.\tThis paper studies the training of Energy Based Models (EBM). The \u201cstandard\u201d technique is maximum likelihood maximization of the observed dataset. While theoretically sound, there are practical difficulties in simulating the MCMC dynamics due to large basins in the energy landscape. For this reason a class of alternative methods, like Score matching, have been explored in the literature. Score matching, however, is deemed to suffer in statistical power.  This work explores how, leveraging known results about Fenchel dualities, it is possible to connect the two training modalities (Maximum Likelihood and Score based) on a continuum, and proposes a practical algorithm based on such considerations. The authors consider for their theoretical analysis of the expressiveness of the models,  shallow neural networks in the lazy and kernel regime (spaces F_1 and F_2 respectively).  Section 2.1 introduces the two considered spaces, referring to the works of Chizat and Bach.   An energy based model is defined at the beginning of section 2.2 by means of the Radon Nikodym derivative. Training an EBM model via maximum likelihood corresponds to maximizing eq. (2), that, when considering networks in F_1, can be rewritten as the expression (3). At the end of Section 2.2. score matching is rigorously defined and the relationship between theoretical and empirical implementations (Hyvarinen) is clarified.  Section 3 is the core of the paper. If Assumption 1 holds (growth conditions), then Thm 1 states that eq. (3) is the Fenchel dual of (5). As (5) is a static functional, the authors propose to use results from Chizat and Santambrogio to rewrite it as the dynamical system (eq. (6))  together with the definitions in (7). Importantly, this formulation contains \\alpha, a free parameter >0, that determines relative time-scales over which functions \\gamma and \\nu are updated. System (6) is still \u201calgorithmically\u201d unsolvable. The authors then propose to use the two following approximations: first, the mean field technique known as propagation of chaos, is used to switch from (6)  to the system (8), where gamma is substituted by neural network  parameters and \\nu by and SDE. Then the continuous time system (8) is discretized using Euler-Maruyama. Algorithm 1 is the result of the process.  Section 4 explores how, in the infinite alpha regime, the dynamical system (6) is equivalent to score matching. By means of eq. (11) and proposition (3) the technical analysis is performed. Intuitively, parameters are updated infinitely faster than the samples.  Section 5 explores the implementation of Algorithm 1 on a synthetic dataset. The authors consider two cases (the two teacher neurons aligned to 164 and 78 degrees respectively). Comparison of the primal and dual (alpha<<1, alpha>>1) shows that the dual performs much better than the primal.  The conclusions stress the theoretical contribution based on Fenchel duality and propose as future work the exploration of convergence rates of (6) and (10) and numerical investigation of the restarting probability p_R. In addition, the authors suggest that \u201cporting\u201d the presented work to deeper network architectures (e.g. to work on realistic dataset, such as images) is straightforward from the practical point of view, and unpractical from the theoretical point of view. ""728\tThis paper discusses the issue of finding tight lower bounds for the number of queries needed to apply a fingerprinting code in differential privacy. The problem is particularly interesting because it highlights the fundamental limits of DP-ERM, the process of applying a privacy\u4fdd\u62a4\u7b97\u6cd5 to a dataset using a recognition algorithm. The paper provides tight lower bounds for both the constrained and unconstrained cases, which are off by some logarithmic terms from the current best possible bounds. The paper also presents a novel biased mean property for fingerprinting codes, which allows for the achievement of tight lower bounds. Finally, the paper discusses some of the techniques used in the previous literature to achieve tight lower bounds and their independent interest.\tThis paper presents tight lower bounds on differentially private ERM, a well-studied topic in the DP literature. It obtains tight bounds for both the constrained and unconstrained settings. The exact quantitative improvements are stated precisely in the abstract, and I won't repeat them here. The improvements are not staggering, but they are tight, and are presumably the final word on this topic.   ""729\tThis paper discusses the lower bounds for the fingerprinting code length in differential privacy. The paper provides tight lower bounds of O(\u221aplog(1/\u03b4)n) and O(\u221aplog(1/\u03b4)n) for both the constrained and unconstrained cases, respectively, which are believed to be tight. The paper proposes a new biased mean property for fingerprinting codes and achieves the tight lower bounds by using a novel `2 loss function instead of linear functions considered in previous papers. Additionally, the paper introducing an auxiliary dimension to simplify the computation of `2 loss. The results close a gap in our understanding of DP-ERM by presenting the fundamental limits of the problem. The techniques may be of independent interest and help enrich the tools so that it readily applies to problems that are not easily reducible from one-way marginals.\tThe paper describes some lower bounds for differentially private empirical risk minimization (DP ERM). The main results are:  * An $\\Omega(\\sqrt{p \\log(1/\\delta)}/\\epsilon n)$ lower bound for unconstrtained ERM under approximate differential privacy. This improves on prior work of Bassily et al. in the presence of the $\\log(1/\\delta)$ term, and in the optimization being unconstrained.  * An $\\Omega(p/\\varepsilon n)$ lower bound for pure differential privacy.  The proofs use now standard techniques from the literature: fingerprinting codes for approximate DP and a packing argument for pure DP.""730\tThis paper presents tight lower bounds on the expected running time of the DP-ERM algorithm, which is a popular algorithm for finding the shortest paths in a graph. The DP-ERM algorithm first uses fingerprinting codes to store the connectivity information of the graph, and then uses a biased mean property to protect the connectivity information from being\u6cc4\u9732. The goal of the algorithm is to find the shortest path between two nodes in the graph, which can be done in $O(\u221ap\\log n)$ time with $p$ being the degree of the nodes.\n\nHowever, current lower bounds on the expected running time of the algorithm are off by some logarithmic terms, in particular, $O(\u221ap n)$ for constrained case and $O(\u221ap\\log p)$ for unconstrained case. The paper presents tight lower bounds of these forms, which are believed to be tight. The paper also presents a novel biased mean property for fingerprinting codes and a novel `2` loss function for pure-DP, which achieve the first ( tight ) $O(p n)$ lower bound. The paper also introduce an auxiliary dimension to simplify the computation brought by `2` loss.\n\nThe paper closes a gap in our understanding of DP-ERM by presenting the fundamental limits of the algorithm. The techniques may be of independent interest, which help enrich the tools so that it readily applies to problems that are not ( easily ) reducible from one-way marginals.\t## Summary of Contributions  This paper studies the unconstrained empirical risk minimization (ERM) under differential privacy (DP). In this setting, there is a loss function $\\ell: \\mathbb{R}^p \\times \\mathcal{X} \\to \\mathbb{R}$ and we are given $x_1, \\dots, x_n \\in \\mathcal{X}$; the goal is to output $\\theta$ that minimizes the empirical loss $L(\\theta; X) := \\frac{1}{n} \\sum_{i=1}^n \\ell(\\theta; x_i)$. The goal is to minimize the excess empirical loss $\\mathbb{E}[L(\\theta; X) - \\min_{\\theta^* \\in \\mathbb{R}^p} L(\\theta^*; X)]$. We want our algorithm to satisfies $(\\epsilon, \\delta)$-DP. Recall that the case $\\delta > 0$ is referred to as *approximate-DP* whereas the case $\\delta = 0$ is referred ti as *pure-DP*. Here we assume that $\\ell$ is 1-Lipchitz; the results easily extends to $C$-Lipchitz functions with an extra multiplicative factor of $C$ in the excess empirical loss.  The main contributions of the paper are: 1. In the approximate-DP setting, the authors show a lower bound of $\\Omega\\left(\\frac{\\sqrt{p \\log(1/\\delta)}}{\\epsilon n}\\right)$. This improves upon the best known bound of $\\Omega\\left(\\frac{\\sqrt{p}}{\\epsilon n \\log p}\\right)$ in the unconstrained case from [Asi et al., 2021] and $\\Omega\\left(\\frac{\\sqrt{p}}{\\epsilon n}\\right)$ in the constrained case [Bassily et al., FOCS 2014]. The new lower bound also matches the known upper bound in both cases [Bassily et al., FOCS 2014]. 2. In the pure-DP setting, the authors show a lower bound of $\\Omega\\left(\\frac{p}{\\epsilon n}\\right)$.  To prove 1., the authors reduce from the 1-way marginal problem (similar to previous work). Recall that in 1-way marginal, we are given $x_1, \\dots, x_n \\in \\\\{-1, 1\\\\}^p$ and the goal is to approximate $\\frac{1}{n} \\sum_{i=1}^n x_i$; a lower bound of $\\Omega(\\frac{\\sqrt{p \\log(1/\\delta)}}{\\epsilon n})$ is known for the problem [Bun et al., STOC 2014]. The authors use an $\\\\ell_1$-distance loss function, i.e., $\\\\ell(\\\\theta; x_i) = ||\\\\theta - x_i||\\_1$. Notice that here the optimal solution is $\\\\theta^*\\_j = sign(\\sum\\_{i} z\\_{i, j})$. This in spirit is very similar to 1-way marginal but not exactly the same. Specifically, if $\\sum_{i} z_{i, j}$ is roughly around zero, then taking $\\theta_j = -1$ or $\\theta_j = 1$ does *not* effect the loss too much. Therefore, a direct \"blackbox\" reduction from 1-way marginal does not seem to work. To overcome this, the authors observe that actually in the construction of [Bun et al., STOC 2014] most of the coordinates' means are not close to zero (formalized as \"biased mean\" property in the current paper) and thus the hard instance gives the desired lower bound for DP ERM.  To prove 2., the authors use a standard packing-style construction together with the $\\\\ell_2$-distance loss function""731\tThis paper presents tight lower bounds on the amount of information\u6cc4\u9732 (i.e., the size of the\u6cc4\u9732 set) for the DP-ERM problem, which is a type of privacy- preserving\u62db\u4fe1\u95ee\u9898 (PP\u6027\u95ee\u9898) where the\u62db\u4fe1\u5bf9\u8c61 (query message) is mixed with a certain distribution and the privacy of the\u62db\u4fe1\u5bf9\u8c61 is desired. The DP-ERM problem is typically phrased in the context of computer vision, where the\u62db\u4fe1\u5bf9\u8c61 is a image or a set of images and the goal is to estimate the\u62db\u4fe1\u76ee\u6807 (target variable) from the mixed image.\n\nThe current best lower bounds on the amount of information\u6cc4\u9732 are of the form O ( \u221a p log ( 1/\u03b4 ) n ), which is believed to be tight, where n is the number of\u62db\u4fe1\u5bf9\u8c61 and p is the number of\u62db\u4fe1\u6837\u672c. However, the current lower bounds are off by some logarithmic terms, in particular for the constrained case, where the number of\u62db\u4fe1\u6837\u672c is fixed and the number of\u62db\u4fe1\u5bf9\u8c61 is large, and for the unconstrained case, where the number of\u62db\u4fe1\u6837\u672c is large and the number of\u62db\u4fe1\u5bf9\u8c61 is fixed.\n\nWe achieve tight lower bounds of \u03a9 ( \u221a p log ( 1/\u03b4 ) n ) and \u03a9 ( p n ) for both cases by introducing a novel biased mean property for fingerprinting codes and utilizing a novel ` 2 loss function instead of linear functions considered by previous papers. We also introduce an auxiliary dimension to simplify the computation brought by ` 2 loss.\n\nOur results close a gap in our understanding of DP-ERM by presenting the fundamental limits of the problem. Our techniques may be of independent interest, which help enrich the tools so that it readily applies to problems that are not ( easily ) reducible from one-way marginals.\tThis paper studies differentially private empirical risk minimisation (ERM) in the unconstrained setting. It gives tight lower bounds for approximate DP ERM for general loss functions, which also implies the same lower bound for the constrained case, which is an improvement over a classic lower bound by Bassily et al 2014. It also gives a lower bound for unconstrained pure DP ERM that recovers the result in the constrained case.""732\tThis paper discusses the Wasserstein gradient flow, a model for the dynamics of probability density functions over the space of functions with respect to the Wasserstein metric. It was first discovered by Jordan, Kinderlehrer, and Otto in their work in 1998 and brought great physical insights to the Fokker-Planck equation. The Wasserstein gradient flow is the gradient dynamics of the free energy with respect to the Wasserstein metric, and it has been important in many areas such as optimal transport, PDEs, physics, machine learning, and many more. Despite the abundant theoretical results, the computation of the Wasserstein gradient flow remains a challenge. Most existing methods are efficient but not always general or apply to different cases. This paper provides a summary of the key points of the Wasserstein gradient flow, including its properties, the Fokker-Planck equation, and its applications. It also presents a new computational method for computing the Wasserstein gradient flow that is general and efficient.\tThis paper proposes a method to solve Wasserstein gradient flows based on the JKO scheme using variational formulations of functional objectives, such as the KL divergence or the generalized entropy (non-linear diffusion). Relying on known reformulations of the JKO scheme as optimization over convex functions, the paper departs from recent related methods in expressing certain objectives as f-divergences, and in turn using the dual formulation of these divergences to circumvent the need to do explicit density computation in these. The resulting method involves parametrizing two types of operators as neural networks (one of them as an input-convex neural network), and solving a mini-max objective. The paper presents experiments on simple PDEs (mostly in 1D or 2D) with known solutions. ""733\tThis paper discusses the Wasserstein gradient flow, a dynamics over the space of probability densities with respect to the Wasserstein metric. It was first discovered by Jordan, Kinderlehrer, and Otto in their work in 1998. The Fokker-Planck equation is in fact the Wasserstein gradient flow of the free energy, bringing physical insights to this type of partial differential equations. The Wasserstein gradient flow has played an important role in optimal transport, PDEs, physics, machine learning, and many other areas. Despite the abundant theoretical results on the Wasserstein gradient flow established over the past decades, the computation of it remains a challenge. Most existing methods are efficient but time-consuming, and there is still much room for improvement.\tThis paper studies the implementation of some Wasserstein Gradient Flows (WGF) in discrete time but without discretizing the space. The methods proposed are based on the JKO operator to discretize WGF in time. The implementation of the JKO can be challenging. The strategy of the authors is to first reparametrize the JKO as a minimization over a space of functions (instead of measures) via pushforward. Then, when the objective function is a f-divergence, the objective inside the JKO admit a variational representation and can be expressed as a sup. Conclusion: each JKO is written as a min max over a space of functions. To solve it, they parametrize the functions by neural networks and alternatively maximize and minimize the problem using Adam. An important feature is that the objective in the min max can be approximated with samples of the current distribution (its density doesn't appear, only integrals wrt to the current distribution). ""734\tThe Wasserstein gradient flow is a type of gradient flow equation that models the dynamics of probability density functions over the space of continuous functions. It was first introduced by Jordan, Kinderlehrer, and Otto in their work in 1998 and has since become an important tool in many fields, including optimal transport, PDEs, physics, machine learning, and more. The Wasserstein gradient flow is defined by the equation: $\\frac{\\partial f}{\\partial t} = \\nabla_{\\cdot} J(f)$, where $f$ is a probability density function, $J$ is the Wasserstein metric, and $\\nabla_{\\cdot}$ is the gradient operator with respect to $J$ (see Ambrosio et al., 2008 and Santambrogio, 2017 for more details). The Wasserstein gradient flow models the gradient dynamics of $f$ with respect to $J$, and it has many applications in various fields, such as machine learning, physics, and optimal transport.\n\nOne of the most important applications of the Wasserstein gradient flow is in the optimization of cost functions, such as the free energy of a system. The Wasserstein gradient flow is used to find the optimal density function that minimizes a cost function, such as the total energy of the system. This has many practical applications in fields like energy management, machine learning, and more.\n\nDespite the existence of many theoretical results on the Wasserstein gradient flow, its computation remains a challenge. Many existing methods for computing the Wasserstein gradient flow involve complex optimization algorithms, such as gradient descent and steepest descent, and require large computational resources. However, some recent advances have made the computation of the Wasserstein gradient flow more practical and efficient (Ambrosio et al., 2008, Santambrogio, 2017).\n\nOverall, the Wasserstein gradient flow is an important tool in many fields, and its applications are vast and continue to expand. Despite its challenges, the computation of the Wasserstein gradient flow has many practical and theoretical applications in fields like machine learning, physics, and optimal transport.\tThis paper proposes a variational formulation of each JKO step for optimizing functionals on measures. Different from existing recent works on emulating JKO steps by training pushforward neural networks (either directly or as gradients of convex functions), the variational formulation involves another inner maximization of a function, without needing density access that typically requires cubic time complexity due to computing the log determinants of the pushforwards. Experiments are done to demonstrate the practicality of the algorithm. ""735\tThis paper presents an overview of the Wasserstein gradient flow, a powerful tool for studying the behavior of probability distributions over a\u7ef4\u7eb3stein space. The Wasserstein gradient flow is a model for the dynamics of a probability density over a\u7ef4\u7eb3stein space, which was first discovered by Jordan, Kinderlehrer, and Otto (JKO) in their work in 1998. The Fokker-Planck equation, which models the behavior of a free energy in optimization problems, is a special case of the Wasserstein gradient flow. The Wasserstein gradient flow has important implications for optimal transport, optimal control, physics, machine learning, and other areas. Despite the availability of a large body of theoretical results on the Wasserstein gradient flow, computational methods for computing it remain a challenge. The paper presents a overview of existing methods for computing the Wasserstein gradient flow, including methods for numerical optimization, computer vision, and machine learning. The paper also discusses some of the challenges and future directions in this field.\tThe paper proposes a method to compute Wasserstein Gradient Flows (WGFs) via neural networks and the JKO scheme. In contrast to prior works, to compute WGFs of functionals involving f-divergences, the authors use variational approximations rather than direct computations. It is claimed to work faster and perform better.""736\tThis paper explores the problem of estimating the performance of an algorithm portfolio on a particular problem instance. The paper highlights the importance of this issue in various domains, including Constraint Programming,Satisfiability, Machine Learning, and highlights the use of general performance models and manually designed meta-features as effective approaches to this problem.\tThe paper presents an approach, called MetaBu, for learning a meta-feature embedding from an existing meta-feature space into a latent space, which is aims at being rank preserving regarding different hyper-parameter configurations. The special kind of embedding and its property of aiming at being performance preserving in the context of AutoML is the main contribution of the paper, in my opinion. The quality of the learned meta-features is assessed through different experiments such as capturing to what degree the embedding is indeed performance preserving and how well AutoML tools perform when initialized with the corresponding meta-features. Moreover, the authors provide a sensitivity analysis of relevant hyper-parameters of MetaBu and demonstrate how to gain insights from the learned embeddings.""737\tThis paper provides an overview of the current state of the art in the field of algorithm optimization and performance estimation, focusing on the use of general performance models and manually designed meta-features in the context of supervised Machine Learning. The paper discusses the challenges and limitations of these approaches, and highlights the need for further research in this area to develop more effective optimization algorithms and meta-features for different problem domains.\tIn this paper, the authors address the AutoML problem, which aims to automatically select the best ML algorithm and its hyperparameter configuration for a dataset, and propose an approach to this problem that learns meta-features of the dataset. The proposed method, MetaBu, learns new meta-features by optimal transport according to the space of distributions of hyperparameter configurations. Meta-features in MetaBu is known only once and induce a topology in a set of data sets. Experiments on the OpenML CC-18 benchmark have shown that MetaBu meta-features can improve the performance of the state-of-the-art AutoML systems AutoSklearn and Probabilistic Matrix Factorization. Furthermore, the examination of MetaBu meta-features provides hints on when an ML algorithm will work. Finally, a topology based on MetaBu meta-features can estimate the intrinsic dimension of the OpenML benchmark for a given ML algorithm or pipeline. ""738\tThis paper explores the problem of estimating the peak performance of an algorithm portfolio on a particular problem instance. The paper explores the use of general performance models and manually designed meta-features in the context of supervised Machine Learning and Constraint Programming, as well as the limitations of these approaches in other domains. The paper also discusses the potential future directions of this research.\tThis paper tackles the AutoML problem. It proposes to learn a linear combination of manually designed meta-features, which aligns meta-features with the space of hyper-parameter configurations via an Optimal Transport procedure. Experiments on OpenML benchmark demonstrate the power of the proposed method on boosting AutoML systems.""739\tThis paper provides an overview of the main challenges and bottlenecks in using algorithms to solve problems, particularly in domains such as Constraint Programming, Satisfiability, and Machine Learning. It discusses the use of general performance models and manually designed meta-features in the context of supervised Machine Learning and highlights the importance of understanding the problem instance and the dataset in order to achieve optimal performance. The paper also discusses recent advances in using machine learning techniques to address these challenges, such as deep learning and natural language processing.\tThis paper focuses on the AutoML problem for tabular data and proposes a meta-learning based novel solution. They consider the optimal transport to define distances between two datasets, utilizing the Wasserstein-Gromov distance between the distribution of the top performing hyperparameters for the respective datasets. Given this distance, they propose learning a linear transformation of existing dataset meta-features such that the Euclidean distance between a pair of datasets in this transformed space is proportional to their Wasserstein-Gromov distance. This method is termed Metabu.  The empirical evaluation compares Metabu to existing meta-learning schemes on (i) their ability to capture the desired Wasserstein-Gromov distance, (ii) their ability to find better hyperparameters via sampling without an underlying optimizer, and (iii) their ability to find better seed hyperparameters for hyperparameter optimizers. The results on the OpenML CC-18 suite with 3 machine learning models indicate that Metabu significantly improves upon existing meta-learning schemes. The paper also demonstrates how the learned linear transformation of existing dataset meta-features allow us understand the importance of different existing dataset meta-features and how these vary between machine learning models.  ""740\tThis paper discusses the concept ofFederated learning and its applications in distributed learning. It provides an overview of the basic principles of Federated learning and discusses how it can be used to improve learning efficiency and reduce data privacy concerns. It also discusses some of the most popularFederated instantiations, such as FedAvg, and their applications in various fields. Finally, the paper concludes by highlighting the challenges and future directions of Federated learning.\tThis paper proposes a customisation strategy named \"Split-Max\" for federated learning. The authors identify the heterogeneity of devices and data in FL scenarios. They present the importance of considering devices' budgets and dynamics when dispatching training models. Split-max can adjust the model size according to the devices' budget while maintaining good accuracy and robustness.  Split-max works in steps. First, multiple base models from different initialisations are trained to improve diversity. These base models are randomly given to clients to extract generalisable features. Then, base models are aggregated to the server. Secondly, to provide devices with models with different robustness, it trains two similar models together to capture both the standard-training accuracy and adversarial-training accuracy. Then layer-wise mixing is conducted to achieve both standard accuracy and adversarial accuracy.   Experiments show that Split-Mix achieves better accuracy than naive approaches. Moreover, with customisation, the models are smaller and more robust under budget constraints.""741\tThis paper discusses the distributed learning paradigm of Federated Learning, which allows remote participants to learn from and share their knowledge without having their raw data transferred to a central server. It also discusses the most popularFederated instantiation, FedAvg, which averages homogeneous model parameters to aggregate knowledge. The paper also discusses the impact of data heterogeneity and resource heterogeneity on Federated Learning and how these factors can be addressed in practice.\tSplit-Mix is a Federated Learning strategy aimed at easing the problems that arise when a heterogeneous pool of devices/clients (i.e. some with more compute/memory capabilities than others, different data distributions) collaboratively train a global model. Split-Mix trains a model that can later on be customized in terms of model size and robustness. As the name suggests, Split-Mix has two stages: During `split`, a large model is split into smaller base models. Base models are constructed by discarding channels but maintaining the the number and type of layers in the network. During `mix`, the server samples a fraction of the base models (depending on a given client's compute capabilities) and fuses them into a single one that is send to a client to train. All base models are trained on all clients (should these meet the compute requirements of a sub-model) in a federated manner. This means that the more capable devices, train all base models. This is envisioned to happen in a parallel fashion in a given device. Once FL training is completed, a customized model can be deployed to a device/client in hardware-aware and robustness-ware fashion. The Authors refer to this as _in-situ customization_ ""742\tThis paper discusses the distributed learning paradigm of Federated Learning and its various instantiations, including FedAvg.Federated Learning allows data from remote participants to be used to aggregate their knowledge without the need for their raw data to be transferred to a central server, reducing concerns about data security and privacy. In many practical scenarios, the participants have different levels of heterogeneity, including data and hardware resources. This heterogeneity can lead to challenges in training models and achieving good performance. This paper provides an overview of the challenges and proposes solutions forFederated Learning.\tThis paper presents a new federated learning approach named Split-Mix FL that allows clients to train customized models efficiently while considering heterogeneity in data and computation resources. The key idea is threefold: 1) the global model is first split into several sub-networks called base models that each have different sizes, thus requiring a different amount of computational resources; 2) Each selected client trains a random subset of base models, under its computational resource constraints; 3) updated base models are aggregated at the server-side and distributed again. Furthermore, the proposed approach can train both accurate and robust models in a joint fashion, where all but batch-norm layers are shared for efficiency. Experimental results on multiple datasets (CIFAR10, Digits, DomainNet) demonstrate the effectiveness of the proposed approach compared to FedAvg and HeteroFL.""743\tThis paper provides an overview of Federated Learning, a distributed learning paradigm that allows remote participants to share data and collectiveize their knowledge without transfer of raw data to a central server, reducing concerns about data security and privacy. It discusses the benefits ofFederated Learning, including improved performance and reduced computational resources required, among others. The paper also discusses the most popularFederated instantiations, including FedAvg, and the challenges of usingFederated Learning, such as data heterogeneity and resource heterogeneity among participants.\tThe paper proposes a new federated learning scheme that is suitable for devices with heterogeneous resources. The proposal, namely Split-Mix, trains multiple models of different sizes and adversarial robustness levels, tailored to the budget of each device. Empirical results demonstrate the efficacy of the method against the main competitor.""744\tThis paper discusses the challenges of training scalable first-order methods for nonconvex-nonconcave constrained minimax problems in machine learning. The paper first defines the problem and discusses the traditional approach to solving it using variational inequalities (VIs). It then presents recent advances in the extragradient-type algorithms and their ability to stabilize training and avoid Poincar\u00e9 recursions. Finally, the paper concludes by highlighting the limitations of these methods and suggestions for future work.\tThis paper constructs methods named CurvatureEG+ (and Adaptive EG+ and CEG+), built upon a recently proposed EG+ [Diakonikolas et al., 2021], a variant of EG, that works under the weak MVI condition. Most importantly, the CurvatureEG+ (and Adaptive EG+/CEG+) works for a range (of the weak MVI) larger than that of EG+. The corresponding nonconvex-nonconcave setting includes non-trivial problems illustrated in the paper, where the proposed method converges, while other existing methods reach limit cycles. Unlike EG+, the CurvatureEG+ can handle both constrained and composite cases. A stochastic variant is also studied. Although the experiments consider toyish problems, they seem interesting.""745\tThis paper discusses the challenges faced by scalable first-order methods in solving nonConvex-nonconcave constrained minimax problems in machine learning. The paper explores the history of the issue, the traditional study of minimax problems under the umbrella of variational inequalities (VIs), and recent algorithms for solving the problem, including the extragradient-type algorithms from the VI literature. The paper also discusses the potential applications of these methods in machine learning and the challenges of using them in practice.\tThis paper focuses on variants of ExtraGradient (EG) by considering different options for the two step-sizes of the method: one for the extrapolation step and the second for the iterate update.  Building on the analysis of (Diakonikolas et al. 2021),  the paper relaxes the setup therein by allowing a larger range of values for the $\\rho$ parameter (see Fig. 2) of weak Minty Variational Inequality (MVI) which parameter controls the degree of nonconvexity.  In particular, it provides the following contributions: *(i)* Primarily it defines Alg.1 where the step size for the iterate update is adaptive and shows the main convergence result (Thm  3.1.) that under some assumptions (Asm 1), Alg. 1 converges on weak MVI problem. *(ii)* It then considers a non-adaptive variant of Alg. 1, dubbed CEG+, which can be seen as a generalization of the EG+ method of (Diakonikolas et al. 2021), and complements the result of the latter by showing that the convergence result is tight on weak MVI; *(iii)* extends CEG+ by using an adaptive scheme for the first step size (for the extrapolation) which uses Lipschitz constant backtracking, and Alg.1 for the latter step size (for the iterate update), dubbed CurvatureEG+, which method is shown to empirically converge on some toy-examples on which CEG+ does not; *(iv)* finally, for the stochastic setup the authors consider one of the step-sizes to be diminishing and the other can be constant, and show that this variant converges on weak MVI problems. ""746\tThis paper discusses the challenges of training scalable first-order methods for nonconvex-nonconcave constrained minimax problems in machine learning. The paper\u5148\u4ecb\u7ecd\u4e86\u4e00\u4e9b machine learning applications that result in such problems, and then discusses traditional studies of minimax problems under the umbrella of variational inequalities (VIs). It then\u4ecb\u7ecd\u4e86 recent extragradient-type algorithms from the VI literature that have been brought to the attention of the machine learning community, and which have provided a principled way of stabilizing training and avoiding Poincar\u00e9 recursions. However, the paper\u8b66\u544a that these results mostly concern the concavity of the problem, not its convexity.\tThis paper proposed CEG+ and CurvatureEG+, which extend extragradient method to a proximal variant (regarding the operator $A$), and apply it in nonconvex-nonconcave minimax optimization with Weak MVI condition in both deterministic and stochastic cases, and proved their complexities, which has the same order as those in literature (e.g., Diaonikolas et al., 2021). The authors also studied the lower bound in the simpler case when $A\\equiv 0$, showing a difference compared to EG+ in (Diaonikolas et al., 2021).  In the deterministic case, it proposed an adaptive stepsize strategy to allow larger range of the MVI parameter $\\rho$, and further a curvature-based strategy to avoid the lower bound requirement of $\\rho$. The authors also executes several experiments, showing that CurvatureEG+ can avoid cycling in the experiments.""747\tThis paper discusses the challenges of solving nonconvex-nonconcave constrained minimax problems in machine learning applications. The problems arise when using scalable ( stochastic ) first order methods, which are commonly used for training deep neural networks. The paper\u8ba8\u8bba\u4e86\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u4ece\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GANs)\u5230 robust reinforcement learning \uff0c\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u90fd\u5bfc\u81f4\u975econvex-nonconcave\u7684 constrained \u6700\u5c0f\u5316\u95ee\u9898\uff0c\u8fd9\u7ed9 scalable ( \u968f\u673a ) \u7b2c\u4e00\u987a\u5e8f\u65b9\u6cd5\u5e26\u6765\u4e86\u5f88\u5927\u7684\u56f0\u96be\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6709\u5f88\u591a\u7ed3\u679c illustrate divergent\u6216 cycling behavior \uff0c\u5f53\u6269\u5c55\u5230\u6700\u5c0f\u5316\u95ee\u9898\u65f6\uff0c\u8d85\u51fa\u4e86 VI \u7684\u6db5\u76d6\u8303\u56f4\u3002\u4f20\u7edf\u7684 minimax \u95ee\u9898\u4e00\u76f4\u53d7\u5230 VI \u7814\u7a76\u7684\u542f\u53d1\uff0c\u6700\u8fd1 Mertikopoulos \u7b49\u4eba\u5c06 extragradient-type \u7b97\u6cd5\u5f15\u5165\u5230\u673a\u5668\u5b66\u4e60\u793e\u533a\u4e2d(Mertikopoulos \u7b49\u4eba\uff0c2018a; Gidel \u7b49\u4eba\uff0c2018; B\u00f6hm \u7b49\u4eba\uff0c2020)\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd principled \u7684\u65b9\u5f0f\u6765\u7a33\u5b9a\u5730\u8bad\u7ec3\u548c\u907f\u514d Poincar\u00e9  recursions\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e3b\u8981\u6d89\u53ca VI \u4e2d\u7684 concavity \u95ee\u9898\uff0c\u800c\u4e0d\u662f convexity \u95ee\u9898\u3002\tThis work extends the extragradient algorithm in Diakonikolas et al. (2021) from unconstrainted and unregularized inclusion problems that satisfy weak Minty inequality (MVI) to their constrained and regularized counterparts. Compared with the original extragradient algorithm, the extended algorithm is proved to converge with a larger range of stepsize choices and MVI-related constant $\\rho$ (implying a larger set of applicable problems) in both deterministic and stochastic inclusion problems. The range of $\\rho$ is also proved tight by providing a lower bound of $\\rho$. The extended algorithm also generalizes the celebrated forward-backward-forward (FBF) algorithm in Tseng (2000). Finally, an improvement of this extended algorithm is proposed using Lipschitz constant backtracking. ""748\tThis paper proposes a new neural contextual bandit algorithm that has finite-time regret, where the learning time horizon is measured in terms of the number of rounds. This is possible because the algorithm only explores the last layer of the deep neural network, which reduces computational complexity and makes the algorithm faster to learn. The paper also compared the performance of the algorithm with existing neural contextual bandit algorithms and found that it has a lower finite-time regret than those algorithms. Additionally, it was shown to be more efficient than existing algorithms in terms of computational complexity.\tThe paper presents a new neural-bandit algorithm with shallow exploration and provides a regret bound for the proposed method. The existing approaches have introduced deep neural networks based bandit algorithms to learn reward functions, in which exploration takes place over the entire network parameter space, which can be inefficient for large-size networks which are typical in NTK based approaches. The authors address this by taking an existing approach that decouples the deep neural network feature representation learning from most of the exploration of the network parameters by only exploring over the final layer of the network.   Despite the fact that this idea of shallow exploration has been proposed previously, there has not been a theoretical analysis with a regret bound. The authors analyze a UCB version of this approach, then build from techniques from both deep neural contextual bandits and linear contextual bandits to prove an O(\\sqrt(T)) regret bound. Finally, the authors present experimental results to show that their algorithm work well in practice.""749\tThis paper discusses a new approach to neural contextual bandit algorithms, which is computationally more efficient than existing ones. The paper\u6307\u51fa\uff0c existing neural contextual bandit algorithms, such as those by Auer et al. (2002), Audibert et al. (2009), and Lattimore & Szepesv\u00e1ri (2020), have a finite-time regret, where the regret is the difference between the optimal policy and the policy that is learned by the algorithm. The paper suggests that this can be improved by exploring only the last layer of the deep neural network, which reduces the number of nodes that need to be trained. This approach is computational more efficient because only the last layer of the deep neural network is explored, which has less nodes than the entire network. The paper also discusses the implications of this approach for different types of neural networks and the learning rate optimization problem.\tThis paper studies neural contextual bandits and proposes an algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network, and uses an UCB approach to explore in the last linear layer. Compared with existing neural contextual bandit algorithms, the proposed algorithm attains computation efficiency. Regret guarantees and empirical results are provided to demonstrate the effectiveness of the proposed algorithms""750\tThis paper discusses a new approach to neural contextual bandit algorithms, which is computationally more efficient than existing ones. The paper\u6307\u51fa\uff0c existing neural contextual bandit algorithms have a finite-time regret, where the learning time horizon is limited. The new approach, which only explores the last layer of the deep neural network, has a lower regret than the existing algorithms. The paper also discusses the technical details of the new approach and its performance on a dataset.\tThis paper study a novel contextual bandit algorithm: Neural-LinUCB. As in (Riquelme et al 2019), the idea of this algorithm is based on decoupling deep representation learning and exploration. A deep neural network learns the mapping between the context $x_{t,a_t}$, while a linear bandit, OFUL (Abbasi-Yadkori 2011), chooses the arm to play. In contrast to (Riquelme et al 2019), a regret upper bound of the algorithm a regret upper bound of the algorithm is stated in Corollary 4.6. The proposed algorithm is also an improvement over NeuralUCB (Zhou et al 2020) for two reasons: the computational cost of the exploration is lesser, since the exploration is only done in the last layer of weights, and the regret upper bound is tighter. Indeed, in contrast to (Zhou et al 2020) it does not depend on the dimension of the tangent kernel matrix, which can be in O(KT). Experiments, done on four contextual bandit problems, show that Neura-lLinUCB outperforms LinUCB and performs as well as NeuralUCB and NeuralTS. ""751\tThis paper discusses a new approach to neural contextual bandit algorithms, which is computationally more efficient than existing ones. The paper presents a deep neural network architecture that can explore more effectively in the last layer of the network\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u63d0\u9ad8\u6027\u80fd\u3002 The approach is compared with existing neural contextual bandit algorithms and found to have a finite-time regret that is within a factor of \u221aT, where T is the learning time horizon\u3002 The paper also discusses the potential applications of the approach in real-world bandit problems\u3002\tAuthors tackle the setting of contextual bandits, using deep representation learning combined with an upper confidence bound algorithm. The main contribution of this work is to provide a regret bound for the setup which decouples the representation learning from the UCB search, by searching only over the last layer of the network. The setting had been studied before, but only empirically, and using Thompson sampling rather than UCB. Authors validate their results empirically on several domains from the UCI data repo, as well as on MNIST, comparing against state of the art baselines. ""752\tThis paper discusses the reinforcement learning (RL) process and its use in achieving a goal in an environment. The RL agent learns how to map states to actions in order to maximize a long-term cumulative award signal. The paper also discusses the various artifacts of an RL algorithm, such as the transition probability and reward function, and how they can be improved for better performance. The paper also provides an example of how an RL algorithm can be used in a real-world environment to help an employee learn how to use a computer system more effectively.\tThe paper proposed a data-driven method for optimal action space selection in a reinforcement learning problem. Given a set of training state, action pair, the proposed approach first filters out the set of indispensable action set and then rank the other action set according to their cumulative reward values. To further improve the efficiency, a Monte Carlo sampling method is proposed for cut-off cardinality computation for action space. An action update rule is devised by computing optimal step size. Finally, a case study on a cloud environment is illustrated to select the optimal set of resources (#vCPUs and Memory size) to optimize the CPU utilization rate. The case study demonstrates that the Monte Carlo sampling based algorithm reduces action search space by 81% and then it creates a list of ranked action set. It is shown that a large action space does not necessarily lead to better performance for an RL agent. ""753\tThis paper discusses the principles of reinforcement learning (RL) and how it can be used to solve problems in an environment. The paper defines an RL problem by specifying the four components (S, A, Pa, and Ra) of an environment, where S is a set of states, A is a set of actions, Pa is the transition probability, and Ra is the reward received after transition. The RL agent learns how to map states to actions in order to maximize the long-term cumulative reward signal in the environment. The paper also discusses the various artifacts of an RL algorithm, such as the Policy Gradient algorithm, the importance of importance feedback, and the need for a well-defined reward function. The paper also provides an example of how an RL agent can be used to solve a problem in a real-world environment.\tThe paper addresses the problem of action set selection, i.e. identifying which actions should be available to a RL agent, during training. The set of available actions can influence the RL agent's performance or even hinder it to reach its goal. A method to evaluate action sets is introduced and a case study is performed on a resource tuning example on cloud infrastructure.""754\tThis paper discusses the use of reinforcement learning (RL) in order to achieve a goal in an environment. The paper defines an RL problem by specifying the quartet (S, A, Pa,Ra) of states, actions, transition probabilities, and rewards. The RL agent learns an optimal policy function \u03c0\u2217 that maps states to actions in order to maximize the long-term cumulative reward. The reward function is defined ab initio for efficient goal accomplishment. The paper\u8ba8\u8bba\u4e86\u4f7f\u7528 Reinforcement Learning ( RL ) \u5728\u5b9e\u73b0\u76ee\u6807\u7684\u73af\u5883\u3002\u8be5 paper \u5b9a\u4e49\u4e86\u4e00\u4e2a RL \u95ee\u9898\uff0c\u7531\u56db\u4e2a\u90e8\u5206 ( S , A , Pa ,Ra) \u7ec4\u6210\u3002S \u662f\u4e00\u4e2a\u96c6\u5408\uff0cA \u662f\u4e00\u4e2a\u96c6\u5408\uff0cPa(s,s')\u662ftransition\u6982\u7387\uff0c\u5b83\u662f\u5728\u7ed9\u5b9as\u548ca\u7684\u60c5\u51b5\u4e0b\uff0c\u4eces\u5230s'\u7684\u901f\u7387\uff0cRa(s,s')\u662f\u7acb\u5373\u5956\u52b1\uff0c\u5b83\u63a5\u6536\u5728\u4eces\u5230s'\u7684\u60c5\u51b5\u4e0b\u91c7\u53d6\u884c\u52a8a\u65f6\u83b7\u5f97\u7684\u3002RL \u4ee3\u7406\u5b66\u4e60\u4ee5\u6700\u5927\u5316long-term\u7d2f\u79ef\u5956\u52b1\u4e3a\u76ee\u6807\uff0c\u6700\u5927\u5316\u77ed\u671f\u5956\u52b1\u3002\u5956\u52b1\u51fd\u6570Ra\u5b9a\u4e49\u4e86\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u76ee\u6807\u800c\u5fc5\u987b\u5b9e\u73b0\u7684\u4efb\u52a1\u3002\u8be5 paper \u4ecb\u7ecd\u4e86\u5982\u4f55\u8bc6\u522b\u6700\u4f18\u7b56\u7565\u51fd\u6570\u03c0\u2217\uff0c\u8be5\u51fd\u6570\u4ece\u6837\u672c(si,ai)\u4e2d\u5b66\u4e60\uff0c\u4ee5\u6700\u5927\u5316long-term\u7d2f\u79ef\u5956\u52b1\u3002\u7b56\u7565\u51fd\u6570\u03c0\u2217\u901a\u8fc7\u5b9a\u4e49\u5728s\u4e0a\u7684\u884c\u4e3aa\uff0c\u4ee5\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u5956\u52b1\u3002\u8be5 paper \u8fd8\u8ba8\u8bba\u4e86RL\u7b97\u6cd5\u7684\u5404\u79cd artifacts\uff0c\u5305\u62ec\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u51fa\u73b0\u7684\u504f\u5dee\u548c\u9519\u8bef\u3002\u6700\u540e\uff0c\u8be5 paper \u4ecb\u7ecd\u4e86\u5982\u4f55\u8bc6\u522b\u6700\u4f18\u7b56\u7565\u51fd\u6570\u03c0\u2217\uff0c\u4ee5\u4fbf\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u5956\u52b1\u3002\tThis paper focuses on Reinforcement Learning (RL). RL methods often entail a potentially large action space exploration to find a good policy. This work proposes a method to reduce such action space exploration. The method separates actions into two categories: dispensable (the action can be ignored) and indispensable (the action must be taken). Dispensable actions are also ranked according to their importance with respect to the final policy. The method is data driven and operates by looking at the global reward returns obtained when a certain action is removed from the action space. The method is evaluated on a case study simulating cloud infrastructure workload optimisation, i.e. the task of reducing high CPU utilisation by allocating more resources.""755\tThis paper discusses the use of reinforcement learning (RL) to solve a wide range of problems in artificial intelligence. The paper starts by defining the basic concepts of RL, including the four ingredients of an RL problem (state, action, transition probability, and reward). The paper then presents a simple example of how an RL agent can learn an optimal policy function to maximize its cumulative reward in an environment. The paper also discusses the various artifacts of an RL algorithm, such as the exploration-exploitation trade-off and the need for efficient strategies for managing state-space complexity. Finally, the paper concludes by discussing some potential applications of RL in real-world problems, such as machine learning and virtual reality.\tThis paper empirically considered the impact of training action space for reinforcement learning in a case study. Understanding the impact of training action space is a valid and important problem. An empirical study of this problem appears to be the main contribution of this paper.""756\tThis paper discusses how prediction sets can be used to quantify the uncertainty in machine learning models. The paper begins by introducing the key challenge of quantifying prediction uncertainty in machine learning, and discusses various approaches to do so. It then presents a number of papers that discuss the use of prediction sets in machine learning, and explains the methods used in these papers to provide probabilistic correctness guarantees. The paper also discusses the limitations of this approach, including the need for assumptions about the training and test distributions and the problem of covariate shift. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b future directions for studying prediction sets in machine learning.\tLet $X$ be an instance space, $Y$ a set of labels, $D$ some underlying (hidden) distribution over $X \\times Y$. This work studies a new method for converting the output of a probabilistic predictor (e.g. a deep net) to a good prediction set: that is a mapping from $C: X \\to 2^Y$, such that for most samples $(x,y) \\sim D$, $C(x)$ contains $y$. Formally, the authors study this problem in the PAC setting with covariate shift. The learner is given access to the score function, a labeled sample from the source distribution $P$ over $X \\times Y$, and unlabeled samples from a target distribution $Q$ over $X \\times Y$ whose marginal over $X$ may be shifted from the source. The goal is to output a prediction set which is as small as possible while still retaining PAC guarantees with respect to the shifted target distribution. This is a well-motivated model in practice. Probabilistic outputs of modern neural nets can be difficult to interpret, and small prediction sets may be useful in settings where one wishes to avoid or check more carefully a few marked outputs (e.g. a problematic medical diagnosis).  Assuming known, bounded importance weights, the authors provide an algorithm satisfying the PAC guarantee by maximizing a cutoff value for the score function that performs well over an empirical sample. They ensure their guarantee holds over the target rather than source distribution by rejection sampling to simulate the target distribution. The authors also extend this to settings where the importance weights are unknown but can be estimated from unlabeled samples. Finally, the authors provide experimental evidence over a couple common settings of covariate shift. They show that their algorithm outperforms baseline methods in the literature in the sense that it maintains PAC guarantees while outputting a smaller prediction set  in expectation. ""757\tThis paper discusses how prediction sets can be used to quantify prediction uncertainty in machine learning. The paper provides an overview of the challenges in quantify prediction uncertainty in machine learning and suggests several promising approaches for solving these challenges. The paper also discusses how prediction sets can be used to provide probabilistic correctness guarantees when the training and test distributions are equal, but this assumption often fails to hold in practice due to covariate shift. Finally, the paper provides examples of how prediction sets can be used in different applications of machine learning, including robot navigation, healthcare, and financial modeling.\tThis paper is concerned with learning prediction sets (in a PAC sense) under the covariate shift assumption, given a model $f(x,y)$. The form of the sets is restricted to $C_\\tau(x) := \\{y : f(x,y) \\ge \\tau\\},$ and the problem is set up as learning a $\\tau$, using a  labelled source dataset $S_m \\sim P^{\\otimes n}$ and an unlabelled target dataset $T_n \\sim Q^{\\otimes n}$ such that with high probability over $S_m, T_n,$ $Q(Y \\in C_\\tau(X)) \\ge 1-\\varepsilon,$ where $\\varepsilon$ is a given target coverage level. It is desired that $\\tau$ is as large as possible to minimise the size of the prediction sets learned.  The main scheme is presented modularly. The paper first describes how finding the maximum $\\tau$ whilst ensuring that the number of points in $S_m$ it captures is large enough (as specified by using a Binomial tail inverse) gives valid sets when $Q = P$. Next, it is argued that when $Q$ is absolutely continuous with respect to $P$, and the derivative $w(x) = \\frac{\\mathrm{d}Q}{\\mathrm{d}P}(x)$ is upper bounded and known, then one can importance sample the set $S_m$ to generate a sample from $Q$, which can then plug into the previous procedure. This step fundamentally uses the covariate shift assumption, and, to my understanding, is folklore. The following, then, constitute the main technical contributions.  Next, the assumption of exact knowledge of $w$ is relaxed, and it is argued that if instead for each $x_i \\in S_m,$ bounds $\\underline{w}_i \\le w(x_i) \\le \\overline{w}_i$ are available, then one can produce a worst-case estimate of the coverage of $C_\\tau$ for any $\\tau$ (by taking points that were missed to have high weights, and points covered to have low weights), and if this pessimistic coverage also satisfies the constraint. So, for each $\\tau$, the worst $w_\\tau$ can be produced, which can then feed into the importance sampling procedure above.  Then, it is pointed out that such a confidence bound on the $w_i$s can be learned using a probabilistic classifier $s(\\cdot|x)$ to separate data from $S_m$ and $T_n$, which leads to the main proposed algorithm. While it is roughly justified in the appendix that an accurate estimate can be obtained under smoothness assumptions with an appropriately fine gridding of the space (and a consequently huge sample complexity), the concrete proposal is to replace this step with a heuristic method for fitting bounds on the weights, and then plugging these into the above strategy. Note that this is not implemented as a direct optimisation over $\\tau$ - instead a set $\\mathcal{T}$ of possible values is pre-selected, and the procedure is executed for each $\\tau$ (the algorithm recommends an increasing order on the same).  Finally, the paper presents experiments in the DomainNet dataset, and in the ImageNet dataset, where the shift in the latter corresponds to adversarial perturbations. The principal baselines are the weighted split conformal inference (WSCI) method, which in my opinion is an appropriate choice, and the \"PS-C\" method, which simply uses a prediction set that has $1-\\varepsilon/b$ coverage on the source data, where $b$ is an estimated upper bound on $w$. The proposed method is ablatively presented, and it is seen that on just the DomainNet data, the method PS-R (which simply takes $(1-s)/s$ as an estimate of $w$) is both reliable and performs well, while the PS-M method, which further integrates samples in bins tends to be slightly optimistic on this dataset. Conversely, in the adversarially perturbed dataset, PS-R produces trivial prediction sets (since presumably this shift goes entirely outside the source domain's support), while PS-M performs well. The proposed method, PS-W, which further uses upper and lower bounds on $w$s after integrating them over bins, tends to be pessimistic (error-rates of $0.06-0.07$ are observed when only $0.1$ is demanded), but is not too much worse than either of these methods, and is effective in both types of shifts.""758\tThis paper discusses the use of prediction sets in machine learning, where the model predicts a set of labels instead of a single label. The paper presents several methods for providing probabilistic correctness guarantees when the training and test distributions are different, which is a common scenario in practice. The paper also discusses the challenges of covariate shift and proposes several solutions to overcome these challenges.\tThe paper presents algorithms for PAC prediction sets under the assumption of covariate shift. Using estimated/known importance weights that encode the shift, the method optimizes for the smallest subset of labels such that with high probability the error of this prediction set is low. A rejection sampling based strategy is then shown to satisfy the PAC constraints. The methodology is extended to the case where the importance weights are uncertain (due to estimation error). It is shown that the robust variant can be solved using the extreme case weights and the rejection sampling based algorithm. Simulations show the efficacy of the method.""759\tThis paper discusses the use of prediction sets in machine learning, specifically how they can provide probabilistic correctness guarantees while still allowing for covariate shift in the training data. It also discusses the limitations of using prediction sets, including the need to make assumptions about the data and the importance of analyzing both the training and test sets. The paper provides a overview of the different methods for using prediction sets, including exchangeable prediction sets and i.i.d. prediction sets, and discusses their advantages and limitations.\tThis paper proposes a new method to construct approximately correct (PAC) prediction sets for uncertainty quantification in the presence of covariate shift. It is a natural and interesting extension of the previous works [Park et al. 2020a, 2021] on PAC prediction sets. The building blocks this paper took from these previous works are the optimization problem in Equation (1) and the Clopper-Pearson confidence intervals for the Binomial distribution in Section 2.2. The extension is based on a rejection sampling Clopper-Pearson bound given in Section 3.2.  The authors propose an algorithm with and without access to the true important weights. The algorithm is evaluated in the settings of \u201crate shift\u201d and \u201csupport shift\u201d on the DomainNet and ImageNet datasets. The experiments show that the algorithm gives the smallest prediction sets among approaches that always satisfy the PAC constraint. ""760\tThis paper discusses the importance of using large amounts of unlabelled data in machine learning and the recent advances in semi-supervised learning (SSL) that make it possible to improve the performance of learning tasks with few labelled data examples. SSL makes use of the abundant unlabelled data to augment the performance of learning tasks, which has been shown to outperform supervised and unsupervised learning under certain conditions. The paper also discusses some of the commonly used SSL methods, including pseudo-labelling, and their empirical generalization performance. The paper concludes by highlighting the potential applications of SSL in real-life machine learning applications.\tThe paper considers a popular approach to semi-supervised learning based on iterative pseudo-labeling the unlabelled data and refining the model parameters thereafter. The paper is supported theoretically for the case of the Gaussian mixture model establishing a generalization error bound based on the KL divergence between the pseudo-labeled and true data distributions. The paper is also supported empirically for binary classification examples coming from CIFAR10 and MNIST datasets ""761\tThis paper discusses the importance of using large amounts of unlabelled data in machine learning, and the techniques for utilizing it in semi-supervised learning (SSL). SSL is a type of machine learning that uses abundant unlabelled data to augment the performance of learning tasks with few labelled data examples. The paper\u8ba8\u8bbas the advantages and limitations of SSL, including its ability to improve the generalization performance of classifiers and the ease and efficiency of its implementation. It also discusses several popular SSL methods, including pseudo-labelling and deep SSL. The paper\u6700\u540e\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5b9e\u9645\u5e94\u7528\u573a\u666f\u548c\u6848\u4f8b\uff0c\u4ee5\u4f9b\u53c2\u8003\u3002\tThis paper considers one common semi-supervised learning algorithm, pseudo labeling, and studies this problem from theoretical point of view. Specifically, it derives an information theoretic upper bound on generalization error in each iterative update of pseudo labeling. They separate the bound into two main parts: one depends on the mutual information between the data samples and model parameters, and the other depends on the KL distance between the underlying data distribution and pseudo labeled samples from previous iteration. Their main conclusion is that as the number of labeled and unlabeled samples grows, the first term vanishes, but the second term does not necessarily vanish.   In the rest of the paper, the authors rely on the simple example of binary Gaussian Mixture Model to give a more understandable and sensible calculation of the their upper bound. Namely, they calculate the KL distance and mutual information terms in the main theorem and study the behavior of generalization error for this model. The conclusion they made is that the iterative pseudo labeling can decrease the generalization error for only the first few iterations and after than has no effect on reducing the generalization error. ""762\tThis paper discusses the recent advances in semi-supervised learning (SSL) and their application in machine learning. SSL is a type of machine learning algorithm that uses large amounts of unlabelled data to augment the performance of learning tasks with few labelled data examples. It has been shown to outperform supervised and unsupervised learning under certain conditions, and various SSL methods have been proposed in the literature. One of the most popular SSL methods is pseudo-labelling, which involves an additional step of class assignment to the unlabelled data. This has been observed to improve the generalization performance of SSL algorithms. The paper also discusses the challenges and limitations of SSL, such as the need for computational resources and the issue of overfitting.\tThe paper considers the problem of semi-supervised learning where pseudo-labeling is used to iteratively assign labels for unlabelled data batches to enlarge the labelled dataset for subsequent re-training of the classification model. The paper first adapts the recent results of Bu et al (2020) and Wu et al (2020) to this set-up and provides a general information-theoretic upper bound for the generalization error of such a learning algorithm. The paper then specializes its set-up to a binary classification problem with Gaussian class conditionals and presents its corresponding generalization bound. Additional experiments are performed on the more practical datasets with deep neural network classifiers. ""763\tThis paper discusses the recent advances in semi-supervised learning (SSL) and its applications in machine learning. It explores the benefits of using large amounts of unlabelled data in SSL, as well as the various methods for making use of this data, such as pseudo-labelling and data parallelization. The paper also discusses the challenges and limitations of SSL, including the issue of overfitting, and the potential applications of SSL in real-world machine learning problems. The paper concludes by highlighting the future directions and potential applications of SSL in machine learning.\tThis paper provides a generalization error bound for iterative semi-supervised learning (SSL) algorithms using information-theoretic principles (see Theorem 1). To provide more intuitions, the authors first work with a simple model, i.e., the binary Gaussian mixture model (bGMM). It is shown in bGMM that when the class conditional variances are not too large, the upper bound on the generalization error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by experiments on MNIST and CIFAR datasets, where similar phenomena are observed, i.e., the generalization error improves after several pseudo-labelling iterations, but saturates afterwards.""764\tThis paper discusses the problem of sample efficiency in reinforcement learning (RL), which is a critical issue in recent years due to the large number of samples required for training. The paper proposes various methods to improve sample efficiency, including temporal consistency of exploration, which can help to increase the number of training samples and improve the performance of the RL algorithm. The paper also discusses the challenges and limitations of the proposed methods.\tThis paper proposes a method for exploration called Generative Planning method (GPM), which generates a multi-step action sequence such that the exploration is more temporally consistent and \"intentional\" compared to regular single-step action noise exploration. The multi-step action sequence is output by a generator with an RNN structure, and the generator is optimized by maximizing the plan value function. The authors show that their method GPM performs better than some other methods in several continuous control tasks and present some interesting qualitative results (e.g. state trajectory) showing that the exploration is more effective.""765\tThis paper discusses the issue of sample efficiency in reinforcement learning (RL) and proposes several solutions. RL has recently received great success, but the required number of samples for training is still large, making sample efficiency a critical problem. Standard model-free RL methods operate at the granularity of individual time steps, but they have some issues with efficient exploration, which makes the probability of consistent movement decay exponentially with the exploration steps. This paper proposes several approaches to improve the temporal consistency of exploration, including policy gradient methods, value-based RL, and path-based RL. The solutions are implemented in a specificRL framework and tested on a real-world task.\tThis paper presents a method called generative planning method (GPM) to improve exploration in RL. GPM performs exploration by learning a planner (a map from state to a sequence of action, aka a \u201cplan\u201d)) and performing MPC with a special rule for whether or not to use the latest plan or keep the old plan. The planner is an auto-regressive model (specifically, a stochastic RNN) and is trained to maximize an auto-regressive Q-function. Each time step, the plan from the previous time step is shifted forward by one time step and compared to the newly generated plan. The plans are compared using their predicted Q-values, and the policy switches plans with some probability that increases monotonically with Q(new plan) - Q(old plan). The exact likelihood is determined by a hyperparameter, l_commit_target, that, intuitively, sets a soft target for how long a plan is typically kept. The authors compare GPM to a variety of action-repeat-based exploration methods, from epsilon greedy policies to policies with learned action repeat counts (DAR & TAAC) and find that it is competitive with or outperforms these methods on various low-dimensional robot domains, as well as the image-based CARLA environment. The authors also visualize the trajectories and qualitatively show that GPM improves exploration.""766\tThis paper discusses the issue of sample efficiency in reinforcement learning, which is a critical problem in the field due to the large number of samples required for training. The paper presents an overview of the current state of the art in reinforcement learning and discusses the challenges associated with sample efficiency. The paper also discusses several approaches to improve sample efficiency, including model-free RL methods, exploration-based methods, and data-efficient techniques. Finally, the paper concludes by highlighting the future directions and challenges in this field.\tA generative planning method is proposed, which sits somewhere between model-free and model-based methods. No explicit model is learned. However, an action plan is generated using a recurrent action-plan generator, paired with a similarly recurrent critic.  At each environment step, a new action plan is generated, and  the current action plan can be abandoned in favor of the new action plan if the estimated benefit is large enough. The method builds on SAC.  Overall the benefits of the temporally extended action plans are: (a) temporally-coordinated exploration; (b) more effective than action-repeat; (c) some degree of interpretability given that an action-sequence plan represents then intent of a policy in a given state.""767\tThis paper discusses the problem of sample efficiency in reinforcement learning (RL) and proposes several solutions. RL methods operate at the granularity of individual time steps, but the number of samples required for training is still quite large. One issue is inefficient exploration, as consecutive one-step actions are not necessarily temporally consistent, which makes the probability of consistent movement decays exponentially with the exploration steps. This makes many of the exploration efforts wasted. The paper proposes several approaches to improve the temporal consistency of exploration, including using persistent exploration, using action replay, and using a combination of action replay and persistent exploration. The paper also discusses the limitations of these solutions and suggests further research in the area.\tThis paper proposes a method called Generative Planning (GPM), which aims to improve exploration for model-free RL. GPM learns a recurrent model to generate short term plans at each time step, and only decides to switch to the new plan if it is a lot better than the old plan. This encourages temporally extended explorations, and also does it in an adaptive manner. Experiments on a set of continuous control benchmarks show that GPM is able to converge faster than prior approaches, explore more effectively, and generate interpretable short-term plans. ""768\tt watch many different items . LRMC and LRTC can be used to recover the missing entries of a lowrank matrix or tensor, which can be used to improve the performance of collaborative filtering by providing more accurate recommendations. These methods have been widely studied and used in various fields, including science and engineering (e.g., data analysis, machine learning, and image processing), and have shown promising results.\tThis paper summarizes the theoretical guarantee for the existing LRMC and LRTC algorithms, and provides the theoretical analysis for a new proposed Multi-Mode Nonlinear deep tensor factorization. The analytical results show that when n2 is larger than n1, the nonlinear DMF provides a tighter generalization bound than MF. Similar analysis has been extended to two-mode matrix factorization and multi-mode  tensor factorization. Experimental results in synthetic data and real data show better results of the proposed algorithm as compared to other algorithms in completion tasks. ""769\tThis paper provides an overview of low-rank matrix completion (LRMC) and low-rank tensor completion (LRTC), which are recent machine learning algorithms for incomplete low-rank matrices and tensors. The paper discusses the background of low-rank matrices and tensors, the importance of LRMC and LRTC in science and engineering, and the applications of these algorithms in data preprocessing, image and video inpainting, and collaborative filtering. The paper also provides an analysis of the technical challenges and potential future directions of LRMC and LRTC. The main focus of the paper is to provide a comprehensive understanding of these algorithms and their applications, rather than providing a detailed technical analysis. However, the paper also includes technical details and practical examples of LRMC and LRTC in order to illustrate their effectiveness and applications.\tTo bridge the gap between deep learning and tensor decomposition, this paper presents two novel approaches named as two mode non-linear deep matrix factorization and multi-mode nonlinear deep tensor factorization (extension of two mode model to multi-mode scenario). The main contribution of the methods lie in full exploration of non-linearity of data in matrix and tensor factorization. To better motivate the proposed models, the authors provide theoretical analysis for why and when nonlinear deep matrix factorization outperforms linear deep matrix factorization in matrix completion. The experimental evaluation demonstrates that in some datasets, the proposed models outperform the existing models on matrix and tensor completion tasks.""770\tThe paper provides an overview of low-rank matrix completion (LRMC) and low-rank tensor completion (LRTC), which are two powerful methods for completing a low-rank matrix or tensor with missing entries. The paper starts by introducing LRMC and LRTC, which are widely used in various fields such as science and engineering. The methods are defined in terms of matrix or tensor completion, which means that they aim to complete a matrix or tensor with missing entries, such that the complete matrix or tensor can be used to solve a certain problem. The paper also provides a review of the previous literature on LRMC and LRTC, and the latest developments in these fields. The paper ends with a discussion of the potential applications of LRMC and LRTC, including data preprocessing, image and video inpainting, and collaborative filtering.\tThis paper studies the nonlinear low-rank completion of matrices and tensors. Specifically, it first presents the theoretical results showing why nonlinear deep matrix factorization is better than the ordinary matrix factorization model. Then, it proposes a model named two-mode nonlinear deep matrix factorization to make full use of the nonlinearity of the nearly square matrices. The authors also extend this method to tensor factorization by further factorizing the factor matrices in the Tucker decomposition using the deep factorization method. Impressive results are obtained using both synthetic and real-world datasets.""771\tt rate more than one item . In this paper , we propose a novel LRMC and LRTC algorithm to address this issue and provide a complete matrix or tensor in most cases . Our algorithm works by first reducing the size of the incomplete matrix or tensor to a complete one , and then completing the missing entries by using the existing knowledge in the field or by using new machine learning techniques . Our algorithm is efficient and has good performance on a wide range of problems , including data preprocessing , image and video inpainting , and collaborative filtering .\tThe paper provides a multi-mode framework for the deep learning based tensor decomposition, which could be useful for dealing with nonlinear high-dimensional data sets. In particular, it extends the deep matrix factorization (DMF) method and proposes a multi-mode deep matrix factorization method for matrix completion with convergence guarantee. Based on this, it also develops a multi-mode nonlinear deep tensor factorization method with convergence guarantee. The proposed models are solved by various optimization algorithms. Numerical experiments on synthetic and real data sets of the matrix/tensor form have shown that the proposed methods outperform other state of the arts. ""772\tThis paper proposes a novel interpretation technique for structured output models, which learn mappings between input vectors and a set of output variables simultaneously. The paper discusses the complex relationship between the computational paths of output variables in structured models and how features can affect the value of output through other ones. The goal is to train a function as an interpreter for the target output variable over the input space, using an energy-based training process. The interpreter function is trained using the structural information incorporated into the model.\tThe paper works on a novel problem of interpreting and explaining structured output models. The paper utilizes an energy based model to account for correlations between structured outputs and learns an interpretability block which given as input an image learns to mask it such that the energy based model would assign a similar score to the ground truth output and input as well as the ground truth output and perturbed input. In essence, the energy based model is a proxy for the actual deep neural network performing the structured prediction task. Results on a couple of datasets demonstrate that the work does better than baselines like LIME which do not utilize the correlations in the outputs modeled by the energy based model.  ""773\tThis paper proposes a new technique for interpretability of structured output models. The paper discusses the complex relationship between the computational paths of output variables and the ability of a structured output model to affect the value of an output variable through other ones. The paper focuses on finding the most important features utilized by the structured model to decide on the target output variable, using a technique called feature importance analysis. The paper also proposes an energy-based training process for the interpretation function, which considers the structural information incorporated into the model. The goal is to train a function that can interpret the target output variable over the input space.\tThe paper proposes a technique for identifying what input variables are most relevant for determining the value of a single, given output variable in structured-output (MAP) inference.  The idea is to learn an energy model that predicts which input variables are relevant to a particular structured prediction (x, y).  The authors propose to implement the energy model using a neural network followed by a Gumbel-softmax activation, and to train it by maximizing a structured hinge loss.  The proposed approach is evaluated on three datasets and compared to standard attribution techniques (LIME, SHAP, L2X).""774\tThis paper proposes a new technique for explaining the behavior of structured output models. The models learn mappings between input vectors and a set of output variables simultaneously. Because of the complex relationship between the computational paths of output variables, a feature can affect the value of output through other ones. The paper focuses on finding the most important features utilized by the structured model to decide on the target output variable at each locality of the input space. The goal is to train a function as an interpreter for the target output variable over the input space. The training process uses an energy-based approach and considers the structural information incorporated into the model. The paper argues that considering the correlation between output variables can improve the explanation performance.\tThe authors propose an energy-based training method for achieving model interpretability, which performs instance-wise feature selection. The proposed model adopts a similar approach of one of the pre-existing interpretable methods by calculating feature-level importance score with regard to each instance. The authors validate their method on synthetic and public datasets.""775\tThis paper proposes a novel interpretation technique for structured output models. The paper assumes that an arbitrary structured output model is available as a black box, and argues that considering the correlation between output variables can improve the explanation performance. The goal is to train a function as an interpreter for the target output variable over the input space using an energy-based training process. The technique involves finding the most important features utilized by the structured model to decide on the target in each locality of the input space. The paper proposes an energy-based objective function that is used to evaluate the performance of the interpreter function. The paper also discusses the potential limitations and challenges of the technique.\tThis paper propose a method for interpreting structured output model. The key idea is to find an \"interpretation\" which explains an \"target\" output random variable based on subset of rest of the output variables. The training objective is on finding a small subset which keeps the target output random variable invariant. The proposed methodology is applied to explain a synthetic energy function and structured prediction energy networks. ""776\tThis paper discusses the issue of how to approach distributional deep reinforcement learning (DRL) in a way that is similar to human decision-making. It proposes an approach where the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF), allowing for outcomes to be weigh based on relative quality. This technique is more direct than evaluating the projected distribution of returns for possible actions and does not require modification of the reward function to modulate agent behavior. The paper also discusses the potential benefits and limitations of this approach.\tThis paper considers a generalization of the policy gradient method to optimize for arbitrary utility functions with weightings that depend on the entire CDF (rather than the expected reward). This generalization has two aspects: (1) a utility function on top of the trajectory reward and (2) a weighting function for the CDF of the trajectory reward with respect to which the expectation is performed.  The paper derives an expression for the policy gradient and also generalizes the standard variance reduction baselines. Inspired by the PPO loss, the authors then propose a clipped version of the policy gradient calling it C3PO and evaluate this on some benchmarks from the OpenAI Safety Gym, where it is found that the conservative weightings can offer improvements over the standard formulation.""777\tThe paper discusses the issue of how to approach distributional deep reinforcement learning (DRL) in a way that is more similar to human decision-making. It proposes an approach where the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF), which allows for weigh outcomes based on relative quality. This technique can be used for both continuous and discrete action spaces and does not require modification of the reward function to modulate agent behavior. The paper also discusses the potential limitations of this approach and suggests other potential ways to approachDRL in a more human-like manner.\tRisk objectives have long been investigated in reinforcement learning (RL). Most of the focus has been on classic risk measures, like exponential utility, value-at-risk (VaR), conditional value-at-risk (CVaR), leaving out, however, the cumulative prospect theory (CPT) developed by Tversky and Nobel Prize Kahneman in 1992, which has not yet been considered.  The advantage of CPT is to better model human decision-making, still allowing a wide class of risk measures, based on the utility $u$ and weighting $\\omega$.  Hence, the authors consider a new risk-aware objective. Following some derivation, they compute a sample-based estimation of the gradient of this new objective w.r.t. the policy parameters.  The authors propose a PPO-like algorithm (called C3PO), which incorporates the new, risk-aware, gradient estimator.  They perform an empirical analysis on some tasks of \"Safety Gym\", showing that proper risk-awareness helps increase the performance of classic PPO.""778\tThis paper discusses the issue of how to approach distributional deep reinforcement learning (DRL) in a way that is more efficient and effective than traditional approaches. The paper proposes a technique that optimizes the distribution of full-episode outcomes based on a chosen function of its cumulative distribution function (CDF), allowing for weigh outcomes based on relative quality and avoiding the need for modifying the reward function to modulate behavior. The paper also discusses the potential benefits and limitations of this approach.\tThe paper presents an alternate approach for distributional DRL via proposing an objective inspired from Cumulative Prospect Theory (CPT, Tversky and Kahneman, 1992). They use this distributional objective in conjunction with policy gradient methodology to propose a distribution policy gradient method for risk-sensitive RL. Under their approach, the distribution of the returns is optimized to maximize some chosen function of its CDF. They experiment with different such possible distributional objectives (risk profiles) on the OpenAI Safety Gym environments and show that their approach performs better than PPO. ""779\tThis paper discusses the problem of distributional deep reinforcement learning (DRL), which aims to learn policies that are based on the distribution of full-episode outcomes rather than the expected reward. Human decision-making, on the other hand, emphasizes the importance of considering both gains and losses andoutlying outcomes. It also saves the agent the need to modify the reward function to modulate behavior based on distributional context. This paper proposes a more direct approach to distributionalDRL, where the distribution of full-episode outcomes is optimized to maximize a chosen function of its cumulative distribution function (CDF). This technique allows for outcomes to be weigh based on relative quality and may be used for both continuous and discrete action spaces. The paper also discusses potential limitations and future directions for this approach.\tThe article propose a policy gradient method for optimizing a CDF based criterion, inspired by CPT.  By varying the weighting function inside the objective, it is possible to change the risk-aversion of the agent. The authors derives the policy gradient for the aforementioned objective and propose an estimation technique for it. Then, they propose an algorithm which extends PPO for optimizing their objective. Empirical analysis is carried on to evaluate the approach on some modified Safety Gym environments, in which a fixed negative rewards corresponded to adverse events. The authors evaluate different objectives obtained by employing a Wang weighting function, with different values of the parameter $\\eta$. They show that optimizing a cautious (or risk-averse) objective allow to obtain better results in terms of average reward w.r.t. optimizing an aggressive (or risk-seeking) one. By further exploring the parameter space, the author demonstrate that some risk-averse values of the parameters allow to outperform also the risk-neutral version of PPO w.r.t. the average reward objective.""780\tThe paper discusses the importance of computational modeling in understanding and forecasting infectious disease dynamics, particularly during the COVID-19 pandemic. It explains that stochastic simulations play a crucial role in providing forecasts about complex interactions among people, environment, space, and time given a set of parameters. The paper also highlights the computational costs associated with simulating fine-grained spatial and temporal resolution, which make it difficult to perform these simulations on a large scale. The paper concludes by discussing the potential of using machine learning techniques to improve the accuracy of stochastic simulations.\tThis paper tackles learning a flexible distribution approximator to approximate the output of a high-dimensional and computationally intensive stochastic simulator.  The contributions of this paper then reduce into three main components:   1 - Definition of a spatiotemporal neural process that can succinctly model a more richly structured latent process.  2 - Definition of a new acquisition function and using a neural process in an active learning setting.  3 - Application to epidemiological simulators to \u201ccompile\u201d estimation.  The method appears to work, validated on a toy-ish SEIR model, and a more sophisticated pre-existing epidemiological model.   ""781\tIntroduction: computational modeling is more important than ever in infectious disease research due to the COVID-19 pandemic. Stochastic simulations play a crucial role in understanding and forecasting infectious disease dynamics, creating what-if scenarios, and informing public health policy making. stochastic simulation provides numerical tools to simulate probabilistic models and stochastic processes in finance, chemistry, and many scientific disciplines. Unfortunately, it is extremely computationally expensive to simulate fine-grained spatial and temporal resolution. \n\nConclusion: computational modeling is essential in understanding and forecasting infectious disease dynamics. Stochastic simulations provide numerical tools to simulate probabilistic models and stochastic processes, and they are essential in informing public health policy making and creating what-if scenarios. However, they are extremely computationally expensive, and therefore, more computational resources are needed to simulate complex models at high resolution.\tThis study proposes a new method for learning surrogate models of stochastic simulators. The new method, Interactive Neural Process (INP), builds on Neural processes and leverages the spatiotemporal structure of the problems at hand to reduce the complexity of the inference task. On a few problems in epidemiology -- a low dimensional SEIR problem (2 parameters, 100-dimensional output), and a complex spatiotemporal LEAM-US problem --, the approach is shown to appropriately learn the surrogate in few iterations.""782\tThis paper discusses the importance of computational modeling in understanding and forecasting infectious disease dynamics, created by stochastic simulations, which provide the numerical tools to simulate probabilistic models and stochastic processes in many scientific disciplines, including finance, chemistry, and many other scientific disciplines. The paper also discusses the computational cost of fine-grained spatial and temporal resolution stochastic simulations and how they can be used to inform public health policy making. Finally, the paper provides an overview of the applications of computational modeling in infectious disease research and how it can help to prevent and control infectious diseases.\tThe authors proposed a novel active learning framework integrated with the neural process. The neural process is used to mimic the simulator dynamics which is later used for Bayesian active learning. They also proposed a new acquisition function utilizing the latent from the neural process. The experimental results show that INP works better to accelerate stochastic simulation than GP, and the proposed LIG leads to faster convergence compared with alternatives.  ""783\tIntroduction: This paper discusses the role of computational modeling in infectious disease research, specifically the COVID-19 pandemic, and the importance of stochastic simulations in understanding and forecasting disease dynamics. It also discusses the computational complexity of simulations at fine-grained spatial and temporal resolution. The paper provides an overview of the key methods used in computational modeling and discusses the advantages and limitations of using these methods in infectious disease research. \n\nTitle: The Importance of Stochastic Simulations in infectious disease Research and the computational Complexity of them\n\nAbstract: This paper discusses the importance of stochastic simulations in infectious disease research and the computational complexity of them. The paper provides an overview of the key methods used in computational modeling and discusses the advantages and limitations of using these methods in infectious disease research. The paper also highlights the need for more efficient computational methods to support the development of accurate and reliable models for infectious diseases.\n\nIntroduction: The COVID-19 pandemic has brought infectious diseases research to a new level, with computational modeling being used more frequently to understand and\u9884\u6d4b disease dynamics. Stochastic simulations play a critical role in understanding and forecasting infectious disease dynamics, creating what-if scenarios, and informing public health policy making (Cramer et al., 2021). These simulations provide the numerical tools to simulate probabilistic models and stochastic processes in finance ( Lamberton & Lapeyre, 2007), chemistry ( Gillespie, 2007), and many scientific disciplines. Unfortunately, stochastic simulations at fine-grained spatial and temporal resolution are extremely computationally expensive. For example, epidemic models whose aim is to simulate realistic diffusion dynamics via in-silico experiments require the exabytes of data that are becoming increasingly available due to the COVID-19 pandemic.\n\nStochastic Simulations: Stochastic simulations are a type of computational modeling that produce forecasts about complex interactions among people, environment, space, and time given a set of parameters. They are used in many fields, including infectious diseases research, to simulateProbabilistic models and stochastic processes. These simulations provide a framework for understanding and\u9884\u6d4b complex systems, such as the spread of infectious diseases.\n\nAdvantages and limitations: The main advantages of stochastic simulations are that they provide a high-level understanding of complex systems, and that they can be used to simulate models that are beyond the scope of human understanding. They can also be used to make predictions that can be applied to real-world situations, such as the spread of infectious diseases.\n\nHowever, stochastic simulations have a number of limitations. Firstly, they are extremely computational expensive, and require vast amounts of data to be generated. Secondly, they can be difficult to analyze, especially when dealing with large datasets. Thirdly, they can be difficult to make predictions, especially when dealing with complex systems.\n\n computational models: computational models are a type of modeling approach that use mathematical models to simulate the behavior of complex systems. They are used in many fields, including infectious diseases research, to simulateProbabilistic models and stochastic processes. computational models are more efficient than stochastic simulations because they can be used to make predictions and because they can be analyzed more easily.\n\n methods used in computational modeling: There are a variety of methods used in computational modeling, including\u65b9\u7a0b\u6a21\u578b\uff0c\u7c92\u5b50\u6a21\u578b\uff0c\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7b49.\u65b9\u7a0b\u6a21\u578b\u548c\u7c92\u5b50\u6a21\u578b\u662f\u6700\u5e38\u7528\u7684\u6a21\u578b\uff0c\u65b9\u7a0b\u6a21\u578b\u7528\u4e8e\u6a21\u62df\u7269\u7406\u7cfb\u7edf\uff0c\u7c92\u5b50\u6a21\u578b\u7528\u4e8e\u6a21\u62df\u751f\u7269\u7cfb\u7edf\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u662f\u8fd1\u5e74\u6765\u53d1\u5c55\u6700\u5feb\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u6a21\u62df\u5404\u79cd\u590d\u6742\u7684\u7cfb\u7edf\u3002\n\nConclusion: Stochastic simulations play a critical role in understanding and forecasting infectious disease dynamics. They provide the numerical tools to simulate probabilistic models and stochastic processes in many fields, including infectious diseases research. However, they have a number of limitations and require vast amounts of data to be generated. computational models are more efficient than stochastic simulations because they can be used to make predictions and because they can be analyzed more easily. It is important to use a combination of models and methods to understand and\u9884\u6d4b the spread of infectious diseases.\tThe manuscript entitled, \"Accelerating Stochastic Simulation with Interactive Neural Processes\", presents a novel approach to the problem of statistical emulation for mechanistic models of epidemic disease transmission.  To this end, structured neural processes are developed to exploit and respect the temporal and spatio-temporal character of these models.  An active learning strategy is developed to train these neural processes to minimise the computational costs of generating training instances of disease simulator outputs.  The methodology developed is applied to two SEIR compartmental model examples: a minimal one with a single homogenous population and a maximal one with many age and space delimited cohorts.""784\tThis paper explores the issue of privacy attacks on machine learning systems trained on sensitive user data, particularly for recent applications involving large deep learning models of text. The paper presents an overview of the current state of the art in data privacy guarantee methods for discriminative and generative models of text, and discusses the challenges and limitations of applying Differential Privacy (DP) to large language models. The paper also highlights the need for further research in this area to develop more effective data privacy guarantee methods for these types of models.\tIn this paper, the authors propose a method that applies DP-SGD to NLP tasks. DP-SGD protects the privacy of the model training against that the individual information about the training samples is detected or inferred. The method is applied to the fine-tuning phase of the pre-trained language models (e.g. bert, gpt), thus it achieves good performances for many applications. To adapt DP-SGD to NLP models, this paper proposes ghost clipping that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model.""785\tThis paper discusses the privacy concerns associated with machine learning systems trained on sensitive user data, and the efforts being made to develop methods to provide data privacy guarantees for discriminative and generative models of text. The paper highlights the importance of differential privacy for protecting the privacy of sensitive data, and discusses the limitations of current methods for providing such a guarantee for large language models. It also discusses the potential applications of data privacy-based methods in the field of machine learning.\tThis paper adapts the widely used DP learning algorithm, DP-SGD, to language models. It achieves to fine-tune the dataset while protecting the private information in the dataset. In this paper, the authors conduct some empirical studies on language models and find some useful conclusions (e.g. fine-tuning on a part of parameters with DP is enough). The authors verify the model on sentence classification, table-to-text generation, and dialog generation tasks, using various pre-trained language models (e.g. GPT, Bert).""786\tThis paper discusses the issue of privacy attacks on machine learning systems trained on sensitive user data, particularly for recent applications of large deep learning models of text, such as those used in language translation. The paper\u8ba8\u8bba\u4e86 Differential Privacy (DP) \u6280\u672f\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5b9e\u73b0DP\u7684\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u751f\u6210\u5668\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u3002\u56e0\u6b64\uff0c\u8be5 paper \u63d0\u51fa\u4e86\u4e00\u4e9b\u65b0\u7684\u60f3\u6cd5\u548c\u6311\u6218\uff0c\u5305\u62ec\u6539\u8fdbDP\u7b97\u6cd5\u3001\u63a2\u7d22\u65b0\u7684\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u4ee5\u53ca\u4e0e\u9690\u79c1\u4fdd\u62a4\u76f8\u5173\u7684\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7814\u7a76\u3002\tThe paper propose a faster algorithm to learn approximate differentially private NLP models. Pretrained NLP models are often very large. Practical procedure involves fine-tuning NLP model on private data, which may leak private information. To avoid leakage, DP-SGD (and DP-AGAGRAD, DP-ADAM) uses norm clipping on each sample\u2019s gradient, and then add isotropic noise to aggregated gradients for samples in a batch. Then the normal update steps of SGD, ADAGRAD, or ADAM are performed. This will ensure privacy under differential privacy definition. However, this procedure requires computing per-sample gradient and keep them in the memory so that the norm of the gradient can be calculated, and the per-sample gradient clipping can be performed. This will introduce memory overhead proportional to the batch_size$\\times$#params, which is impossible for very large NLP models.   This paper propose GhostClipping method to save the memory, without the need of per-sample gradient instantiated. The idea is compute the partial sum of gradient element-wise square using two small matrices of the size of $T\\times T$, where $T$ is sequence length (<=1024 in practice), and aggregate them to obtain the per-sample norm (just a scale for each). And then it performs a second back-propagation to compute aggregation on the clipped gradient. It will uses almost the same memory as standard SGD (or ADAM), with one forward pass and two backward passes.   This paper also introduce two additional techniques to improve, one is choose a larger batch size, and the other is to introduce multi-task finetuning (fine-tuning includes masked prediction task on the target dataset). These two are important in boost the performance of the final models.   The paper evaluates on text classification, data-to-text generation, and dialog generation tasks. The results shows it gains prior DP methods.     ""787\tThis paper discusses the increasing concerns about privacy attacks on machine learning systems trained on sensitive user data, particularly for recent applications of large deep learning models of text, such as those in the field of natural language processing (NLP). The paper presents an overview of the existing methods for providing data privacy guarantees for discriminative and generative models of text, and\u8ba8\u8bbas the limitations of these methods when applied to large language models. The paper also proposes a new approach, Differential Privacy (DP), which can provide a meaningful privacy guarantee for large language models. Finally, the paper concludes by highlighting the need for further research in this area to develop more effective privacy\u4fdd\u62a4\u63aa\u65bd for machine learning systems.\tThis paper investigated the problem of privately fine-tuning large language models for downstream NLP tasks, including sentence classification and language generation. The authors showed that by appropriately selecting hyper-parameters (including batch size, learning rate, training epochs, and clipping norm) and making the fine-tuning task aligned with pretraining tasks, directly fine-tuning large language models with DP-SGD yields strong performance, and provided an empirical guideline for setting a good training configuration. The authors also proposed ghost clipping trick for further memory saving when fine-tuning large language models. Finally the authors showed through experiments that low dimensional updates do not necessarily lead to better performance.""788\tThis paper discusses the automatic and efficient design of robotic agents in simulation, specifically optimization of both the skeletal structure and joint attributes to maximize performance on a given task. The problem is challenging due to the vast and combinatorial design space, as well as the bi-level nature of the problem, where we need to search an immense design space and evaluate each candidate design for its optimal controller. Existing gradient-based continuous optimization methods are difficult to apply in this setting. The paper proposes a new approach to the problem, which uses a combination of machine learning and traditional optimization methods to find the optimal designs more efficiently. The approach is shown to be effective in solving a real-world robotic agent design problem.\tThis paper proposes a transform-and-control policy to optimize the robotic agents' designs. Contributions include: - A novel perspective on agent design: rather than formulating agent design as a bi-level optimization, this paper embeds both design generation and control into a single decision-making process such that both design and control are optimized by the same RL algorithm. - In this formulation, the training experience from different designs is shared to improve sample efficiency. - Joint-specialized MLP on top of the GNN policy that further finetunes the control of individual joints.""789\tThis paper discusses the challenges of automatically designing robotic agents in simulation, specifically optimizing both the skeletal structure and joint attributes to maximize their performance on a given task. The design space is vast and combinatorial, making gradient-based continuous optimization methods difficult, and the problem is inherently bi-level, requiring both search of an immense design space and evaluation of each candidate design for inner optimization. The paper presents a solution using a mixed-integer linear programming model that can solve the problem efficiently.\tThe paper poses the problem of morphology design for robots as RL training of one joint GNN policy. Their policy first generates the robot's morphology and then evaluates the design with a common behavior policy that conditions on it. The authors also introduce a technique called JSMPL to allow asymmetric morphologies. Experiments in the Mujoco simulator demonstrate a large improvement over evolutionary methods in terms of sample efficiency and final performance (although the latter is less clear, as neither method has clearly converged). Ablation studies show that the GNN architecture is essential, but are less clear about the impact of JSMPL. ""790\tThis paper presents an analysis of the challenges faced in automatic design of robotic agents in simulation. The paper focuses on the optimization of both the skeletal structure and joint attributes of an agent to maximize its performance on a given task. The design space is vast and combinatorial, making gradient-based continuous optimization methods difficult to apply. Additionally, the problem is inherently bi-level, requiring the search of an immensely large design space and the evaluation of each candidate design, which is computationally expensive. These challenges make the automatic design of robotic agents in simulation difficult and require significant research and development to overcome.\tThe paper proposes an algorithm for simultaneous agent design and policy optimization. The choice of the body structure is treated as another action available to the agent. Therefore, the policy is parameterized by graph neural networks (GNNs), and it outputs i) the skeleton structure, ii) node attributes such as bone length, size, motor strength, and iii) motor control commands. Thanks to the parameterization via GNNs, the policy can be trained with PPO. Experiments show that the proposed method outperforms prior approaches, which mainly employ evolutionary methods for optimization, whereas the proposed method leverages more sample efficient policy gradient algorithms.""791\tThis paper discusses the challenges of automatically designing robotic agents in simulation, specifically optimizing both the skeletal structure and joint attributes to maximize their performance on a given task. The problem is challenging for two main reasons. First, the design space is vast and combinatorial, making gradient-based continuous optimization methods difficult to apply. Second, the problem is inherently bi-level, requiring both searching an immensely large design space and evaluating each candidate design for its optimal controller.\tThe paper introduced a reinforcement learning algorithm that simultaneously optimizes the design as well as the controller of a simulated robot to perform locomotion tasks. The core idea is to train a conditioned policy that performs the task in three stages: 1) morphology design of the robot, 2) design parameter adjustment, and 3) controlling of the robot to perform the task. By integrating the design process into the policy learning framework, they are able to design novel and effective agents to complete a variety of tasks. To support the proposed algorithm, graph neural networks is heavily used to support different morphologies. They further propose a joint-specific architecture to improve flexibility of the network, which improves the performance of the algorithm.""792\tThis paper proposes a new architecture for implicit neural representations called CoordX, which can significantly accelerate the training and inference of MLPs for implicit neural representations. The architecturesplit the initial layers of the MLP to learn each dimension of the input coordinates separately, andthen fusion the intermediate features by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference. The proposed architecture has been evaluated on a set of test images and shown to be effective in accelerate the training and inference of implicit neural representations.\tThe paper proposes a novel coordinate-based network architecture which proposes to process each of the input coordinates independently in the first layer instead of together in a fully connected layer. This input style results in a speed-up in terms of evaluation of the network, and thus faster training and inference in tasks where coordinate-based MLPs are used, without incurring a significant degradation in terms of the quality of the signal fit. These benefits are demonstrated for the tasks of image representation, video representation, and 3D shape representation.""793\tThis paper proposes a new architecture for implicit neural representations, called CoordX, that can significantly accelerate the training and inference of MLPs for implicit neural representations. The architecturesplit the initial layers of the MLP to learn each dimension of the input coordinates separately,before fusion by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required, leading to large speedups in training and inference. The proposed architecture is evaluated on a set of tasks relevant to implicit neural representation, such as novel view synthesis and 3D object representation and rendering, and shows significant improvements over the existing methods.\tThis paper proposes a new architecture for implicit neural representations, called CoordX, which splits each dimension of the input signal into separate branches (e.g. the x and y coordinates of pixel locations in an image) and processes each of these separately before fusing them. The authors achieve this by projecting each of these branches into a hidden feature and then using shared fully connected layers to process these. Each branch is then fused by an outer product which then reconstructs the entire input grid (e.g. for an image of size H x W, 2 branches take in H locations and W locations respectively which are then combined into H x W features by the fusion operation). The fused layers are processed by a few more MLP layers to output the predicted features. In addition, the authors propose a method for effectively subsampling the grid during training as well as different splitting strategies for the branches.  The authors perform experiments on various data modalities, including images, videos, 3D shapes and NerF scenes.  The main contributions of the paper in my eyes are then: - Introducing a new architecture for implicit neural representations that in certain cases can improve training/inference speed without incurring a decrease in reconstruction - Experiments on various data modalities showing the strengths/weaknesses of the method ""794\tThis paper proposes a new architecture for implicit neural representations, called CoordX, which can significantly accelerate the training and inference of MLPs for implicit neural representations. The architecturesplit the initial layers of the MLP to learn each dimension of the input coordinates separately, which reduces the amount of computation required. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This leads to large speedups in training and inference. The proposed architecture has been evaluated on a set of real-world tasks, and itshows promising results.\tSummary: This paper proposes a modification to INR models on multidimensional coordinate grids where a subset of the earlier layers operate on the decomposed coordinate grid. In this setup, the grid (which is assumed to be regularly sampled to permit this decomposition) is broken into its constituent components, (e.g. x and y instead of (x,y)), passed through a single linear layer unique to that component, then through a stack of shared layers, followed by an outer product to return to the joint (x,y) space, and at least one layer that operates on the joint space. This approach lightly reduces parametric efficiency but strongly improves compute efficiency (both in terms of FLOPS, memory usage, and actual observed runtime) for common implicit neural representation tasks, including fitting images, videos, shapes, and volumetric rendering via radiance fields. ""795\tThis paper discusses how to accelerate the training and inference of implicit neural representations with multi-layer perceptrons (MLPs) by using a new split MLP architecture called CoordX. The architecture split the initial layers of the MLP to learn each dimension of the input coordinates separately, which reduces the amount of computation required. The intermediate features are then fusion by the last layers to generate the learned signal at the corresponding coordinate point, which leads to large speedups in training and inference. The paper also discusses potential applications of the CoordX architecture in various tasks such as novel view synthesis and 3D object representation and rendering.\tThe paper proposes an interesting tweak to the network architecture to accelerate CoortMLP. The idea is to split the input coordinates along the dimensions and then share weights before fusion.  The authors analyze the theoretical upper bound (as far as the MAC ops are concerned) and show about 2X speedup on actual machines.""796\tThis paper presents a review of recent advances in machine learning and visual reasoning for object recognition and 3D scene understanding in visual systems. The paper highlights the challenges and opportunities in this field, particularly in terms of how to address the limitations of traditional 2D approaches and how to integrate different sensory information to improve object recognition and 3D scene understanding. The review also discusses the applications of these technologies in various fields, such as object manipulation, navigation, and forecasting.\tThis paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. The benefits over existing models are the structured, 3D object representations which allows to manipulate objects in the scenes such as moving and replacing objects. This paper also shows that the inferred object representations can be used in a visual reasoning task.""797\tThis paper provides an overview of recent advances in machine learning for visual reasoning, specifically for inferring objects and their 3D geometry in a scene. It highlights the challenges that remain in achieving this goal and discusses the recent work in this area. The paper\u5148\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u57fa\u672c\u6982\u5ff5\uff0c\u5982 visual reasoning\u3001 object-centric representation\u3001NeRF\u7b49\uff0c\u7136\u540e\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u8fd1\u5e74\u6765\u5728\u89c6\u89c9\u63a8\u7406\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\u7684\u65b9\u6cd5\u548c\u6280\u672f\u3002\u6700\u540e\uff0c\u8be5 paper\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002\tThe paper aims to decompose a scene into objects and infer the representations of 3D occupancy, color, and pose for each object from a single image of the scene without supervision. To this end, the paper proposes an autoencoding solution by combining the Slot Attention encoder with the GIRAFFE decoder. Each object is represented as a Neural Radiance Field (NeRF) additionally parameterized by the latent variables inferred from the encoder. The decoder then compositionally renders the objects. The experiments show that the proposed model (1) achieves competitive 2D segmentation performance on CLEVR6, (2) supports object-wise scene manipulation, and (3) outperforms non-object-centric methods on CATER snitch localization when combined with a powerful transformer.""798\tThis paper discusses the recent advances in neural networks and their use in object recognition and 3D scene understanding. The paper also discusses the challenges that remain in achieving good performance on these tasks and the potential applications of these networks in various fields. The paper begins by introducing the basic concepts of neural networks and their use in computer vision. It then discusses the recent advances in object recognition and 3D scene understanding using neural networks, including the use of differentiable renderers inNeRFs. The paper also discusses the challenges that remain in these tasks and the potential applications of these networks in various fields. Finally, the paper ends by\u603b\u7ed3\u51faping the main contributions of the paper and highlighting the future directions for this field.\tThis paper proposes a model which is able to segment 3D scenes into objects by a combination of slot-attention (For inference) and a mixture of object NeRF functions which mix together (in 3D) to compose a scene. The method receives a single input image (with the camera coordinates though these are fixed) and extracts a set of slots - one slot for each object. These slots are decoded using a NeRF renderer: one part (the shape) generates the density, one (the appearance) generates the colours and one (the pose) transforms the points of the object to the appropriate pose in scene space. Results are demonstrated on CLEVR data as well as CATER (which is visually very similar) and some downstream tasks.""799\tThis paper discusses the recent advances in learning object-centric representations and their application in visual reasoning. The paper\u5148\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6700\u8fd1\u7684\u7814\u7a76\u6210\u679c\uff0c\u5305\u62ec\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5b66\u4e60\u4ece\u4f4e\u7ea7\u522b\u7684 Perceptual features \u4e2d\u63d0\u53d6\u7684 object-centric Representations\uff0c\u5e76\u5b66\u4f1a\u4e86\u4ece\u4e00\u4e2a\u5355\u4e00\u7684\u56fe\u50cf\u4e2d\u8bc6\u522b\u7269\u4f53\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u76d1\u7763\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u6570\u53ea\u8003\u8651\u4e86 2D \u56fe\u50cf\u7684\u7ed3\u6784\uff0c\u800c\u5ffd\u7565\u4e86 3D \u89c6\u89c9\u573a\u666f\u7684 3D \u51e0\u4f55\u3002\u53e6\u5916\uff0c\u8be5\u8bba\u6587\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Neural Radiance Fields (NeRFs) \u7684\u6280\u672f\uff0c\u5b83\u4f7f\u7528\u4e0d\u540c\u7684\u6e32\u67d3\u5668\u548c\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5\u6765\u5b66\u4e60\u7269\u4f53\u7684 3D \u51e0\u4f55\uff0c\u5e76\u53ef\u4ee5\u5e94\u7528\u4e8e\u7269\u4f53\u64cd\u7eb5\u3001\u5bfc\u822a\u548c\u9884\u6d4b\u7b49\u8bb8\u591a\u5e94\u7528\u4e2d\u3002\tThis paper proposes a model to infer structured 3D object representations from a 2D scene in an unsupervised fashion so as to represent the visual scene in an object-centric way. Specifically, the inference part adopts a similar mechanism as Slot Attention to derive object slot latent code, and then maps slot latent code to 3D object representations with MLP. The rendering part takes the idea from 3D neural rendering where a shared NeRF function is used to represent all objects excluding the background. Rendering is performed by querying NeRF with 3D location, 2D view direction as well as object latent to get object color value and occupancy value. Object rendering is composed into scene rendering according to location derived at the inference stage and weighted by occupancy value.  To sum up, the paper interprets a 2D visual scene with 3D object-centric representation. With the existing 2D object-centric scene segmentation method and 3D neural rendering approach, it achieves comparable segmentation performance and derives manipulable object representation.""800\tThis paper discusses the problem of evaluating the explainability of graph neural networks (GNNs), which is the ability to understand why a GNN made a specific prediction. The feature attribution framework is used to distribute the prediction of a GNN to its input features, identifying an influential subgraph as the explanation. However, the standard way of evaluating the explanation, based on the subgraph, is problematic due to a distribution shift between the full graph and the subgraph. The OOD effect, which refers to the problem of finding the source of a variable in a graph, is also considered a confounder. To overcome these issues, the paper proposes Deconfounded Subgraph Evaluation (DSE), which assesses the causal effect of an explanatory subgraph on the model prediction. DSE uses a different approach to evaluate the explanation, which is more reliable than using the subgraph alone.\tThe paper sheds an exciting light on the problem of producing a meaningful evaluation of GNN explanation methods (at least a subset of them). The idea is to introduce a deconfounder D to capture the effect of OOD explanations. The authors make an interesting example for a well-known synthetic dataset where the weight of the explanation in the ground truth is lower than a clear non-valid explanation when evaluated using the model to explain. The introduction of the deconfounder D creates a spurious path between the graph variable and the explainer variable. To mitigate this effect, then, they introduce a front-door adjustment to the causal graph. The front door adjustment requires a graph generator and authors use a novel Conditional-VGAE to generate graphs that will also cover the OOD case. The paper finally presents some experiments showing the evaluation method in action.""801\tThis paper discusses the importance of understanding graph neural networks (GNNs) and how to make them more interpretable. The paper proposes a feature attribution framework for identifying influential subgraphs in GNNs, which can be used to explain the model prediction and make it more interpretable. The standard way to evaluate the importance of an explanatory subgraph is to audit the model prediction based on the subgraph alone, but this can lead to distribution shift and out-of-distribution problems. The paper proposes Deconfounded Subgraph Evaluation (DSE), which assesses the causal effect of an explanatory subgraph on the model prediction. The DSE framework\u53ef\u4ee5\u907f\u514d\u7531\u4e8e distribution shift \u548c out-of-distribution problem \u5e26\u6765\u7684spurious associations\u3002\tIn this paper, the authors use a causal view to investigate the OOD effect on the explanation evaluation of GNNs. They find the confounder between the extracted subgraphs and the model prediction, which makes the evaluation less reliable. To solve this problem, the authors proposed a deconfounding evaluation method based on the front-door adjustment from causal discovery. To generate a reliable surrogate subgraph, they proposed a generative model, which contains three losses for training. The experimental results show the effectiveness of the proposed method (DSE).""802\tThis paper discusses the importance ofExplainability of graph neural networks (GNNs), which aims to answer the question \u201cWhy the GNN made a certain prediction?\u201d and help interpret the model prediction. The feature attribution framework distributes the prediction of a GNN to its input features, identifying an influential subgraph as the explanation. However, the standard way to evaluate the explanation, i.e., subgraph importance, is based on the subgraph alone, which can cause the out-of-distribution problem. To address this issue, the paper proposes Deconfounded Subgraph Evaluation (DSE), which assesses the causal effect of an explanatory subgraph on the model prediction. The problem with evaluating the explanation is that the distribution shift between the full graph and the subgraph can cause the out-of-distribution problem, and the OOD effect can act as a confounder, bringing spurious associations between subgraph importance and model prediction. To address this, the paper proposes a new method, DSE, which assesses the causal effect of an explanatory subgraph on the model prediction.\tThis paper has done an excellent work of finding the out-of-distribution between the subgraph and graph as the confounder. Further, this paper proposes a conditional variational graph auto-encoder in assessing the causal effects of subgraph on the prediction. They also introduce a surrogate variable to denote this out-of-distribution effect. Through adversarial training, the effects of the proposed model is correctly verified.  ""803\tThis paper discusses the issue of explainability of graph neural networks (GNNs), which aims to answer the question of \u201cWhy the GNN made a certain prediction?\u201d This is crucial to interpret the model prediction. The feature attribution framework distributes the prediction of a GNN over its input features, identifying an influential subgraph as the explanation. However, the standard way to evaluate the explanation, which is based on the subgraph, has problems, such as a distribution shift between the full graph and the subgraph, and the out-of-distribution problem. To address these issues, the paper proposes Deconetsug Evaluation (DSE), which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift and the out-of-distribution problem can make evaluation less reliable, DSE can still provide reliable information about the causal effect of the explanatory subgraph.\tThis paper presents a novel explainer-agnostic method to adjust the biases of feature importance scores of feature attribution for GNNs. The paper first describes the feature importance scores of the GNN feature attribution framework have biases due to the out-of-distribution (OOD) problem. The subgraph important scores are calculated by inputting a subgraph instead of data graphs, but subgraph patterns can fall into regions outside the distribution of training data graphs. To address this problem, the paper proposed a method to generate surrogate graphs within the data graph distribution by CVGAE to make a front-door adjustment for deconfounding these biases by distribution shift. Experiments using several state-of-the-art GNN explainers shows demonstrated the effectiveness of the proposed framework. ""804\tThis paper discusses a challenge in few-shot settings, where pretrained models are used to perform tasks with relatively little data. The challenge is task ambiguity, where the desired behavior is not uniquely specified by the provided examples, which can manifest in different ways. The paper\u63a2\u8ba8s how this problem can be addressed through changes to the training set or the model architecture.\tThis paper investigates if using large, pretrained models in an active learning setup helps achieve better performance with lesser data when compared to using randomly sampled data. In order to conduct this investigation the authors study the empirical performance of large pre-trained models on some image datasets and a text dataset. In both cases large pre-trained model is finetuned on a small amount of seed data and then an active learning procedure is used (in this paper the AL procedure is an uncertainty sampling procedure) to collect more data. The datasets are chosen to illustrate several conceptual issues (i) distinguishing causal from spurious correlations (ii) measuring robustness to distribution shifts (iii) role of data imbalance.   Experiments are performed to show that using an active learning procedure indeed helps improve performance using only a small amount of actively labeled training dataset.  The paper is well written and the results are convincing and insightful. ""805\tThis paper discusses the challenges that arise when using modern pretrained models in few-shot settings, specifically task ambiguity. Task ambiguity occurs when the desired behavior is not uniquely specified by the provided examples, which can make it difficult to train models in a few-shot setting. The paper\u63a2\u8ba8\u4e86\u51e0\u79cd\u89e3\u51b3task ambiguity\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u3001\u91c7\u7528\u66f4\u590d\u6742\u7684\u6a21\u578b\u7ed3\u6784\u548c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u7b49\u3002\u6700\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e9b\u5b9e\u8df5\u5efa\u8bae\uff0c\u4ee5\u5e2e\u52a9\u8bfb\u8005\u66f4\u597d\u5730\u5e94\u5bf9task ambiguity\u7684\u6311\u6218\u3002\tThe authors describe interesting empirical observations regarding using uncertainty sampling to select examples to fine-tune models that use pretrained embeddings and provide some hypotheses regarding the reasons for these performance improvements. Specifically, (1) from a methodological perspective, they propose using uncertainty sampling (i.e., least confident selection) to select examples for fine-tuning image/NLP pretrained models and (2) from an empirical perspective, they use Waterbirds/Treeperson/iWildCam2020-WILDS for image classification and Amazon-WILDS for review star prediction based on text and compare with random sampling \u2014 noting that these are settings where there is known covariate shift between train/test with semantic meaning to induce interpretable spurious associations (e.g., background in images). The proposed method works overall, especially on the image datasets, and they also dig into the types of examples selected \u2014 noting that they align with expected \u2018difficult\u2019 examples (depending on the setting).""806\tThis paper discusses a challenge in few-shot settings, where pretrained models are used to perform tasks with relatively little data. The challenge is task ambiguity, where the desired behavior is not uniquely specified by the provided examples, which can manifest in different ways. The paper\u63a2\u8ba8s how this problem can be addressed through improved training sets and models that are more sensitive to task ambiguity. It also discusses the underdiverseness of the training set and the potential effects of distribution shifts on model performance.\tThe authors set out to investigate if active learning is an emergent property of pre-training. That is if running active learning with pre-trained models gives better result than using the same models without pre-training. They run several experiments on different text and image datasets first showing that active learning performs better than random sampling on pre-trained models and secondly that pre-trained models perform better than un-pretrained ones for active learning.""807\tThe paper discusses the challenges faced by modern pretrained models when applied to few-shot settings, specifically task ambiguity. Task ambiguity can arise when the desired behavior is not uniquely specified by the provided examples, or when the training set is under diverse and causes models to be unsure of the desired behavior for minority groups or during distribution shifts. The paper\u63a2\u8ba8\u4e86\u51e0\u79cd\u89e3\u51b3task ambiguity\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u3001\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u4ee5\u53ca\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u6570\u636e\u589e\u5f3a. It also\u63d0\u51fa\u4e86\u4e00\u4e9b\u7814\u7a76\u95ee\u9898\uff0c\u5305\u62ec\u8fdb\u4e00\u6b65\u63a2\u7d22\u591a\u4efb\u52a1\u5b66\u4e60\u5728 few-shot \u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3001\u4ee5\u53ca\u63a2\u8ba8\u5982\u4f55\u5728\u6570\u636e\u589e\u5f3a\u65f6\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027.\tThis paper investigates the active learning performance of pre-trained models vs their non-pre-trained counterparts on both vision and NLP tasks. Specifically, the investigation focuses on datasets with spurious correlation, domain shift, and label imbalance. Empirical results generally show that the pre-trained models with the uncertainty acquisition function performs much better than the random baseline and their un-pre-trained counterparts. ""808\tThis paper discusses the issue of automated program repair, which is the problem of automatically detecting and fixing bugs in programs. It explains that the process of detecting and fixing bugs is often manual and labor-intensive, and that automated tools can help ease this burden. The paper explains the different types of program analysis techniques and the recent advances in deep learning that have led to the development of data-driven approaches to automated program repair. It also discusses the challenges and opportunities associated with this problem. The paper ends with a summary of the main findings and a call for further research in the field.\tAutomated program repair benefits from knowledge of its many properties, which includes its inherent (parse-)tree structure and graphical properties such as data-flow. This work proposes a graph-based encoder coupled with a tree-edit decoder, and optionally pretrained on a tree-based objective comparable to masked language modeling. The resulting model efficiently leverages relatively few parameters to achieve near-SOTA performance on a benchmark compiled from real-world bug fixes.""809\tThis paper discusses the issue of automated program repair, which is the process of detecting and fixing bugs in software programs without the need for manual intervention. It explains the various techniques that have been developed for this task, including hand-written rules, other program analysis techniques, and data-driven approaches using deep learning. The paper also highlights the increasing use of automated tools in software development, and the challenges that come with using these tools, such as the need for proper training and maintenance. Finally, the paper concludes by discussing the potential future of automated program repair and the need for further research in this area.\tThis paper presents a model over sequential structural tree edits, used for program repair based on ASTs. The model itself uses a graph encode and decoder to predict the sequential tree edits. The authors also introduce a method for pretraining the model on existing (non program repair) code data: they delete subtrees of arbitrary size from the code, and predict their reconstruction. The resulting model performs comparably to several other state of the art code repair models on the Patches in the Wild Java repair dataset, but with fewer parameters than several of the best-performing pre-trained models (CodeBERT, CodeT5). ""810\tThis paper discusses the issue of automated program repair, a task of automatically detecting and fixing bugs in programs. It explains the importance of this task in software development, the different approaches to this problem, and the recent advances in deep learning that have made it possible to use data-driven approaches for automated program repair. The paper also discusses the challenges and future directions of automated program repair.\tThe paper proposes a new approach to abstract syntax tree-based automatic program repair. The novel technique called deleted-subtree reconstruction is based on dropping parts of the syntax tree and training the model to grow them back. The method is evaluated against edit-based and sequence-based approaches, where it outperforms only the edit-based ones.""811\tThis paper discusses the issue of automated program repair, which is the problem of detecting and fixing bugs in software programs without the need for manual intervention. There are several different approaches to this problem, including hand-written rules, machine learning algorithms, and data-driven approaches. The paper also discusses the benefits and challenges of these approaches.\tThe paper presents GRAPHIX, a graph edit model for program repair. The work is directly related to Hoppity (Dinella et. al. 2020) which proposed using a sequence of graph edit for program repair. GRAPHIX employs multi-head graph encoder which improves upon Hoppity in terms of accuracy and complexity. Notably GRAPHIX is able to learn longer edit sequence and thus work on more program repair samples. The work has also proposed a pre-training task to improve model performance. Empirically the authors evaluated GRAPHIX on the *Patches in the Wild* Java bug-fix benchmark. It outperforms various baselines without pre-training. With pre-training, GRAPHIX-P stays roughly on par despite having much smaller model.""812\tThis paper discusses the importance of distributed training for handling data privacy and governance issues, as well as the recent advances in applyingdversarial training methods to Federated Learning. The paper also highlights the potential challenges that arise when combining Adversarial Training with Federated Learning, such as communication costs and hardware requirements. The paper provides an overview of the current state of the art in this field, and discusses future directions for further research.\tThis paper tackles an important problem in federated adversarial training: robustness accuracy significantly drops at the later stage of training. The authors first raise their assumption for the cause of this phenomenon: Adversarial training amplifies the heterogeneity of data distributions across different clients, and overfitted local robustness can not well generalized to other clients. Based on this assumption, the authors proposed \\alpha-weighted federated adversarial training, which essentially up-weights the model trained on benign distributions and down-weights those on harsh distributions when averaging them up at the cloud center. Results show the proposed method outperforms previous state-of-the-arts under different adversarial training and federated learning settings. ""813\tThis paper discusses the issue of adversarial attacks on distributed training in the context of Federated Learning. It notes that local training in Federated Learning is vulnerable to these attacks, and explores the use of Adversarial Training methods to enhance the model robustness. However, the combination of Adversarial Training and Federated Learning presents potential challenges due to communication cost and hardware requirements. This paper proposes a dynamic schedule to address these issues and achieve better performance.\tThis work studied the limitation of conventional Federated Adversarial Training approach, and proposed an \\alpha-weighted relaxation for Adversarial Training in the federated learning setting. Then it proposed a novel \\alpha-Weighted Federated Adversarial Training for minimizing a lower bound of the inner-maximization in Federated Adversarial Training. The performance of the proposed \\alpha-Weighted Federated Adversarial Training were validated for both IID and Non-IID federated learning settings.""814\tThis paper presents the recent advances in the field of distributed training, specificallyFederated Learning, towards handling data privacy and governance issues. It explores the vulnerability of local training in Federated Learning due to adversarial attacks, and proposes the use of Adversarial Training methods to enhance the model robustness. The combination of Adversarial Training and Federated Learning, as well as the challenges associated with it, are also discussed. The paper\u6700\u540e presents a possible solution to these challenges, which includes dynamic communication schedule and hardware requirements optimization.\tThis paper introduces the alpha Weighted Federated Adversarial Training algorithm. The key of the idea is that in the aggregate step, the center prefers the local machine that yields smaller lost. Some theoretical results are delivered with numerical experiments. The paper claims that the alpha-weighted mechanism is tailored for the inner-maximization of Federated Adversarial Training, which is the rationale of the whole work.""815\tThis paper presents an overview of the state-of-the-art in the field of distributed training, specifically Federated Learning, and its recent applications in the face of adversarial attacks. The paper discusses the challenges of combining Adversarial Training with Federated Learning, and proposes a solution based on communication constraints and dynamic scheduling. The paper also highlights the importance of robust models in the face of these attacks and the need for further research in this area.\tThe authors explore the adversarial robustness of federated learning. They claim that the inner-maximization optimization of AT can exacerbate the data heterogeneity among local clients. They propose an algorithm, $\\alpha$--WFAT, which relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning.. The authors also experimentally establish that federated learning models are most susceptible to attacks when clients are using non-IID training sets. The experiments are performed over the CIFAR-10 , SVHN and CIFAR-100datasets.""816\tThe paper discusses the history of machine learning and the efforts made to create learning frameworks that better mimic the human learning process. It also discusses the concept of lifelong machine learning, which is defined as training machine learning models for continual learning over task sequences with a knowledge base to store information that could be helpful in the future. The paper\u6307\u51fa\uff0c while individual tasks in LML can still be supervised, the state-of-the-art in this field is still limited because labels can be expensive and rare for each task. Additionally, the paper notes that supervised training can rarely provide acceptable performance. The paper also discusses the challenges of training LML models and the potential applications of this technology.\tIn contrast to previous works on lifelong machine learning (LML) that put their focus on the supervised learning settings, this paper concentrates on the scenario that only a limited amount of data is available. The proposed method MAKO is mounted on the top of supervised LML model, without introducing additional knowledge based overhead, for better leveraging the unlabeled data. Labeling new data can be realized by using the data programming method which is supervised by the labeled data. The target of this paper is to design a SSL LML framework that minimizes the performance between using partially labeled data, and the upper-bound performance using fully labeled data. Several experiments on standard image classification data sets including MNIST, CIFAR-10 and CIFAR-100 are used to evaluate the the effectiveness of MAKO. ""817\tThe paper discusses the history of machine learning and the recent efforts to achieve human-like learning through memories from past experiences. It explains the concept of lifelong machine learning, which is defined as training ML models for continual learning over task sequences, with a knowledge base to store information that could be helpful in the future. The paper also highlights the current state-of-the-art in LML, which still remains largely supervised, and points out a challenge of providing labels for each task, which makes supervised training difficult to achieve acceptable performance.\tThis paper propose a wrapper tool that mounts on top of supervised Lifelong Machine Learning (LML) frameworks, leveraging a well-known method data programming. The contributions of this paper can be summarized in three aspects. 1)  Adapting automatic label generation by semi-supervised learning/data programming to LML in some special scenario. 2) Implementing a LML wrapper that can accomplish some tasks under some restrictions. 3) Through detailed experimental results prove the superiority of its method.""818\tThe paper discusses the concept of lifelong machine learning (LML), which is a type of machine learning (ML) that involves training models over task sequences and using a knowledge base to store information that could be helpful in the future. It is\u4e0d\u540c\u4e8e traditional isolated ML, where knowledge is never accumulated, and defines LML as continual transfer learning with memory orSequential multi-task learning. The paper discusses the current state of LML at the individual task level, and\u6307\u51fa\u5728\u8bad\u7ec3\u65f6\u4ecd\u9700\u8981\u6807\u6ce8 labels \uff0c\u4e14\u6bcf\u4e2a\u4efb\u52a1\u7684\u6807\u7b7e\u4ecd\u7136\u5f88\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64 supervised training \u96be\u4ee5\u63d0\u4f9b acceptable performace\u3002\tThis paper proposes  data programming method, named Mako,  for semi-supervised continual learning. Mako automatically generates labels for unlabeled data with a set of weak labeling functions, each of the functions is trained on subset of training set with bootstrapping. Experimental on several datasets demonstrate the effectiveness of the proposed methods. ""819\tThis paper discusses the concept of lifelong machine learning (LML), which is a type of machine learning that involves training models to continually learn over task sequences and using a knowledge base to store information that could be helpful in the future. It is\u4e0d\u540c\u4e8e traditional isolated machine learning, where knowledge is never accumulated, and is defined as continual transfer learning with memory orSequential multi-task learning. The paper highlights the challenges of training models in individual tasks in LML, including the issue of labels being expensive and rare, and the difficulty of providing acceptable performance using supervised training. It also discusses the potential benefits of LML, including improved performance and reduced computational complexity.\tThis paper presents an interesting idea of using data programming techniques to enable continual semi-supervised learning with limited labeled data. It proposes a stage-wise pipeline where probabilistic pseudo labels are first produced by a Snuba based Data Programming framework, then calibrates them by the temperature scaling, and finally inputs into the mounted Lifelong Machine Learning (LML) tools. Experiments show that the proposed framework achieves similar performance to fully supervised methods.""820\tThe paper discusses the problem of avoiding adversarial example detection defense mechanisms by finding adversarial examples that must simultaneously (a) be mis classified by the model and (b) be detected as non-adversarial. The paper finds that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. It introduces two improved attack techniques: Selective Projected Gradient Descent and Orthogonal Projected Gradient Descent, which avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. The paper uses these techniques to evade four state-of-the-art detection mechanisms, reducing their accuracy to 0% while maintaining a 0% detection rate.\tThe authors propose an attack that could break 4 adversarial detection methods published recently. Traditionally, attacks against detection methods have attempted to maximize the loss for both classification and detection simultaneously. However, using a toy example the authors show that this is suboptimal, as it may not find the worst-case adversaries. The authors propose to minimize the loss (targeted attack setting) iteratively by optimizing either only for the classification pipeline or the detection pipeline at a time.  The attack first considers the classification loss and further tries to fool the detection pipeline until the classification prediction remains incorrect. The authors also propose a variant of the attack by considering gradient steps for the classification pipeline to be orthogonal to the gradients of the detection pipeline and vice versa. Finally, the paper shows that these two proposed attacks completely circumvent four recent adversarial detection methods.""821\tThe paper discusses the need to generate adversarial examples that can be both mis classified and detected as non-adversarial by a model, as well as existing attacks that attempt to meet this challenge. The authors propose two improved attack techniques, selective Projected Gradient Descent and Orthogonal Projected Gradient Descent, that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. The paper shows that these techniques can be used to evade four state-of-the-art detection defenses, reducing their accuracy while maintaining a 0% detection rate.\tThis paper considers the problem of finding adversarial examples that simultaneously defeat a detector of adversarial examples. An argument is made that existing attacks often achieve one goal at the expense of another. This argument motivates the proposal of two attack techniques. These are evaluated against four existing detection-based defence methods, with successful results.""822\tThe paper discusses how to generate adversarial examples that are both mis classified by the model and detection as non-adversarial. The paper finds that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. To avoid this problem, the paper introduces selective projected gradient descent andOrthogonal Projected Gradient Descent, two improved attack techniques that generate adversarial examples that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. The paper uses these techniques to achieve state-of-the-art results inevading four state-of-the-art adversarial example detection defense methods, reducing their accuracy to 0% while maintaining a 0% detection rate.\tThis paper targets on attacking the defensive mechanism of adversarial examples detection. It proposes a new optimization algorithm to simultaneously meet two different requirements. It verifies its effectiveness on several state-of-the-art adversarial example detection methods.""823\tThe paper discusses how to generate adversarial examples that can simultaneously be mis classified by a model and detected as non-adversarial by a defense mechanism. The paper finds that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. To avoid this problem, the paper introduces selective projected gradient descent andOrthogonal Projected Gradient descent attack techniques, which generate adversarial examples that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. The paper uses these techniques to evade four state-of-the-art adversarial example detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate.\tThis paper proposes two techniques for generating adversarial examples: Selective Projected Gradient Descent (SPGD) and Orthogonal Projected Gradient Descent (OPGD). In order to fool both the victim model $f$ and a detector $g$, SPGD selectively optimise either $f$ or $g$ depending on whether the modified input is misclassified as the target class, while OPGD further orthogonalizes the gradients. Evaluation on four previously unbroken, state-of-the-art defence methods demonstrate the effectiveness of the proposed attacks.""824\tThis paper explores a probabilistic treatment of cooperative games, also known as cooperative game theory, which is a fundamental problem in mathematical game theory. The paper proposes a new approach to solving this problem, which is to assign importance vectors to players based on their collective behavior. The importance vectors provide a way to weight the contributions of each player in a way that makes sense for the overall game. The paper also explores the connections between this approach and classical game theory, and the potential applications of this technology in various machine learning applications.\tThis paper studies valuation problems from cooperative game theory. There are $n$ agents and a valuation function $F: [n] \\to R$ where $F(S)$ is the collective payoff of the coalition $S \\subseteq [n]$. The goal is to use this function $F$ to define an importance vector $\\phi(F) \\in R^n$. Examples include the Shapley value and Banzhaf index.  The authors introduce a probabilistic treatment of this problem, where they use $F$ to define a probability distribution $p$ where $p(S)$ is the probability that coalition $S$ forms. They then phrase the problem of defining an importance vector $\\phi(F)$ as a decoupling problem. Under $p$, the $n$ agents may be correlated in a complicated way, but to assign each of them an individual importance value, one must decouple their interactions, or simplify their correlations. The goal is then to find a product distribution $q$ that is as close to $p$ as possible under the KL divergence. Specifically, the authors define $q$ to be an $n$ independent Bernoulli distribution, where the probability that agent $i$ participates in the coalition is denoted $x_i$. The authors show how to optimize the probabilities $x_1, \\dots, x_n$ using coordinate ascent. Finally, they define the importance score of player $i$ as $\\log(x_i/(1-x_i))$ (ignoring a temperature $T$ term for simplicity). The authors show that the resulting importance vector satisfies many of the game-theoretic axioms that the Shapley value and Banzhaf index satisfy, like the null player, marginalism, and symmetric axioms.  In the experiments, the authors look at small instances with $n = 25$ where it is actually possible to compute the gradients exactly (as opposed to an approximate sampling method). The applications they look at are for data valuation and feature attribution in the context of machine learning. For these tasks, they show that their proposed approach performs about the same as the Shapley value and Banzhaf index, and sometimes a bit better.""825\tThis paper discusses aProbabilistic treatment of cooperative games, which is a fundamental problem in cooperative game theory. The paper explores how to assign importance vectors to n players in a way that allows for learning and inference to be conducted in a unified manner. The paper also discusses connections with classical game theory and the literature on machine learning.\tValuation criteria based on game-theory (e.g. Shapely value) have been used in the ML literature for analyzing feature importance and for data subset selection. These criteria serve as solution concepts for cooperative games and have been adapted by some works in ML for subset valuation problems.    The present paper presents a probabilistic treatment of cooperative games, and shows that two classical valuation criteria can be seen as a one-step factored approximation to maximum entropy solution to the game. They then propose a new valuation criterion \"Variational Index\" that uses a multi-step factored approximation and show it satisfies some common axioms for cooperative games. The paper also has experimental results on the proposed criterion.  ""826\tThe paper discusses the importance assignment problem in cooperative games, which is often posed as a problem in cooperative game theory. It proposes a probabilistic treatment of cooperative games, which allows for a unified learning and inference process. The paper also explores connections with classical game theory and presents some existing literature on the topic. The paper concludes by providing future directions and perspectives on the problem.\tThe paper studies valuation problems for cooperative games. It proposes a new valuation measure called Variational Index. The idea is to create a coalition probability distribution based on a maximum entropy criterion. Player valuations are then derived by creating decoupled surrogates of this distribution. The authors then present a gradient ascent algorithm to compute this decoupling. Classical valuation criteria like the Shapley value and the Banzhaf index can be recovered as special cases or modifications of the algorithms iterates.""827\tThis paper explores a probabilistic treatment of cooperative games, which are often formulated as importance assignment problems in cooperative games. The paper\u8ba8\u8bba\u4e86\u5982\u4f55\u4ee5\u4e00\u79cd\u6982\u7387\u7684\u65b9\u5f0f\u6765\u89e3\u51b3 cooperative games \uff0c\u8fd9\u53ef\u4ee5\u4f7f\u673a\u5668\u5b66\u4e60\u548c\u63a8\u7406\u53d8\u5f97\u7edf\u4e00\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e\u7ecf\u5178\u535a\u5f08\u8bba\u7684\u8054\u7cfb\u3002\u5728 cooperative games \u4e2d\uff0c\u4e00\u4e2a\u8054\u76df N = { 1 , ... , n } \u548c\u4e00\u4e2a\u503c\u51fd\u6570 F ( S ) : 2N \u2192 R \u63cf\u8ff0\u4e86\u4e00\u4e2a\u96c6\u4f53\u6536\u76ca\u5bf9\u4e8e\u6bcf\u4e2a\u73a9\u5bb6 S \u7684\u503c\u51fd\u6570\u3002\u4e00\u4e2a\u57fa\u672c\u7684\u95ee\u9898\u5728 cooperative game \u7406\u8bba\u4e2d\u662f\u5206\u914d\u4e00\u4e2a\u91cd\u8981\u6027\u5411\u91cf \u03c6 ( F ) \u2208 Rn \u5bf9\u4e8e n \u4e2a\u73a9\u5bb6\u3002\u672c paper \u8ba8\u8bba\u4e86\u5982\u4f55\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3 cooperative games\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e\u7ecf\u5178\u535a\u5f08\u8bba\u7684\u8054\u7cfb\u3002\tThis paper proposes an energy-based perspective on cooperative games that permits a gradient-based calculation of Shapley/Banzhaf values, as well as the definition of a new alternative value - the variational index. A quick summary of the paper's key ideas is:  - For a given cooperative game $F$, we can seek an entropy maximizing distribution over coalitions $p(S)$ that satisfies a constraint on the mean coalition value $\\mu$ - Solving the entropy maximization problem via its Lagrangian yields the Boltzmann distribution $p(S) \\propto \\exp(F(S)/T)$, where the temperature $T$ has a one-to-one correspondence with the mean coalition value $\\mu$ (this result is in the appendix). This distribution gives more probability mass to coalitions that achieve higher values - We can seek a simpler alternative to $p(S)$ by doing mean-field variational inference, i.e., finding a factorized surrogate $q(S)$ where each player's participation is determined by independent Bernoulli RVs. The result will intuitively assign higher probabilities to players that belong to high-value coalitions, so these probabilities can serve a function similar to Shapley/Banzhaf values - The VI approach suggests a KL divergence minimization (or ELBO maximization) objective for learning $q(S)$, which is parameterized by $x \\in [0, 1]^n$. Doing gradient descent on this objective yields a relatively simple update rule, where we repeatedly set $x_i^+ = \\sigma(\\nabla_i f_{mt}(x) / T)$ for $i = 1, \\ldots, n$ - The authors define the \"variational index\" as a function of the solution to the KL divergence minimization problem: $s^* = T\\sigma^{-1}(x^*)$ - The authors find that the Banzhaf value can be found using a single-step update to a particular initialization of the KL divergence minimization problem (luckily the temperature $T$ is not important for single-step updates). Similarly, they find that the Shapley value is the average of the single-step update applied to different initializations (again, the temperature doesn't matter). Finally, the authors point out that any single-step update applied to a symmetric initialization will be a probabilistic value (a class of solution concepts in cooperative game theory, of which Shapley/Banzhaf values are special cases) - Lastly, the authors suggest a practical sampling-based approach to calculating the necessary gradients, which are just as difficult to calculate as the Shapley/Banzhaf values because they require calculating the value for every coalition $S \\subseteq N$  The experiments compare the variational index to Shapley and Banzhaf values in data and feature removal tasks, finding that it performs quite favorably in the settings examined.""828\tThis paper discusses the issue of estimation of epistemic uncertainty in machine learning, which can be a key factor in purposeful knowledge-seeking by learning agents. The paper proposes a new proxy for this measure, called model variance, as a way to quantify the level of uncertainty in a machine learning model. The paper also discusses the existing literature on this topic and the challenges and opportunities that arise when working with epistemic uncertainty.\tThis paper uses out-of-sample prediction error as a measurement of epistemic uncertainty. Using this definition, it develops an estimator: direct epistemic uncertainty prediction.  The idea is to have a main predictor to learn the task, and an error predictor to predict the generalization error. Empirical studies show that their proposed estimator produces better estimation on downstream tasks such as sequential model optimization and reinforcement learning. ""829\tThis paper explores the topic of epistemic uncertainty in machine learning and proposes a way to estimate it using model variance as a proxy. The paper provides an overview of the current state of the art in this area and discusses how model variance can be used to inform decision-making in machine learning. It also highlights the potential applications of epistemic uncertainty estimation in machine learning, such as in purposeful knowledge-seeking and in reinforcement learning.\tGiven some supervised task, this paper redefines the uncertainty of a solution as its generalisation risk. The authors then propose to learn a secondary function to estimate the generalisation risk of the first. This is done by using held-out points as training data for the secondary model. In turn, this held out used to estimate the primary model error comes from a k-fold split.  The inputs to the secondary model are the data points being evaluated, estimates of the predictive variance of the primary model, density estimates from a generative model and whether the point has been observed by the primary model.  The authors posit that the advantage of their method is that it can capture uncertainty due to model selection bias, something omitted by existing work, which focuses on the variance of the learnt estimator. The authors provide a diverse range of experiments: OOD rejection in image classification, active learning for drug discovery, function optimisation and exploration in reinforcement learning. ""830\tThis paper discusses the importance of estimation of epistemic uncertainty in machine learning for purposeful knowledge-seeking by learning agents. It explains that epistemic uncertainty can be used to inform which areas of state-space or input data space could potentially be gained from learning, and how much data is needed to eliminate lack of knowledge. The paper also explores how previous work has used model variance as a proxy for epistemic uncertainty. The paper\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u91cf\u5316 epistemic uncertainty, including using measures of information gain and information loss. The paper\u6700\u540e\u547c\u5401 researchers to continue exploring ways to estimate and use epistemic uncertainty in machine learning, to further improve the quality of learning and the ability of machines to learn from data.\tThis paper proposes a new approach for computing epistemic uncertainty. The proposed approach, DEUP, builds a new model (in addition to the original model) which predicts epistemic uncertainty, defined as generalization error minus aleatory uncertainty. I highlight some features of DEUP below: - DEUP works with a hold-out dataset that is used for training the error predictor.  - In case there does not exist a hold-out set or in interactive settings (like RL or active learning), DEUP is extended to be used in a cross-validation setting and the features used to fit the error predictor is extended to include data density estimates and model variance. DEUP is evaluated on different settings including OOD data, sequential model optimization and RL.""831\tThis paper discusses the importance of estimation of epistemic uncertainty for machine learning and the challenges involved in achieving this goal. It argues that epistemic uncertainty can be used to inform the choice of state-space or input data space for learning, as well as the choice of learning algorithm. The paper provides a discussion of previous work on estimation of epistemic uncertainty and suggests that a better proxy for this measure should be model variance. The paper also presents a new approach to estimation of epistemic uncertainty that takes into account the complexity of the model and the data.\tThis paper proposes a method to estimate the epistemic uncertainty (uncertainty due to lack of knowledge/data) at a new model input. The paper takes an indirect approach towards this goal by a) first estimate the generalization error at the new input, b) next estimate the aleatoric error (inherent uncertainty in the data distribution / irreducible error), and c) subtract aleatoric error estimate from the total generalization error estimate to obtain the epistemic uncertainty estimate.  The authors claim that this method captures both the uncertainty due to lack of data, as well as model misspecification in the process - while other/existing methods focus mostly on the variance of the posterior distribution (or its approximation) as a measure of epistemic uncertainty (and thereby implicitly assume the model is well specified.)  Estimating the total generalization error itself is performed with a second neural network model, which uses residuals obtained from the primary model as its labels. Estimating aleatoric error either assumes the presence of an Oracle or   The authors apply their technique on both static (fixed data set) as well as interactive (active learning) settings, though mostly focused on the mean squared error loss function.""832\tThis paper proposes a family of block Givens coordinate descent algorithms to learn rotation matrices that are provably convergent on any convex objectives, while using product quantization coupled with space rotation as a compression method for embeddings in modern approximate nearest neighbor search systems. The givens algorithms are much more parallelizable and reduce runtime by orders of magnitude on modern GPUs, and converge more stably compared to the state-of-the-art SVD method. The paper also presents experimental studies that demonstrate the effectiveness of the proposed algorithms in practice.\tThis paper proposes a block coordinate descent algorithm for rotation learning. The algorithm is based on Lemma 1 and Theorem 1. The rotation matrix on SO(n) is decomposed into diverse simple Givens rotation matrices. Then the optimized variable is converted into these Givens rotation matrices so that the rotation matrix is always on SO(3) and the projection is not required anymore. The authors also discuss how to select the coordinate, including random strategy, greedy strategy, and steepest strategy. Different from the existing work, it considers multiple Given rotations matrices in one step. ""833\tThis paper discusses the use of product quantization (PQ) coupled with a space rotation in modern approximate nearest neighbor search systems. The paper proposes a family of block Givens coordinate descent algorithms that learn rotation matrices that are provably convergent on any convex objectives. The Givens algorithms are much more parallelizable than existing rotation learning methods and reduce runtime by orders of magnitude on modern GPUs. They also improve upon vanilla product quantization significantly. The paper also discusses the limitations of existing methods and\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848 for\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002\tCurrent rotation learning methods are trying to minimize quantization distortion for fixed embeddings, which are not applicable to an end-to-end training scenario where embeddings are getting updated constantly. Therefore, this paper tries to address this issue to fully enable end-to-end training of Product Quantization (PQ) based embedding index with retrieval models, by using mathematical studies of the decomposition of orthogonal group. They proposed a family of block Givens coordinate descent algorithms to learn rotation matrices that are provably convergent on any convex objectives by leveraging geometric intuitions from Lie group theory. Authors claimed that their algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies in comparison to the state-of-the-art SVD method.   Their main contributions can be summarized as follows:  - Changing the landscape of learning rotation matrix in approximate nearest neighbor (ANN) search from SVD based to iterative Givens rotation-based, to be applicable to end-to-end neural network training.  - Proposing a family of Givens coordinate block descent algorithm with complexity analysis and convergence proof.  - Proves that for the fixed embedding, their algorithm shows similar convergence result as the existing rotation matrix learning algorithms. Therefore, their proposed algorithm is able to learn the rotation matrix more effectively for the end-to-end training.""834\tThis paper proposes a family of block Givens coordinate descent algorithms to learn rotation matrices that are provably convergent on any convex objectives, using geometric intuitions from Lie group theory and the special orthogonal group SO(n). The proposed algorithms are more parallelizable and converge more stably than existing rotation learning methods. They improve upon vanilla product quantization significantly. The paper also discusses the limitations of existing product quantization methods and suggests ways to overcome them.\tThis paper proposes to learn rotation matrix by Givens coordinate descent algorithms in the context of minimizing the quantization distortion for efficient storage. The proposed family of Givens coordinate descent algorithms are based on geometric intuitions of the special orthogonal group and are provably convergent on any convex objectives. The experiments show that the proposed algorithms are much time efficient and lead to performance improvement in an end-to-end training of embedding indexes.""835\tThis paper discusses the use of product quantization coupled with space rotation in modern approximate nearest neighbor search systems. The existing rotation learning methods minimize the quantization distortion for fixed embeddings, which are not applicable in an end-to-end training scenario where embeddings are constantly updated. This paper proposes a family of block Givens coordinate descent algorithms that learn rotation matrices with provably convergent objectives and are much more parallelizable than existing methods. The algorithms further improve upon vanilla product quantization significantly. The experimental studies show that the proposed algorithms are more stable and reduce runtime by orders of magnitude on modern GPUs.\tThe paper address rotation matrix learning during product quantization in modern ANN embedding search systems. The main contribution is addressing rotation matrix learning via gradient descent of small rotation updates. The approach relies on the decomposition of any small rotation matrix into a product of Given rotations, so that partial derivatives can be obtained in parallel, but the product causes the computation of the rotation matrix itself to be slow, in O(n^2) matrix multiplications, hence the need to select a subset of coordinates and do coordinate descent. Experiments on product quantization show a marginal improvement in results over existing approaches OPQ and Cayley.""836\tThe paper discusses the ability of artificial neural networks to build useful abstractions from data and then reason about them, which is central to human intelligence. It reviews the recent progress made in developing deep learning models that can perform well on computer vision tasks, such as image recognition, object detection, and scene classification, while also demonstrating human-level performance in building useful abstractions from data. However, like humans, these models struggle with isolate and apply these Abstractions to out of distribution test scenarios. The paper also highlights the need for further research in developing more effective models and in understanding the role of Abstraction in human intelligence.\tThis paper targets the problem of abstract reasoning, with a special focus on the task of learning visual analogies. The authors propose a multi-stage neural network (Neural Structure Mapping, NSM) for decomposing the problem into vision relationship recognition and concept inference. They tested their model on an existing RPM (Raven's Progressive Matrices) based visual analogy benchmark that contains different systematic generalization tests and outperformed existing models. The authors made further discussion on these experimental results to support their proposals on model designs.""837\tThe paper discusses the ability of artificial neural networks to build useful abstractions from data and then reason about them, which is central to human intelligence. It also discusses the recent success of deep learning (DL) models in computer vision in building visual abstractions from raw images and demonstrating human-level performance on curated test datasets. However, the models struggle with isolation and systematic application of these abstractions to out of distribution test scenarios, which is a significant limitation compared to humans.\tA model called Neural Structure Mapping (NSM) is introduced to solve the task of abstract visual analogy making. The NSM model consists of a visual relationship encoder and an analogy inference engine. The visual relationship encoder extracts the visual domain elements, including object, attribute, and relation, while the analogy inference engine is a neural modular architecture that constructs the model layout based on the relation and predicts the final answer. On the dataset proposed by Hill et. al., the NSM shows better performance than other baselines.""838\tThe paper discusses the ability of artificial neural networks to build abstract concepts from data and then reason about them, as well as the comparison between humans and machines in this area. The paper also highlights the limitations of deep learning models in out-of-sample applications compared to humans.\tThis paper tackles the problem of analogical reasoning. In particular, it presents a framework for learning the Raven Progressive Matrices (RPM) task, an abstract analogy task.  In the RPM task, a sequence of three images from a source domain are given. There is some relationship that holds for the sequence, e.g. the third image is the union of the first two. Then, given an incomplete sequence of two images from the target domain, the third image must be chosen from a list of four possible candidates.  The proposed Neural Structure Mapping (NPM) system consists of two pieces. The first piece is the Visual Relationship Encoder. Given the source sequence of images, the encoder predicts the type of relationship exhibited in the sequence. This information is passed to the second piece, the Analogy Inference Engine. The architecture of the engine is assembled dynamically, according to the predicted relationship. The assembled network takes the target sequence and the candidate matrices as input and selects the completion of the sequence from among the candidates.  The encoder is trained with the ground truth relationship labels. The engine is trained using the ground truth candidate labels.   The paper presents an experiment to test systematic generalization, in which particular attributes are held out during train time. The NSM system is found to achieve better performance.  In contrast to Hill 2019, which presents the model with semantically-contrasting alternative candidates at train time, NSM achieves good performance even when the alternative candidates are not necessarily semantically related.  ""839\tThis paper discusses the ability of artificial neural networks to build useful abstractions from data and then reason about them, as well as the comparison between human and machine intelligence in this area. The paper highlights the fact that over the past decade, deep learning models have demonstrated human-level performance in building useful abstractions from data when tested on well-defined and constrained tasks, such as image recognition, object detection, and scene classification. However, these models struggle with isolation and systematic application of these abstractions to out of distribution test scenarios, unlike humans. The paper also discusses the limitations of current deep learning models and the need for further research in this area to improve their performance.\tThis paper proposes a new architecture for learning visual analogies, based on Gentner\u2019s Structure Mapping Theory for how humans might draw analogies. Gentner\u2019s theory proposes representing the relationships between objects explicitly, so that this relational structure can be reused in new domains (and suggests that this commonality in structure is what permits analogies to be made between perceptually dissimilar objects). The authors propose a neural network model architecture and test it on the Raven\u2019s Progressive Matrices dataset. The proposed architecture first splits a series of \u2018source\u2019 visual scenes into objects, attributes and the relationships between those scenes, before feeding just the relationship head into a second network. The second network then switches between two different architectures (depending on the relation fed in). The architecture in the second network (whichever is chosen) receives the \u2018source\u2019 relationship and two \u2018target\u2019 scenes before trying to predict which of a set of 4 candidate  \u2018target\u2019 scenes completes the visual analogy between source and target. The authors test their architecture on the generalisation splits in the RPM dataset and compare test accuracy results to the baseline models used by Hill et al, 2019. The authors show that their model (which builds in additional architectural structure) performs better at a subset of tests than more general architectures.""840\tThis paper discusses a new self-supervised method for nucleotide Genome representation learning, called Self-GenomeNet. The method learns and parametersize a representation of nucleotide genomic data using domain-specific characteristics, and is the first to do so. The method uses a reverse-complement of genomic sequences to construct a context network, which captures semantic representations. The network is trained with an unsupervised contrastive loss. The method outperforms state-of-the-art deep learning methods on different datasets, and the learned representations generalize well. The source code of Self-GenomeNet is available.\tRecently several works have proposed semi-supervised learning methods to leverage unlabeled biological sequences for learning their general-purpose representations. In this work, the authors proposed the Self-GenomeNet, a novel contrastive learning method for nucleotides based on the reverse-complement (RC) context prediction. First, given a sequence, they divide it into two subsequences and transform one into its RC. Then, the model is trained to distinguish their representations from those of other random nucleotide sequences. The authors claimed that the proposed method considerably outperforms previous self-supervised baseline models on three benchmark datasets in both self-supervised and semi-supervised evaluation ""841\tThis paper presents a new self-supervised method for learningNucleotide Genome Representation, called Self-GenomeNet. The method uses domain-specific characteristics of nucleotides to learn a representation of genomic data, which can be used for different tasks such as disease diagnosis and genetic counseling. To the best of the authors' knowledge, Self-GenomeNet is the first self-supervised framework for nucleotide Genome Representation. The method uses a reverse complement of genomic sequences as the input and is trained with unsupervised contrastive loss. Extensive experiments on different datasets show that Self-GenomeNet outperforms state-of-the-art deep learning methods and generalizes well. The source code of the method is available.\tThis paper presents a self-supervised learning approach using contrastive loss for representation learning of genomic sequences. The contrastive loss has been used for self-supervised learning in the NLP and computer vision domains and the paper presents its application for genomics. Self-supervised contrastive learning tries to maximize the agreement between augmented views of a sample. Therefore, Self-GenomeNet splits a sequence into two subsequences, and the learned representation of subsequence 1 is compared to subsequence 2 and its revers compliment (positive samples) as well as other sequences (negative samples). The described method has two considerations specific to the genomics tasks - (1) it handles variable sequences (2) it incorporates reverse complement information of the sequences. The method is applied on two prediction tasks and one transfer learning task using sequences from viral, bacterial, and human genomes. Its performance is compared to the supervised model, generative language model, and self-supervised learning models - CPC and Contrastive-sc. The results show improved classification performance over the baseline for both supervised retraining and semi-supervised training settings. ""842\tThis paper proposes a new self-supervised method for learning nucleotide genome representation using domain-specific characteristics. The method, called Self-GenomeNet, uses a reverse-complement of genomic sequences to parameterize the latent space and learn semantic representations. The context network is added to capture semantic representations and is trained with an unsupervised contrastive loss. Extensive experiments with different datasets show that Self-GenomeNet outperforms state-of-the-art deep learning methods. The learned representations generalize well and can be transferred to new datasets and tasks. The source code is available online.\tThis submission introduces self-genomenet, a self-supervised training method for learning from DNA sequences. The self-supervision is done by predicting the end of a sequence from its start (both broken into smaller subsequences), through a contrastive loss against other random sequences. The predicted part is also reverse-complemented (RC), making the network learn the expected reverse-complement invariance of the prediction function. The method is extensively tested on several learning tasks, where it shows good performances.""843\tThis paper proposes a self-supervised method for nucleotide genome representation learning called Self-GenomeNet. The method learns and parameterizes the latent space of genomic data using domain-specific characteristics, leveraging the reverse-complement of genomic sequences. It uses a novel context network on top of features extracted by an encoder network to capture semantic representations. The method is trained with an unsupervised contrastive loss and outperforms state-of-the-art deep learning methods on different datasets. The source code is available.\tThe authors proposed a self-supervised learning method for nucleotide-level genomic data utilizing reverse-complement of genomic sequences. The proposed method achieved a considerable performance improvement. In addition, the authors proposed an architecture called Self-GenomeNet that handles varying-length genome sequences.""844\tThis paper discusses a steerable model for point clouds that operates usingspherical decision surfaces. The model is designed to be able to makeclass predictions equivariant to rigid transformations, which is a key goal in computer vision. The paper focuses on 3D geometry and derived a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting rotational equivariance, the model parameters are fully steerable at inference time, allowing for online optimization and equivariant class predictions. The paper also discusses the challenges and limitations of the approach, as well as potential future directions.\tThis paper proposes to make the geometric neurons of Melnyk et al.'21 to be steerable so that objects undergoing arbitrary rotations can be classified with higher accuracy. This is done in multiple stages. First, the neurons of Melnyk et al. are trained to convergence with a hyperspherical output layer. Then, the frozen weights are transformed such that the input of a *steerable neuron* can be written as a linear combinations of the rotated versions of itself. The experiments act as a sanity check while demonstrating the validity of the algorithm on a simple human pose dataset.""845\tThis paper discusses a steerable learning-based approach to operating on point clouds and making predictions on known point sets in unknown orientations. The paper proposes aspherical decision surfaces and derivations a 3D steerability constraint for hypersphere neurons. The model is shown to be fully steerable at inference time and allows for equivariant and invariant class predictions. The approach is applied to a real-world case study and shown to be effective in generating\u51c6\u786e\u7684\u9884\u6d4b\u7ed3\u679c.\tThe paper proposes a method for constructing steerable spherical neurons, building on the recent Geometric Neurons developed in Melnyk et al 2021. The main technical result in the paper is a steerability constraint for a geometric neuron, as given by eq. (13) in the paper. This constraint is used in an implementation whereby a steerable model is constructed and is then use for two tasks. The first task involves the use of steerable spherical neutrons for the classification of 3D Tetris objects seen under rotations, and the second is a similar experiment applied to 3D skeleton data. The results demonstrate a very large performance boost when using the steerable versions. The second experiment builds on this idea to construct a version that adapts a possibly imperfect initial rotation estimate, using the representation. This second experiment is in the spirit of demonstrating equivariance under 3D rotations with perturbations.""846\tThe paper presents a steerable model for point clouds that is based onspherical decision surfaces and operates on point clouds. The model is capable of making equivariantclass predictions and is fully steerable at inference time. The paper focuses on 3D geometry and derivations a 3D steerability constraint for hypersphere neurons. It uses a synthetic point set and real-world 3D skeleton data toshow how the proposed spherical filter banks enable making equivariant and invariable class predictions for known point sets in unknown orientations.\t## Summary and contributions. Authors propose 3D \"spherical neurons\" leading to rotationally equivariant layers. They do so by building on the spherical and geometric neurons introduced in Melnyk et al. (2021), which leverages the conformal space (for $\\mathbb{R}^n$) to perform operations. The authors then solve for the steerability constraint for this neuron and empirically show that the proposed approach overperforms Melnyk et al. (2021) on rotated 3D data. ""847\tThe paper describes a steerable model that can operate on point clouds and make predictions based on the relationships between the points. The model is based onspherical decision surfaces and has the ability to be flexible and learn representations that are equivariant to rigid transformations. The paper also discusses how the model is able to achieve rotational equivariance byExploiting the rotational equivariance, we show how our model parameters are fully steerable at inference time. The model is used to make predictions on a known point set in unknown orientations, and the results show that it is able to make accurate class predictions.\tThe paper aims to derive a steerability constraint for spherical neurons (3D point classifiers with spherical decision boundary). The steerability constraint enables test-time optimization of a pre-trained classifier to make predictions equivariant to 3D rotation perturbations applied to the input. When input rotation perturbations are unknown, the authors propose a method to recover the unknown rotations and therefore make rotation-invariant predictions. The experiments on a few small scale datasets verifies some of the claims.""848\tThis paper discusses the use ofContinual Learning (CL) methods in natural language processing (NLP), specifically in\u51cf\u8f7b catastrophic forgetting (CF) while\u9650\u5236\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58 footprint\u3002 CL methods have been proposed in computer vision and robotics, but they are still a relatively new topic in NLP, reflected by the relatively smaller number of proposed methods (Biesialska et al., 2020)\u3002 The paper explains the importance of CL in NLP, including how it can help improve model performance by relieve CF\u3002 The paper also discusses the challenges of evaluating CL methods, including the subtle differences in the way methods are evaluated and the use of pretrained language models\u3002 The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9bCL\u65b9\u6cd5\uff0c\u5e76\u89e3\u91ca\u4e86\u5b83\u4eec\u7684\u4f18\u70b9\u3002\tAs the title suggests, the paper is a comparison of recent continual learning methods that prevent catastrophic forgetting and their effectiveness in some text classification tasks using popular pretrained language models such as BERT, RoBERTa, etc. The paper divides continual learning methods into three categories: (1) rehearsal-based, (2) regularization-based, and (3) dynamic architecture. The experimental results show that rehearsal based methods are superior to the other two, and also that BERT is generally better than other candidates. The paper then proposes a new probing techniques to find out what makes rehearsal-based method better and what's happening inside BERT. The paper finds that the last layer has the biggest catastrophic forgetting and lower layer is less impacted.""849\tThis paper discusses the issue ofContinual Learning (CL) in natural language processing (NLP), specifically how to evaluate and compare the effectiveness of CL methods. It also discusses the challenges of evaluating CL methods, including the need for subtle differences in evaluation criteria and the use of pretrained language models. The paper provides a summary of existing CL methods and their strengths and weaknesses, and suggests future directions for research in this area.\t This paper conducts an empirical study on the catastrophic forgetting of pretrained language models. On two continual learning settings (class incremental and task incremental), the paper evaluates multiple pre-trained models on different data sets, to see how severe the catastrophic forgetting issue is for these pre-trained models. Then the paper also tests the effectiveness of multiple continual learning methods on such pre-trained models and draws some conclusions. ""850\tThis paper discusses the topic ofContinual Learning (CL) methods in natural language processing (NLP). CL methods strive to train a model from a stream of non-i.i.d. samples, relieving catastrophic forgetting (CF), while limiting computational costs and memory footprint. The paper provides an overview of the existing CL methods in NLP, including some that have been proposed recently. It also discusses the challenges of evaluating these methods, including the need for subtle differences in evaluation settings and the use of pretrained language models. The paper concludes by providing a summary of the advantages and limitations of CL methods in NLP.\tThe authors perform a comprehensive study of how pretrained language models work in the continual learning setting. The authors study 5 relevant pretrained language models (masked and unmasked) and somewhere between 3 and 6 continual learning strategies depending on where in the paper they are counted. In addition to a thorough everything-by-everything evaluation, the authors hone in on the details of how the different models and CL approaches are reflected in the transformer layers. The authors find that the different language models studied perform relatively differently, both qualitatively and quantitatively, and these insights may provide useful for directing future improvements.""851\tThis paper presents the latest advances inContinual Learning (CL) methods in natural language processing (NLP). CL is an approach to training models that alleviates the issue of catastrophic forgetting while reducing computational and memory costs. The paper discusses the history of CL in NLP, the various approaches proposed, and the challenges faced in evaluating these methods. The paper also highlights the recent advances in using pretrained language models in CL methods in NLP, and the difficulties in evaluating the performance of these models.\tThis paper explores the continual learning performance when combining different PLMs and common continual learning methods with 3 challenging NLP classification tasks.  To benchmark these combinations the methods are evaluated in task-incremental and class-incremental learning settings over various NLP end-tasks, which covers common learning settings in continual learning and NLP. There is also a layer-wise performance analysis to identify which layers keep or forget task relevant information during training.  Overall the paper shows that forms of replay outperform other methods like regularization.""852\tThis paper discusses the issue of Byzantine clients in Federated Learning, which can bring down the overall accuracy of the learning process. The paper\u4ecb\u7ecd\u4e86 various approaches for countering Byzantine clients, including Krum Blanchard et al. (2017), Bulyan Mhamdi et al. (2018), Trimmed Mean and Median Yin et al. (2018), FoolsGold Fung et al. (2020), and FABA Xia et al. (2019). The paper also presents a state-of-the-art approach for untaging the malicious gradient attacks, which is not yet widely used.\tThe paper tackles the problem of adversarial attacks in federated learning settings. The main proposal is a defensive technique to address the \u201cbyzantine generals\u201d problem in federated learning: how to ensure that the general ML model is not affected by \u201cpoisonous\u201d attempts made by corrupted clients. The proposed technique is experimentally validated on four datasets, outperforms previous defensive methods, and the evaluation also considers adaptive adversaries with increasing degrees of knowledge.   Overall, the presentation of the paper is very good. The quality of the English text is good. Figures are appropriate, Tables require some editing. The topic addressed by the manuscript is trendy and in-line with ICLR\u2019s scope. The references should be improved The contribution is significant  STRENGTHS: + Adaptive adversary  + Trendy subject (federated learning) + Evaluation on multiple datasets + Technically sound  WEAKNESSES - Unclear assumptions and threat model. - Problem or Feature space attacks? - Lack of a concrete use-case - Tradeoff? ""853\tThis paper discusses the problem of Byzantine clients in Federated Learning (FL), which can attack the training phase of the system and affect the overall accuracy. The paper\u4ecb\u7ecd\u4e86 various approaches for countering Byzantine clients, including Krum Blanchard et al. (2017), Bulyan Mhamdi et al. (2018), Trimmed Mean andMedian Yin et al. (2018), FoolsGold Fung et al. (2020), and FABA Xia et al. (2019). The paper also presents a state-of-the-art attack surface against Byzantine clients, using the latest techniques and algorithms. The paper concludes by discussing the potential implications of this threat and recommendations for addressing it in FL systems.\tThe authors propose TESSERACT, an aggregation scheme that is robust to the directed deviation attack (proposed in Fang et. al. 2020). ""854\tThis paper discusses the topic of FL, which is a way for multiple clients on heterogeneous platforms to learn collaboratively without sharing their local data. The clients send their local gradients to the parameter server, which aggregates the gradients and updates the global model for the local clients to download. However, FL can be attack during the training phase by compromising a set of clients that then send maliciously crafted gradients. The attack can be targeted against particular data instances or can be untargeted, which brings down the overall accuracy by affecting all classes. To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL, including Krum Blanchard et al. (2017), Bulyan Mhamdi et al. (2018), Trimmed Mean andMedian Yin et al. (2018), FoolsGold Fung et al. (2020), and FABA Xia et al. (2019). This work uses the state-of-the-art untarge methods to counter the attack.\tThis submission with the title \"Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks \" discusses defenses against data poisoning in federated learning. The authors propose a novel defense against the recently popularized attack \"Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks \" by Yang et al. This attack reduces model availability by sending malicious updates from compromised client that maximize sign flips in the global model gradient.  This defense then proposes a measure of change in gradient direction that can be evaluated for each local update and used to dynamically down-weight clients with a large number of flips in direction. ""855\tThis paper discusses the topic of FL, a technique for collaborative learning where multiple clients on heterogeneous platforms learn without sharing their local data. The clients send their local gradients to the parameter server, which aggregates them and updates the global model for the local clients to download. However, FL can be\u653b\u51fbd by compromising a set of clients that then send maliciously crafted gradients, which can affect the overall accuracy of the learning process. To counter this threat, a set of approaches have been developed for countering Byzantine clients in FL, including Krum Blanchard et al. (2017), Bulyan Mhamdi et al. (2018), Trimmed Mean and Median Yin et al. (2018), FoolsGold Fung et al. (2020), and FABA Xia et al. (2019). This paper presents an analysis of these approaches and discusses how they can be used to counter the malicious gradient attack on FL.\tThis paper studied a very important topic in the field of federated learning: how to efficiently resist untargeted model poisoning attacks. In order to defend against such a poisoning attack, the authors developed TESSERACT, an aggregation algorithm that assigns reputation scores to participating clients based on their behavior in the training phase and weights the client's contribution. Extensive case studies have verified the effectiveness of the algorithm. In particular, the experimental results show that TESSERACT provides robustness against even a white-box version of the attack. ""856\tThis paper discusses the issue of debiasing linear functional estimation methods to reduce the effects of regularization and/or model selection on the object of interest. The paper proposes an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. The method only requires value query oracle access to the linear functional. The multi-tasking Neural Net debiasing method involves minimizing a combined Riesz representer and regression loss, while sharing representation layers for the two functions. The Random Forest method also involves minimizing a regression loss while sharing representation layers with the Neural Net. The paper demonstrates that the proposed methods achieve \u221a n-consistent and asymp-totically normal estimation of the object of interest.\tThis paper considers the problem of estimating an expectation over covariates of some functional of an unknown regression function. The authors propose two estimators, one based on neural networks and one based on random forests. In contrast to many (all?) previous estimators which were derived for specific functionals, these estimators are applicable to general functionals. They employ a (new?) debiasing technique. Moreover, they use a novel multi-task architecture based on the observation that it suffices to estimate the regression function as a function of the Riesz representer. The paper demonstrates improved accuracies compared to prior work and good coverage on semi-synthetic tasks derived from two data sets.""857\tThis paper discusses the need for debiasing in order to achieve \u221a n-consistent and asymp-totically normal estimation of an object of interest. It proposes an automatic debiasing procedure based on learning the Riesz representation of a linear functional using Neural Nets and Random Forests. The method only requires value query oracle access to the linear functional. The paper proposes a multi-tasking Neural Net debiasing method and a Random Forest method for reducing the effects of regularization and model selection on the object of interest. The methods are implemented and compared against traditional debiasing techniques. The results show that the proposed methods provide more accurate and efficient estimation of the object of interest.\tThe authors address the problem of estimating the average value of a moment function that depends on an unknown regression function, which is commonly used in causal inference.  By the Riesz representation theorem, the authors design a loss function where the Riesz representer turns out to be the minimizer of the proposed loss.  The authors propose both a Neural Network method (RieszNet) and a random forest method (ForestRiesz), i.e. a multi-tasking Neural Network method where the loss function is a combination of the Riesz representer and regression loss, and a random forest method that leans representation of both the regression function and the Riesz function.  The authors conduct experiments of estimating the average treatment effect and average marginal effects, and show that the proposed RieszNet and ForestRiesz beat the state-of-the-art methods.   ""858\tThe paper discusses the issue of debiasing in machine learning, where it is often necessary to reduce the effects of regularization and/or model selection on the object of interest in order to achieve \u221a n-consistent and asymp-totically normal estimation. The paper proposes an automatic debiasing procedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. The method only requires value query oracle access to the linear functional. The multi-tasking Neural Net debiasing method involves minimizing a combined Riesz representer and regression loss while sharing representation layers for the two functions. The Random Forest method also discusses the issue of debiasing and proposes a method for reducing the effects of regularization and model selection.\tThis paper shows novel methods based on the recent findings on the Riesz representation in econometrics and machine learning, such as Chernozhukov et al. (2021). While the proposed applications make sense and are persuasive, the contributions of this paper are a bit limited. Although the authors support the soundness of the proposed algorithms in experiments, the experiments are a bit simple and may not be sufficient for the justification. ""859\tThe paper discusses the need for debiasing in order to achieve \u221a n-consistent and asymp-totically normal estimation of an object of interest. It proposes an automatic debiasing procedure based on learning the Riesz representation of a linear functional using Neural Networks and Random Forests. The method only requires value query oracle access to the linear functional. The paper proposes a multi-tasking Neural Net debiasing method and a Random Forest method for reducing the effects of regularization and model selection on the object of interest.\tThe authors study the nonparametric estimation of an important class of (causal) estimands that includes the average treatment effect (ATE) in experiments and observational studies under unconfoundedness. The main innovation is the \"automatic\" nature of the procedures, one based on deep learning and one on random forests. While previous work has developed hand-tailored constructions for the ATE (a traditional problem in statistics), the authors show that existing constructions can be generalized considerably based on recent advances in semi- and nonparametric statistics based on Riesz representers. The key challenge this work addresses is the fact that the Riesz representer is typically unknown.  The main proposed method based on deep learning, RieszNet, may be seen as a generalization of the DragonNet procedure by Shi et al. (2019) from the ATE to more general estimands. Even in the case of the ATE, DragonNet and RieszNet are not the same and RieszNet outperforms DragonNet in simulations: RieszNet directly targets the inverse propensities, while DragonNet estimates the propensities and then plugs in their inverse.""860\tThis paper discusses the finite-time convergence result of the actor-Critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. MARL problems involve a group of N agents working together to maximize the global average reward over a communication network. The agents have only known the rewards and actions of themselves, and not the joint actions of the other agents. The goal of the paper is to establish a finite-time convergence result for the actor-Critic algorithm in this setting, which would be a significant step forward in the field of MARL. The paper also proposes a new algorithm, the mini-batch Markovian sampled fully decentralized actor-Critic algorithm, and analyzes its finite-time convergence and sample complexity. The analysis shows that the sample complexity of the algorithm is O(N2/\u01eb2log(N/\u01eb)), which matches that of the state-of-the-art single-agent actorCritic algorithms for reinforcement learning. This result is significant because it demonstrates that the actor-Critic algorithm can be used in a practical setting for MARL, even in a setting where the knowledge of the joint actions of the agents is limited. 2 Keywords: fully decentralized multi-agent reinforcement learning, average reward, MARL, actor-Critic algorithm, finite-time convergence, sample complexity. 2  Introduction: Multi-agent reinforcement learning (MARL) is a problem that has received significant attention in the field of artificial intelligence (AI). In MARL, a group of N autonomous agents interact over a communication network to achieve a goal. The agents have limited knowledge of the actions of the other agents, and the goal is to maximize the global average reward over a period of time. The problem of MARL is widely applicable to a wide range of real-world applications, such as supply chain management, autonomous vehicles, and social media marketing. 3  Background: In reinforcement learning, an AI system is trained by interacting with an environment and receiving rewards for each action. The goal is to learn a policy that maximizes the expected cumulative reward over time. The policy is typically represented as a function that takes an action and receives an outcome. The actor-Critic algorithm is one of the most popular algorithms for reinforcement learning, which is used to optimize the policy. The algorithm consists of two components: the actor, which takes an action, and theCritic, which takes an action based on the actor's decision. The algorithm is designed to balance the trade-off between exploration and exploitation, where exploration refers to the use of new information to find good actions, and exploitation refers to using information learned from the past actions. 4  motivation: In this paper, we focus on the finite-time convergence result of the actor-Critic algorithm for fully decentralized multi-agent reinforcement learning problems with average reward. In this setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed, the goal is to train a policy that maximizes the global average reward over a period of time. The problem is widely applicable to a wide range of real-world applications, such as supply chain management, autonomous vehicles, and social media marketing. 5 Conclusion: The paper presents the first finite-time convergence result of the actor-Critic algorithm for fully decentralized multi-agent reinforcement learning problems with average reward. The analysis shows that the sample complexity of the algorithm is O(N2/\u01eb2log(N/\u01eb)), which matches that of the state-of-the-art single-agent actorCritic algorithms for reinforcement learning. This result is significant because it demonstrates that the actor-Critic algorithm can be used in a practical setting for MARL, even in a setting where the knowledge of the joint actions of the agents is limited. The paper also proposes a new algorithm, the mini-batch Markoviansampled fully decentralized actor-Critic algorithm, and analyzes its finite-time convergence and sample complexity. The analysis shows that the sample complexity of the algorithm is O(N2/\u01eb2log(N/\u01eb)), which is significantly better than the previous methods. Overall, this paper provides a useful analysis and algorithm for MARL problems in a practical setting.\tThis paper establishes the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. It focuses on the practical setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. The established finite-sample complexity matches that of the state-of-the-art single-agent actor-critic algorithms.""861\tThe paper \"Finite-time Convergence of the Actor-Critic Algorithm for Decentralized Multi-Agent Reinforcement Learning\" is an interesting one, as it proposes a new algorithm for MARL problems with average reward, where each agent only knows its own rewards and actions, and the knowledge of the joint actions of the agents is not assumed. The problem is a classic one in reinforcement learning, and many algorithms have been proposed for it, but the ones that have shown the best performance in practice are those that are fully decentralized and have no centralized component.\n\nThis paper aims to establish the first finite-time convergence result of the actor-Critic algorithm for fully decentralized MARL problems, which would be a significant step forward in the field. It also proposes a new algorithm, the mini-batch Markovian sampled fully decentralized actor-Critic algorithm, and analyzes its finite-time convergence and sample complexity. The sample complexity of this algorithm is O(N2/\u01eb2log(N/\u01eb)), which matches that of the state-of-the-art single-agent actorCritic algorithms for reinforcement learning, which are of O(N2/\u01eb2log(N/\u01eb)) or O(NlogN) sample complexity.\n\nOverall, this paper provides a valuable contribution to the field of MARL and demonstrates the potential of fully decentralized algorithms in complex reinforcement learning problems.\tThis paper considers cooperative multi-agent reinforcement learning (MARL) for average reward MDPs with fully decentralized actor-critic methods. In particular, the authors make some progress on top of existing works in this direction, and in particular (Zhang et al., 2018). More precisely, the authors remove the assumption in (Zhang et al., 2018) that the joint actions are observable to all agents, and propose to modify the actor updates with mini-batch TD sharing to accommodate the scenario where each agent only observes its own action. The authors then establish a finite-sample bound for the proposed algorithm in terms of convergence to stationary points under linear value function approximation. Numerical experiments are also provided to showcase the benefits of the modifications over the algorithm in (Zhang et al., 2018). ""862\tThe paper discusses the finite-time convergence of the actor-Critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. MARL problems are a type of reinforcement learning problem where a set of N agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network. The paper proposes a mini-batch Markovian sampled fully decentralized actor-Critic algorithm and analyzes its finite-time convergence and sample complexity. The sample complexity of this algorithm is O ( N2/\u01eb2 log ( N/\u01eb) ), which matches that of the state-of-the-art single-agent actorCritic algorithms for reinforcement learning. The paper also discusses the challenges and opportunities in analyzing the convergence of the actor-Critic algorithm for MARL problems. The paper provides an overview of the background and motivation for MARL problems and the challenges in analyzing the convergence of the actor-Critic algorithm. 2  Theorems : The paper establish the following finite-time convergence results for the actor-Critic algorithm: ( 1 ) The algorithm will eventually reach a state where the expected value of the action-return sequence is\u8d8b\u5411\u4e8e 0 . ( 2 ) The algorithm will eventually reach a state where the expected value of the action-return sequence is\u8d8b\u5411\u4e8e 1 . 3 Conclusion : The paper provides the first finite-time convergence result of the actor-Critic algorithm for fully decentralized MARL problems with average reward. The paper also discusses the challenges and opportunities in analyzing the convergence of the actor-Critic algorithm for MARL problems. The paper provides an overview of the background and motivation for MARL problems and the challenges in analyzing the convergence of the actor-Critic algorithm. The paper provides a practical solution for the MARL problem by propose a mini-batch Markovian sampled fully decentralized actor-Critic algorithm with O ( N2/\u01eb2 log ( N/\u01eb) ) sample complexity. The paper also discusses the future research directions and applications of themarL problems.\tThis paper studies the cooperative average reward fully decentralized multi-agent reinforcement learning (MARL) problems, where the agents interact with their neighbors over a communication network. It proposes a consensus-based actor-critic algorithm and shows its convergence to the stationary point. The convergence rate and sample complexity of this algorithm are provided and comparison with existing algorithm is shown in the numerical experiments. ""863\tIn recent years, there has been increasing interest in the use of multi-agent reinforcement learning (MARL) problems for real-world applications, such as machine learning and artificial intelligence. MARL problems involve a group of N agents working together to achieve a common goal, such as improving a machine or achieving a certain objective. The agents communicate with each other over a communication network and take actions that affect the state of the environment. The goal of the problem is to find the optimal action sequence that maximizes the global average reward of the agents.\n\nThe use of fully decentralized systems, where the actions of each agent are only known to itself, has been proposed as an improvement over traditional centralized systems, where the actions of all agents are known at once. In fully decentralized systems, each agent can make decisions based on its own knowledge and data, rather than being influenced by the actions of other agents. This allows for more flexibility and complexity in the design of the system, and has the potential to improve the performance of the system.\n\nIn this paper, we focus on establishing the finite-time convergence result of the actor-Critic algorithm for fully decentralized MARL problems with average reward. This is a significant step forward in the field of MARL, as it provides a guarantee for the performance of the algorithm over time, even in the presence of large communication networks and high computational complexity.\n\nThe paper also proposes a mini-batch Markovian sampled fully decentralized actor-Critic algorithm, and analyzes its finite-time convergence and sample complexity. We show that the sample complexity of this algorithm is O ( N2/\u01eb2 log ( N/\u01eb ) ), which matches that of the state-of-the-art single-agent actorcritic algorithms for reinforcement learning. This result provides a strong foundation for the use of fully decentralized systems in MARL problems, and highlights the importance of analyzing the performance of the algorithms in a practical setting.\n\nOverall, this paper provides a novel finite-time convergence result for the actor-Critic algorithm for fully decentralized MARL problems, and proposes a new algorithm that matches the sample complexity of the state-of-the-art single-agent actorCritic algorithms for reinforcement learning. This work has the potential to improve the performance and scalability of MARL systems, and has important implications for the use of fully decentralized systems in real-world applications.\tThis paper studies a networked MARL problem based on the model in [Zhang et al 2018], where each agent can observe the global state, take local action and observe local rewards. The key difference in setting from [Zhang et al 2018] is that [Zhang et al 2018] assume the global action can be observed, but in this paper, only local action is known to each agent. To deal with this, an additional consensus loop is added to estimate the average TD error, which can be used to estimate the advantage function. Further, compared to [Zhang et al 2018], a finite time error bound is provided.  ""864\tContrastive Learning is a paradigm for learning data representations without labeled data. It involves maximizing the similarity between augmented views of the same image (a.k.a. positive samples) and minimizing the similarity between that of two random images (a.k.a. negative samples). Intuitively, it is an instance discrimination task ( differing each image from others) instead of a classification task. Contrastive Learning has achieved impressive results and has gradually closed the gap between supervised and unsupervised learning, resolving the hunger for labeled data in the deep learning field. However, a theoretical understanding of how contrastive learning actually works in practice is still under-explored.\tThis paper aims to provide theoretical understanding for contrastive learning where \"similar pairs\" of points $x$ and $x^+$ are encouraged to have similar representations through an InfoNCE inspired objective function. Some prior works show the benefit of learned representations for linearly classifying downstream classes, by making conditional independence like assumption on the similar pairs or positive samples, i.e. $x$ and $x^+$ are (approximately) conditionally independent given downstream label $y$. This work argues that these assumptions are quite strong for contrastive learning with data augmentations, and aims to show guarantees under the following weaker and more realistic assumption: support of augmentation distribution of different inputs from the same class overlap to form a \"connected graph\" of inputs within a class, whereas support of augmentations of inputs from different classes do not overlap. Lower and upper bounds using this and some other assumptions, connecting the downstream performance of representation function to the contrastive loss. Some simulation experiments are presented to support some aspects of the theoretical analysis.  Using the insights from the analysis, the paper proposes an \"Average Confusion Ratio (ACR)\" metric that can be used to predict the ranking of downstream performances of different augmentations **using only unlabeled data**. Experimental evidence is provided on CIFAR and STL datasets to verify the efficacy of this metric for some practical augmentations.    While there are some interesting aspects in the paper (especially the ACR metric), the theoretical analysis seems to have raised many questions and concerns that I have summarized below (details in main review).   - **Soundness of assumptions**: Assumption 4.6, which is crucial, seems questionable and may not be coherent or appropriate to make in this setting. More on this in point (W2) of main review  - **Deeper dive into theoretical results**: There is a lack of discussion about the (non-)vacuousness of the bounds in the main results Theorem 4.2 and 4.8, that puts the interpretation and significance of the result in question. More on this and related issues in point (W2) of main review.  - **Comparison to prior work**: The work of HaoChen et al. in particular is not adequately compared to, especially since some of the points being addressed here are covered through a different kind of analysis in that paper. More on this in point (W3) of main review.""865\tContrastive Learning is a paradigm that can learn data representations without labeled data. It involves maximizing the similarity between augmented views of the same image (positive samples) and minimizing the similarity between that of two random images (negative samples). Intuitively, it is an instance discrimination task (differs each image from others) instead of a classification task. Contrastive Learning has achieved impressive results and has gradually closed the gap between supervised and unsupervised learning, resolving the hunger for labeled data in the deep learning field. However, a theoretical understanding of how contrastive learning actually works in practice is still under-explored.\tThe authors provided a new understanding of contrastive learning from the perspective of data augmentation for intra-class samples. In particular, the authors proposed to understand the role of data augmentation as to create certain ``chaos'' between intra-class samples so to encourage the clustering of intra-class samples and also the learning of class-separated representations. Additionally, a new metric ARC is proposed to evaluate the downstream performance. The conclusion is validated via both synthetic and real-world datasets. ""866\tContrastive Learning is a paradigm for learning data representations without labeled data. It involves maximizing the similarity between augmented views of the same image (a.k.a. positive samples) and minimizing the similarity between that of two random images (a.k.a. negative samples). Intuitively, it is an instance discrimination task (differencing each image from others) instead of a classification task. Contrastive Learning has achieved impressive results and has gradually closed the gap between supervised and unsupervised learning, resolving the hunger for labeled data in the deep learning field. However, a theoretical understanding of how contrastive learning works in practice is still under-explored.\tThe current leading theory of what contrastive losses are doing and why they work interprets contrastive learning as balancing alignment with uniformity, as proposed in [2].  This paper seeks to augment that understanding of contrastive learning using a new perspective, focusing on the role of data augmentation.  It is well-known that contrastive learning techniques are highly sensitive to the data augmentation schemes used, most notably discussed in [1].  In this work, the authors interpret augmentation as a way to connect different intra-class images together.  Then, the contrastive loss is seen as a way to gradually cluster intra-class samples together by aligning augmented views, producing representations that are class-separated even in feature space.  On top of introducing a new lens with which to understand contrastive learning, the authors also provide proofs on performance guarantees, as well as a new evaluation metric.  The metric is inspired by their augmentation-oriented understanding, and was also found to align well with downstream performance.  The authors provide a scenario where alignment and uniformity are satisfied, but fails to translate well to downstream classification accuracy.  This suggests to them that the instance discrimination task alone cannot guarantee the learning of class-discriminative features that would enable better downstream classification, and directs their attention to the other important component of contrastive-learning to help explain the story: augmentation.  They then build off the analytical work of [3] to prove guarantees for the downstream performance with a relaxed assumption.   [1] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, 2021.  [2] Wang and Isola., Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, 2020.  [3] Saunshi et al., A theoretical analysis of contrastive unsupervised representation learning, 2019. ""867\tThis paper discusses the emerging field of contrastive learning, which is a paradigm for learning data representations without labeled data. Contrastive learning achieved impressive results and has gradually closed the gap between supervised and unsupervised learning, resolving the hunger for labeled data in the deep learning field. The general methodology of contrastive learning is to maximize the similarity between augmented views of the same image (a.k.a. positive samples) and minimize the similarity between that of two random images (a.k.a. negative samples). Intuitively, it is an instance discrimination task (diffusing each image from others) instead of a classification task. The paper provides a theoretical understanding of how contrastive learning works in practice and\u63a2\u8ba8 its potential applications.\tThe paper proposes a new theory for understanding contrastive representation learning. The novelty is the focus on the interplay between alignment and augmentation. Prior work has identified alignment as one of the factors of contrastive learning, but have not investigated how different types of augmentations may affect the learned embeddings. This work adds that missing piece. The results intuitively make sense, showing that proper amount of augmentation (that connects samples of the same class) has positive effect on downstream classification. Empirically, the authors verify that too weak or too strong augmentation harms performance. Based on observations, the authors define a metric on ratio of positive pairs among nearest (embedding) neighbors, and found the change of this metric throughout training positively correlate with performance.""868\tThis paper provides an overview of speech restoration, a challenging but important research topic that aims to restore degraded speech signals to high-quality signals. It explores the background of speech restoration, the types of distortions that can affect speech, the existing approaches to speech restoration, and the challenges that remain in the field. The paper also discusses the potential applications of speech restoration, including online meeting, hearing aids, and audio editing. Overall, the paper provides a comprehensive review of the current state of the art in speech restoration and highlights the areas for future research.\tThe paper proposes an approach called VoiceFixer which is aimed at restoring degraded speech signals. The paper considers a variety of speech degradations - additive noise, reverberations, clipping and limited bandwidth. The paper describes a two stage approach in which the first stage aims to produce restored mel-spectrogram and then a vocoder is used to synthesize the  speech from the restored mel-spectrogram. Experiments are done using the VCTK dataset and experiments are done using single distortions as well as combinations of all 4 distortions. ""869\tThis paper provides an overview of speech restoration, a challenging research topic that aims to restore degraded speech signals to high-quality signals. The paper\u8ba8\u8bba\u4e86 speech restoration\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u5305\u62ec\u4fe1\u53f7\u91cd\u6784\uff0c\u5931\u771f\u6d88\u9664\u548c\u4fe1\u53f7\u6062\u590d\u3002\u8fd8\u8ba8\u8bba\u4e86\u76ee\u524d\u7684\u7814\u7a76\u8d8b\u52bf\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002\u6700\u540e\uff0c\u6587\u7ae0\u603b\u7ed3\u4e86 speech restoration\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5728\u7ebf\u4f1a\u8bae\uff0c hearing aids, \u97f3\u9891\u7f16\u8f91\u548c\u6539\u5584\u542c\u529b\u3002 The paper provides an overview of speech restoration, which is a challenging research topic that aims to restore degraded speech signals to high-quality signals. The paper discusses the basic concepts of speech restoration, including signal reconstruction\uff0c\u5931\u771f\u6d88\u9664 and signal recovery. The paper also discusses the current research trends and future directions of speech restoration. Finally, the paper summarizes the application of speech restoration, including online meetings, hearing aids, audio editing and improving\u542c\u529b.\tThis paper proposes a general speech restoration (GSR) task that tries to remove multiple distortions in a single model. In addition, it also presents a generative framework called VoiceFixer consisting of analysis and synthesis stages to address the general speech restoration task. In VoiceFixer, the authors employ a ResNet for modeling the analysis stage and a TFGAN-based neural vocoder for synthesis stage. They report that their baseline GSR and VoiceFixer surpass the single speech restoration (SSR) models with more improved results by the latter. Their idea was well described and the experiments are systematical and extensive. The results are consistent and clear.  The contribution of this paper is to incorporate a variety of speech restoration tasks including speech denoising, super-resolution, dereverberation, declipping, etc. in a single unified task called GSR. Another is the proposal of a well-performing generative speech restoration framework called VoiceFixer. ""870\tThis paper presents an overview of speech restoration, a challenging but important research topic due to the ubiquitous nature of speech distortion. The paper discusses the importance of speech restoration, the types of distortion that can be removed using speech restoration, previous work in single task speech restoration (SSR), and the challenges of speech restoration, including the need for a multi-task approach, the variety of distortion types present in the world, and the need to consider the environment in which the speech was recorded. The paper also highlights the potential applications of speech restoration, including online meeting, hearing aids, and audio editing.\tThis paper introduces a unified view of several speech restoration problems including denoising, decliping, dereverberation and audio super-resolution. In order to solve the problem of the general speech restoration task, the authors propose a U-Net architecture which is trained on all of these tasks simultaneously during training time. The authors conduct extensive experiments for the general speech restoration task as well as the individual tasks where they compare the proposed models with more specialized models bounded to each distortion. The experimental results show that the proposed VoiceFixer combination of the model and the analysis-synthesis procedure are capable of effectively removing the speech distortions and in some cases outperform previous approaches in the literature.""871\tThis paper provides an overview of speech restoration, a challenging research topic due to the wide variety of distortions in the world. The paper discusses the importance of speech restoration, the types of distortions that can be caused, and previous work in single task speech restoration (SSR) and multi-task speech restoration (MSRT). The paper also highlights the need for future research in speech restoration to address the challenges that remain. Finally, the paper concludes by suggesting possible future directions for research in speech restoration.\tThe paper proposes a single system to deal with the speech enhancement tasks of denoising, dereverb, bandwidth extension (BWE) and declipping. The system is a two-stage system composed of an analysis module producing mel-band masks and a synthesis module using a vocoder. Both modules reuse existing architectures. Results comparing the proposed system to author-derived counterparts of it and to some existing systems for specific tasks show improvement for the proposed system in some cases, while achieving similar performance as existing systems in other cases. Data and code for reproducibility are provided.""872\tThis paper provides an overview of the importance of learning the laws and dynamic interaction information between variables in multivariate time series, particularly for accurately forecasting future time series. The paper presents case studies where the dependency relationship between variables is demonstrated and discusses the challenges of modeling such dependencies. The paper highlights the importance of including all available information in the time series in order to capture the true dependency structure, and suggests that there is a trade-off between the amount of information required and the computational complexity of the model. The paper also provides practical advice for data scientists and other researchers who are working with multivariate time series, such as to use a combination of models and to have a clear understanding of the dependency structure.\tFor multivariate time series forecasting, this paper proposes to use a tensor network to model the variable space and improve the quality of the variable space by designing the series-variable encoder. Under the variable space, this paper also proposes an N-order residual connection approach and the skip-connection layer for processing the long-term data. The proposed model MVSRTN achieves good results on some datasets. However, this paper has not well explored MVSRTN, and the results are not very competitive. In addition, there are many typos in the paper.""873\tThis paper provides an overview of the dependencies in multivariate time series and their use for forecasting. It discusses the challenges of learning these dependencies and proposes several methods for identifying and analyzing them. The paper provides case studies of different real-world examples to illustrate the effectiveness of the proposed methods. The paper also discusses the limitations and future directions of this field.\tThe paper proposes a deep learning architecture for time series forecasting called MVSRTN. The model is composed of 3 blocks with skip-connections in-between them: a 1) \"Series-Variable Encoder\", 2) an \"N-Order Residual Tensor Network\", as the authors call it, and 3) the output layer. Out of these, 1) is essentially a 1D CNN combined with (causal and non-causal) self-attention, and 2) seems to be a tensor network built from taking the tensor product of the sequence entries, and then contracting that by a TT-rank-constrained weight tensor. The computation of the tensor network output is formulated as a recursion across time-steps, and motivated by residual networks, an identity mapping with respect to the hidden state at the previous time-stamp is added to the recursive formulation. Then, by the analogy with higher-order solvers for ODEs, a \"higher-order residual connection\" is introduced. The model is applied to four time series forecasting datasets, where some improvements are achieved, and an ablation study is included to show how the various model blocks contribute to the performance.""874\tThe paper presents an overview of the multivariate time series, including the types of dependencies that can be observed in these data, the challenges of forecasting them, and the techniques used to capture and model these dependencies. The paper focuses on two specific examples to illustrate the dependencies in a Multivariate time series: a variable with periodic laws and a variable with dynamic dependencies. The paper provides a detailed analysis of the strengths and weaknesses of different forecasting methods, including both\u4f20\u7edf\u7684 time series forecasting methods and new, more advanced techniques that take into account the dependencies in the data. The paper also discusses the potential applications of multivariate time series data, including in fields such as energy production, traffic management, and financial markets.\tThis paper proposed the MVSRTN architecture for multivariate time series modeling. The MVSRTN consists of an encoder to extract latent variables and residual tensor network (TN) blocks to capture the interactions in the latent space.  The main contribution of the model is the TN block part. In particular, the authors used tensor-products to fully represent the latent variable space (Eq 2). Then, they proposed an N-order residual TN block to alleviate potential gradient problems of high-order TNs in long-term time series. They conducted experiments on four multivariate time series data for prediction. Moreover, an ablation study shows the effectiveness of the proposed residual TN blocks.  Combining ResNet and TN is an interesting direction. However, I think this paper did not present this problem sufficiently and some notations are confusing. ""875\tThis paper discusses the importance of learning the laws and dynamic interaction information between variables in multivariate time series for accurate forecasting. It provides case studies of two different types of dependency relationships in these time series, including one with periodic laws and another with dynamic ones. The paper highlights the benefits of using machine learning techniques to identify and model these dependencies, which can help improve forecast accuracy and reduce the need for manual intervention. The paper also discusses the challenges and limitations of learning these dependencies, such as the need for a large amount of data and the potential for overfitting.\tThe paper considers the problem of forecasting of multivariate time series. The authors propose an architecture for such forecasting that incorporates several layers, including a residual tensor network layer. The idea is to tensorize (via outer products/Kronecker products) the features passed on from the encoder layer and then handle these very large tensorized features with a tensor network. The point of the tensorization is to better incorporate the effect of combinations of variables from different time steps. Experiments are done on four benchmark datasets.""876\tThis paper provides an overview of Graph Neural Networks (GNNs), a machine learning model for graph-structured data. GNNs learn an embedding of nodes by recursively aggregating the features of their neighbors, which enables them to perform downstream tasks such as node classification and link prediction. However, training GNNs on large graphs is computationally challenging due to the recursive neighbor aggregation. To alleviate this burden, various neighbor sampling methods have been proposed to estimate the aggregation results in each layer of GNNs. These sampling techniques enable GNN training on large graphs, but they also introduce biases into the learning process. This paper discusses the challenges and opportunities of training GNNs on large graphs and proposes some solutions.\tThis paper formulated the graph neural network as a stochastic compositional optimization problem and then developed a new optimization algorithm to train GNN. However, there are some errors. It is not ready for publication.""877\tThis paper discusses the use of graph neural networks (GNNs) for machine learning tasks on graph-structured data. GNNs learn embeddings of nodes by recursively aggregating features of their neighbors, which is a challenging task due to the recursive neighbor aggregation. To alleviate this computational burden, various neighbor sampling methods have been proposed to enable training of GNNs on large graphs. These sampling techniques provide an unbiased estimation of the aggregation result in each layer, which helps to improve the performance of GNNs. However, the composition of the aggregation function can still affect the performance of GNNs.\tThis paper proposed an improved variant of the Stochastic Compositional Optimization (SCO) framework to train GNNs, replacing all nodes' moving averages with a sparse representation. The proposed algorithm only requires a fixed-size buffer, regardless of the graph size, solving the memory issue of SCO algorithms and making it practically applicable to large graphs. The paper showed that the proposed algorithm preserves the convergence rate of the original SCO algorithm and experimentally validated that the algorithm could outperform the traditional Adam SGD for GNN training with a small memory overhead.""878\tThis paper discusses the use of graph neural networks (GNNs) for machine learning tasks on graph-structured data. GNNs learn an embedding of the nodes by recursively aggregating the features of neighboring nodes. However, this process is computationally challenging on large graphs, so various neighbor sampling methods have been proposed to alleviate the computation burden. These methods sample a subset of neighbors to compute an unbiased estimation of the aggregation result in each layer. These sampling techniques enable GNN training on large graphs, but they also introduce some biases into the embeddings.\tThis paper studies neighbor sampling techniques for training GNNs. Previous work has observed that sampling-based GNN training can be formulated as a Stochastic Compositional Optimization (SCO) problem. The authors argue that naive implementation of existing SCO algorithms incurs huge memory cost for training GNNs. This paper propose a Sparse Stochastic Compositional (SpSC) gradient method, which only stores the data for nodes sampled in the past few iterations. Convergence analysis on SpSC is provided and empirical results show that SpSC has better performance than naive SCO. ""879\tThis paper discusses the design and performance of Graph Neural Networks (GNNs), a type of neural network that is commonly used for tasks such as node classification and link prediction on graph-structured data. GNNs learn an embedding of the nodes by recursively aggregating the features of their neighbors, and they are capable of using this embedding for downstream tasks such as node classification and link prediction. However, training GNNs on large graphs is computational challenging due to the recursive neighbor aggregation, which requires the use of various neighbor sampling methods to alleviate the burden. These sampling techniques enable GNN training on large graphs, but they also introduce a bias into the resulting embeddings. This paper provides an overview of the different neighbor sampling methods and their performance on different graph datasets. Additionally, it discusses the challenges and future directions of GNN research.\tThis paper studies the problem of training GNNs on large-scale graphs. Specifically, it proposes to use sparse moving average for sampling-based GNN training strategies. The convergence rate of the proposed approach has been proved under several assumptions. The authors conduct experiments on several large-scale datasets to show the effectiveness and efficiency of the proposal.""880\tThis paper discusses the use of the Jaccard similarity metric in machine learning and web search applications. It provides an overview of the Jaccard similarity, its definition, and its applications. It also discusses the well-known method of \"minwise hashing\" (MinHash) for computing the Jaccard similarity in massive binary datasets. The paper\u6700\u540e\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u53ef\u80fd\u7684\u6311\u6218\u3002\tMin-wise hashing (MinHash) is a fundamental and popular algorithm in machine learning. This paper proposes Circulant MinHash (C-MinHash) to approximate the Jaccard similarity in massive binary data. Compared with MinHash, C-MinHash only requires two (or maybe one in practice) random permutations in a circulant manner for approximation. The authors also systematically demonstrate that the C-MinHash can provide a smaller estimation variance than MinHash. Extensive experiments validate the effectiveness of C-MinHash.   ""881\tThis paper discusses a commonly used similarity metric, the Jaccard similarity, and its application in machine learning and web search. The Jaccard similarity is defined as the ratio of the size of the intersection over the size of the union of two binary vectors v and w. The paper\u63a2\u8ba8\u4e86\u5982\u4f55\u4f7f\u7528\u201cminwise hashing\u201d(MinHash)\u6280\u672f\u6765\u8ba1\u7b97 Jaccard similarity\uff0c\u5e76\u5728\u8bb8\u591a\u5e94\u7528\u9886\u57df\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u5305\u62ec\u8fd1\u90bb\u641c\u7d22\u3001 duplicate detection\u3001 malware detection\u3001Web\u641c\u7d22\u3001Clustering\u3001\u5927\u578b\u673a\u5668\u5b66\u4e60\u3001\u793e\u4f1a\u7f51\u7edc\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86 Jaccard similarity\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e9b\u6311\u6218\u548c\u6539\u8fdb\u65b9\u5411\u3002\tThe paper designs an improved version of the classical MinHash data structure for calculating the Jaccard similarity between two binary strings.  The classical MinHash data structure generates K hash values for a binary string by generating K independent random permutations of the binary string and taking the index of the first \u201c1\u201d in the permuted string as the hash value in each random permutation. The estimator is then the percentage of equal hash values out of K hash values for the two binary strings. This is an unbiased estimator.  This paper\u2019s algorithm, called C-MinHash, first applies a random permutation to the binary string. For the K hash values, instead of using K independent random permutations, the new algorithm uses the same permutation but shifted by 1, 2, \u2026, K positions. The estimator is the same as that of the MinHash and can be easily shown to be unbiased. The paper then shows that this simple scheme, with only two independent random permutations, yields a smaller variance than that of MinHash. ""882\tThis paper discusses a commonly used similarity metric, the Jaccard similarity, and its application in machine learning and web search. The Jaccard similarity is defined as the ratio of the size of the intersection over the size of the union of two binary vectors. The paper\u8ba8\u8bba\u4e86\u5982\u4f55\u4f7f\u7528\u201cminwise hashing\u201d(MinHash)\u6280\u672f\u6765\u8ba1\u7b97 Jaccard similarity\uff0c\u8be5\u6280\u672f\u5728\u5927\u89c4\u6a21\u4e8c\u8fdb\u5236\u6570\u636e\u96c6\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5176\u5728\u5404\u79cd\u5e94\u7528\u9886\u57df\u7684\u5b9e\u9645\u6548\u679c\u3002\u8fd8\u8ba8\u8bba\u4e86 minwise hashing \u6280\u672f\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u65b9\u5411\u3002\tThis paper proposes C-MINHASH to improve vanilla MINHASH. Instead of using K random permutations to generate K hash values, C-MINHASH requires only two permutations. Theoretically, C-MINHASH provides unbiased estimate, and its variance is smaller than MINHASH. Extensive empirical experiments verify the theoretical analysis. ""883\tThis paper discusses the use of the Jaccard similarity metric in machine learning and web search applications. It explains the basic concept of the Jaccard similarity and its use in various applications, including near neighbor search, duplicate detection, malware detection, web search, clustering, and large-scale learning. The paper also discusses the well-known method of \"minwise hashing\" (MinHash), which is a technique for computing the Jaccard similarity in massive binary datasets, and its various applications. The paper concludes by highlighting the advantages and limitations of the Jaccard similarity metric.\tThis paper proposes an effective approach for MinHash by permutating data vectors. It first randomly shuffles the data to break structures exhibited in the original data and then performs permutation K-times to obtain K hash values. Besides, this paper proposes an approach that performs only one permutation to compute hash values. This paper shows the theoretical approximation error of the proposed approach. By using text and image datasets, it shows that experimental results follow the results of the theoretical analysis.""884\tThis paper discusses the problem of adversarial examples, which are small changes to the input that can change the decision of a machine learning model, and how to prevent or mitigate these attacks. The paper highlights the challenges of implementing adversarial defense methods, including the need for provable defense methods, the limitations of simple CNNs, and the recent advances in adversarial training. The paper also provides an overview of the recent progress in improving adversarial training methods.\tThis paper proposes a method to produce image classifiers which are adversarially robust against multiple $\\ell_p$ threat models\u2014in particular, against $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ attacks. The method involves training against $\\ell_1$ and $\\ell_\\infty$ attacks with the hypothesis that this will additionally give robustness for $\\ell_p$ threat models with $1 \\leq p \\leq \\infty$. This hypothesis is supported by prior results that proved that affine classifiers robust to $\\ell_1$ and $\\ell_\\infty$ threat models are also be robust to other $\\ell_p$ threat models. The authors test their method on CIFAR-10 and ImageNet for both training classifiers from scratch and for fine-tuning robust models trained on one $\\ell_p$ threat model to the other $\\ell_p$ threat models.""885\tThis paper discusses the issue of adversarial examples, which are small changes to input data that can change the decision of a machine learning model, and how to overcome them. The paper highlights the difficulties in developing adversarial defense methods, and discusses the only successful technique so far, which is adversarial training. The paper also discusses recent advances in improving adversarial training, such as the use of proven defense methods and various techniques for reducing the computational budget ofdversarial attacks.\tThe paper tackles the problem of robustness against multiple perturbations and proposes extreme norms adversarial training (E-AT) that adaptively alternates between $\\ell_1$ and $\\ell_\\infty$-norm. Furthermore, the paper fine-tunes Gowal et al. (2020) to improve its multi-norm robustness. Finally, the experiments are conducted on CIFAR-10 and ImageNet with APGD for training, showing the proposed method's effectiveness.""886\tThis paper discusses the issue of adversarial examples, which are small changes to the input that can change the decision of a machine learning model, and how to prevent or mitigate these attacks. The paper highlights the challenges of developing adversarial defense methods and explains that most existing methods are either vulnerable to stronger attacks or have limitations in their ability to address adaptive attacks. The paper also discusses the recent advances in adversarial training and their impact on improving the effectiveness of adversarial defense methods. The paper\u6700\u540e provides an overview of the current state of the art in this field and suggests future directions for research.\tThis paper addresses the problem of multiple perturbation adversarial robustness for attacks subsumed within $\\ell_p$ regions for $p\\in{1,2,\\infty}$. The main contribution of this work is to show how a model robust to a particular attack type (typically $\\ell_\\infty$) can be fine-tuned (at low cost) to be robust against multiple (or alternate) perturbation types. The authors build on prior formalization about the geometry of $\\ell_p$ balls (by Croce et. al.) to empirically demonstrate its effect. The results are convincing and evaluated against AutoAttack which is""887\tThis paper discusses the issue of adversarial examples, which can change the decision of a machine learning model, particularly in safety-critical systems. It presents a review of the existing literature on adversarial defense mechanisms and their limitations. The paper also discusses the recent advances in adversarial training, which remain the only successful technique currently available for\u9632\u5fa1 adversarial examples. The paper concludes by highlighting the need for further research in this area to develop more robust and effective adversarial defense mechanisms.\tThis paper mainly studies the problem of defending multiple norm adversarial perturbations. The authors propose extreme norms adversarial training (E-AT), which leverages different geometry of the $\\ell_{p}$-balls to conduct adversarial training by adaptively alternating between the $\\ell_{1}$ norm and $\\ell_{\\infty}$ norm. They also show that using E-AT fine-tune could turn $\\ell_{p}$ robust model into a model that is robust against the union of $\\ell_{p}$ adversarial perturbations. The authors also provide some theoretical proof for their method. ""888\tThis paper discusses the Multi-Task Learning (MTL) algorithm, which aims to improve the generalization performance of multiple learning tasks by sharing knowledge among those tasks. MTL has been proposed in the context of machine learning and has shown promise in improving the performance of various applications. The paper discusses the advantages of MTL, including its ability to reduce training and inference time and improve the performance on some tasks while performing worse on others. It also explains how the negative sharing phenomenon can occur in MTL models and how it can be addressed to improve the overall performance of the system. The paper concludes by providing a summary of the main findings and future directions for MTL research.\tThe paper propose a simple method for safe multi-task learning where there is no negative transfer or \"negative sharing\" among tasks. It jointly trains shared encoder, task-specific (private) encoders, gate, and decoder. The gate computes importance for each output of shared and task specific encoder and combine the outputs with simple convex combination. ""889\tThis paper focuses on Multi-Task Learning (MTL), a recent advance in machine learning that aims to improve the generalization performance of multiple learning tasks by sharing knowledge among them. MTL has shown promise in improving the performance of machine learning models in many applications, but it cannot always guarantee to improve the performance of all tasks. As empirical evidence has shown, some existing MTL models can achieve better performance on some tasks than a single-task model, but perform worse on other tasks. This phenomenon, known as negative sharing, is a significant\u6311\u6218 for MTL and requires further research to address. The paper provides an overview of MTL, its properties, and the negative sharing phenomenon, and proposes some potential solutions to this problem.\tThis paper presents a multi-task learning approach that avoids negative sharing in training deep neural networks.  A novel network architecture is proposed, which consists of a public encoder shared by all the tasks, private encoder for each task, and a gate for each task to combine encoded features from public and private encoders. Their experiment results indicate the proposed approach is effective on image recognition related tasks.""890\tThis paper provides an overview of Multi-Task Learning (MTL), a technique that aims to improve the generalization performance of multiple learning tasks by sharing knowledge among them. MTL has been proposed in the context of machine learning and has shown promise in improving the performance of models in various applications. The paper discusses the advantages of MTL, including reducing the training and inference time, improving the performance on specific tasks, and achieving better generalization performance. The paper also explores the negative sharing phenomenon, which is observed in some MTL models and can lead to better performance on some tasks but worse performance on other tasks. The paper\u6700\u540e provides a conclusion and future directions for MTL.\tThis paper focuses on the negative sharing problem in multi-task learning, which has not been studied sufficiently in existing work. The authors propose the Safe Multi-Task Learning (SMTL) model and several of its variants to avoid negative sharing and achieve safe multi-task learning. Both theoretical analysis and comprehensive experimental results are provided to demonstrate the effectiveness of the proposed method.""891\tThis paper discusses Multi-Task Learning (MTL), a technique that aims to improve the generalization performance of multiple learning tasks by sharing knowledge among them. MTL has been shown to be useful in many applications, not only improving the performance but also reducing the training and inference time. However, MTL cannot guarantee to improve the performance of all tasks compared to single-task learning, and an empirical phenomenon known as the negative sharing phenomenon has been observed. This paper defines the negative sharing phenomenon and presents some possible explanations for it. Finally, the paper discusses future directions for MTL and challenges that need to be addressed.\tThis paper aims to solve the negative sharing problem, which is defined as that a multi-task learning model has inferior performance than single-task learning on some tasks, for multi-task learning. To address negative sharing, Safe Multi-Task Learning (SMTL) model is proposed. The model combines hard-sharing model and single-task learning together and is expected to achieve performance that not inferior than single-task learning. ""892\tThis paper discusses energy-based models of sequences, which are often parametrized as neural models. The paper\u5148\u4ecb\u7ecd\u4e86 EBMs \u7684\u6982\u5ff5\uff0c\u5373\u5bf9\u4e8e\u4efb\u4f55\u7ed9\u5b9a\u7684\u5b57\u7b26\u4e32 x\uff0c\u6709 weight ? \u0303 ( x ) \u5b9a\u4e49\u4e00\u4e2a\u6743\u91cd\u5206\u5e03\u3002EBMs \u4e0d\u9700\u8981\u8868\u793a\u6982\u7387\u5206\u5e03\uff0c\u4f46\u5b83\u4eec\u88ab\u63d0\u51fa\u7528\u6765\u5bf9\u6297\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\u7684expressivity\u3002EBMs \u7684 partition function \u8868\u793a\u6bcf\u4e2a\u5355\u8bcd\u7684\u6743\u91cd\u5206\u5e03\uff0c\u5e76\u5b9a\u4e49\u4e86\u53ef\u4ee5 queries \u4ee5\u83b7\u53d6\u6982\u7387\u5206\u5e03\u7684\u53c2\u6570\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u6587\u7ae0\u4ecb\u7ecd\u4e86 EBMs \u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7 neural networks \u5bf9 EBMs \u8fdb\u884cparametrization\u3002\u6700\u540e\uff0c\u6587\u7ae0\u603b\u7ed3\u4e86 EBMs \u7684\u4f18\u70b9\u548c\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76 EBMs \u7684\u5c55\u671b\u3002\tThe paper works in a computational model where various architectures of energy-based-models are viewed as computational models for accepting languages. EBM's assign a weight to each string. The sum of weights is called the partition function and this needs to be computed (or at best approximated) if the energy is to be thought of as a probability measure. The paper shows that expressive EBM's are turning complete, and uses that to show that the partition function may be uncomputable. And that is even in the case the energy of a sequence could be computed in poly-time. The paper then shows some corollaries and variations of this result, showing that model selection is undecidable as well. The paper concludes by suggesting scenarios where the partition function is computable but this naturally comes at the expense of the expressivity of the model.""893\tThis paper discusses energy-based models of sequences, which are often parametrized as neural networks. These models are used to model discrete sequences, such as words in a language, and can be used in a variety of applications, including natural language processing and bioinformatics. The paper provides an overview of the basic principles of EBMs and discusses how they are often parametrized as neural networks. It also discusses the use of EBMs in probabilistic inference, which allows for querying of the distribution of a sequence under the principle of probabilistic inference. The paper\u6700\u540e discusses some potential limitations of EBMs and\u63d0\u51fa\u4e86 some possible future directions for research in this area.\tThis theoretical work highlights uncomputability issues arising with energy-based sequence models. It is shown that an EC-complete family (a certain computational model capturing neural networks and transformers that is essentially equivalent to weighted Turing machines) cannot approximate the partition function of energy-based sequence models (EBMs) even given an unlimited amount of time and space. A consequence is the impossibility of model selection for EBMs. This paper also rules out popular estimators such as rejection and important sampling. This work concludes with a discussion of restricted EBMs that avoid such uncomputability issues. ""894\tThis paper discusses energy-based models of sequences, which are often used in natural language processing and bioinformatics to model discrete events. The paper provides an overview of the basic concept of EBMs and discusses their use in modeling sequence data. It also notes that EBMs do not require probability distributions and can be used to counter the inexpressivity of autoregressive sequence models. The paper then focuses on the parametrization of EBMs as neural networks and discusses the advantages and limitations of this approach. Finally, the paper provides an example of how an EBM can be used to model a sequence of words and how this can be used for text classification or sentiment analysis.\tThis paper studies \"efficiently computable (EC) energy-based sequence models (EBMs)\" which are simply sets of strings equipped with nonnegative weights that can be computed by a poly-time Turing machine (upon normalization, the weights induce a probability distribution over the strings). In practice, it is common for these weights to be computed by neural networks. In fact, certain neural sequence model families like RNNs and Transformers are expressive enough to be Turing-complete. In this paper, the authors find that the expressivity of such sequence model families, so-called \"EC-complete parametric families\", comes at a significant cost in terms of *computability/decidability* of various primitives for inference. For instance, they show that if one could actually take in any vector of parameters specifying a model in such a family and output the corresponding partition function (sum of the weights of all strings) even approximately in expectation, then would be able to decide the halting problem (Theorem 2, 4). They also exhibit a single efficiently computable EBM for which proving (within ZFC) that the partition function is one of two possible values would either disprove Godel's second incompleteness theorem or disprove consistency of ZFC (Theorem 3); this can be extended to show that *asymptotic* estimates like rejection sampling and importance sampling will also fail (Theorems 5, 6). Along similar lines, they can reduce from the halting problem to other tasks like deciding whether two given parameter vectors give rise to the same EBM (\"parameter identifiability\", Theorem 7), or deciding which of two given parameter vectors gives rise to an EBM which is distributionally closer to some other given EBM (\"model selection\", Theorem 8).  In terms of techniques, the reductions from the halting problem are all based on a simple weight function that is tiny unless if the string corresponds to a valid accepting trace of an input-free Turing machine M. The point is that the weight for any string x can be computed by M simply by simulating M on x, but the partition function is large iff M halts. The ZFC results follow by taking M which enumerates all provable propositions under ZF and halts iff it proves 1 = 0.""895\tThis paper provides an overview of energy-based models, which are used to model discrete sequences, including EBMs and neural networks. The paper highlights the importance of EBMs in natural language processing and bioinformatics, as well as their potential to counter the inexpressivity of autoregressive sequence models. The paper also discusses the\u5982\u4f55parametrization EBMs and the principles of probabilistic inference that are used to query these models.\tThe paper studies the trade-off between expressiveness and computability (of the partition function) for energy-based sequence models. The theoretical results show that the high expressively of unrestricted energy based models comes at the cost of un-computability and in-approximability of the partition function. These negative results further show that rejection and importance sampling are not a panacea either.""896\tThis paper discusses the Wasserstein distance, a distance measure used in machine learning to compare probability distributions. The Wasserstein distance is a Finsler distance that is related to the optimal transport theory. It is a popular choice in machine learning because of its favorable properties, such as robustness to disjoint supports and numerical stability. However, it has high computational complexity and the result of an optimization problem, so it is not always possible to calculate the Wasserstein distance with ease. Additionally, the Wasserstein distance is not always the best choice for comparing two distributions, as it may not provide as good a measure of agreement as other distances.\tThe paper proposes a new slice-based approach to efficiently compute the Wasserstein distance between two distributions $\\nu$ and $\\mu$. The method termed ASWD (augmented sliced Wasserstein Distance) first  projects the samples from $\\nu$ and $\\mu$ onto a higher dimensional space using a non linear injective mapping function and then uses the classical random linear projections onto 1D to compute the sliced Wasserstein Distance. Overall, the procedure amounts to applied a spatial Radon Transform to perform the slicing. Theoretical results establish conditions  under which ASWD is a metric. A numerical algorithm is given along with the design of the injective mapping using NN. Empirical evaluations on simulation datasets and on generative modeling highlight the potential of the proposed method over existing approaches.""897\tThis paper discusses the Wasserstein distance, a distance measure between probability distributions that is commonly used in machine learning and statistics. The paper begins by\u4ecb\u7ecd optimal transport theory, which provides a powerful and flexible tool for comparing degenerative distributions. The Wasserstein distance arises from the optimal transport theory and is a popular choice in various machine learning domains, including generative models and transfer learning. The paper\u8ba8\u8bba\u4e86 Wasserstein distance\u7684\u4e00\u4e9b\u4f18\u70b9\uff0c\u5982 robustness to disjoint supports and numerical stability\uff0c\u4f46\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5b83\u7684\u7f3a\u70b9\uff0c\u5982 computational complexity and the fact that it is the result of an optimization problem, which may not be as easy to solve as one would like. The paper also discusses some recent advances in using the Wasserstein distance in machine learning, such as its use in transfer learning and the application to image recognition.\tThis manuscript introduces the concept of augmented sliced Wasserstein distances. The main idea is to extend the sliced Wasserstein distance based on mapping samples to higher -dimensional hypersurfaces. The proposed distance is shown to be a metric. Moreover, given that the optimal choice of the nonlinear maps is rather computationally intensive to obtain, an approximation based on neural networks is proposed. Several experiments are shown where a better performance is obtained with respect to existing methods.""898\tThe paper presents an introduction to the Wasserstein distance, a commonly used machine learning tool for comparing two probability distributions. The Wasserstein distance is a metric that measures the difference between two probability distributions and arises from the optimal transport theory. The paper discusses some of the key properties of the Wasserstein distance, including its robustness to disjoint supports and numerical stability. It also discusses the computational complexity of computing the Wasserstein distance and the non-existence of a simple algorithm for finding the minimum of the Wasserstein distance between two probability distributions. Finally, the paper provides examples of applications of the Wasserstein distance in machine learning, including generative models and transfer learning.\tThis paper introduces the augmented sliced Wasserstein distance (ASWD), a new variety of sliced Wasserstein distance (SWD), that allows comparing two probability distributions by combining a nonlinear embedding of the sample data points to a higher-dimensional space with a slicing scheme to calculate 1D Wasserstein distances between uniformly projection directions. The authors introduce the spatial Radon transform, which includes the standard Radon transform and the special case of polynomial generalized Random transform (introduced in Kolouri et al. 2019). They further prove that ASWD is a valid metric if and only if the mapping is injective. Several experiments are conducted on generative modelling (CIFAR10, CelebA,  MNIST, color transferring).""899\tThe paper discusses the Wasserstein distance, a measure of the difference between two probability distributions, and its use in machine learning. It notes that the optimal transport theory provides a powerful and flexible tool for comparing degenerative distributions, and the Wasserstein distance is a result of this theory. The paper also discusses the computational complexity of the Wasserstein distance, its favorable properties, and its limitations. Finally, the paper provides an example of how the Wasserstein distance can be used in machine learning, such as in generative models and transfer learning.\tThis paper proposes a variant of sliced Wasserstein distance, named augmented sliced Wasserstein distance. ASWD maps input data points to hypersurfaces using neural networks, then calculates SWD on the hypersurfaces. ASWD alleviates the low efficiency problem of SWD for high-dimensional data. Various tasks including flow, generative modeling, and barycenters show the advantage of AWSD against some existing methods.""900\tThis paper discusses the use of Cooperative Multi-agent Reinforcement Learning (MARL) for solving complex tasks in multi-agent systems. MARL is a powerful tool that enables autonomous agents to learn coordinated joint actions in order to achieve optimal outcomes. However, in many MAS, the reward signal provided by the environment is not sufficient to guide the agents towards coordinatedBehaviour, resulting in inefficient exploration. This problem is further\u6076\u5316 by the fact that MAS can have many stable points, some of which lead to arbitrarily bad outcomes. As in single agent RL, inefficient exploration can significantly reduce the number of possible outcomes, making it difficult to achieve the desired goal. The paper presents a number of technical challenges that need to be overcome in order to effectively use MARL for multi-agent systems, and proposes some potential solutions.\tThis paper introduces a learned centralized exploration reward for multi-agent settings. The exploration reward is factorized into an on/off gate (dubbed \u2018switching control\u2019) and a scale function. Some mathematical derivations are included (sketched in the main text, with details in the appendices) to provide theoretical guarantees on how the exploration reward changes the solutions the training procedure might find. Evaluations on gridworld environments that target specific difficulties of multi-agent exploration show promising results. Some maps from the SMAC benchmark are also included, and again show good results.""901\tThis paper discusses the use of Cooperative Multi-Agent Reinforcement Learning (MARL) to solve various tasks in multi-agent systems. MARL is a powerful tool that allows autonomous agents to coordinate during exploration and learn coordinated joint actions in order to achieve optimal outcomes. However, in many multi-agent systems, the reward signal provided by the environment is not sufficient to guide the agents towards coordinatedBehaviour, relying on solely individual rewards may not lead to optimal outcomes, and inefficient exploration can significantly reduce the performance of the system. This problem is worsened by the fact that MAS can have many stable points, some of which may lead to arbitrarily bad outcomes. The paper also discusses the challenges of implementing MARL in real-world systems and suggests some potential solutions.\tThis paper proposes a method, Learnable Intrinsic-reward Generation Selection (LIGS) to improve coordinated exploration. LIGS incorporates an extra agent, called Generator to learn what state to give what intrinsic reward for each agent. The intrinsic reward is potential-based, so it preserves the optimality. Experimental results on several domains show its advantages over several MARL methods. ""902\tThis paper explores the use of cooperative multi-agent reinforcement learning (MARL) for autonomous agents to solve various tasks such as ride-sharing and swarm robotics. MARL allows multiple autonomous agents to communicate and coordinate their actions in order to achieve optimal outcomes. However, in many multi-agent systems, the reward signal provided by the environment is not sufficient to guide the agents towards coordinatedBehaviour, resulting in inefficient exploration. This problem is worsen by the fact that MAS can have many stable points, where the agents may receive arbitrarily bad outcomes. As in single agent RL, efficient exploration can contribute to the success of MARL. The paper presents a review of the literature on MARL, highlighting the key challenges and recent advances in the field. It also discusses potential future directions and applications of MARL.\tThe paper describes a novel reinforcement learning algorithm for multi-agent system (MARL) that employs a generator of intrinsic reward and a switching control system that helps to regulate intrinsic control. Crucially, the intrinsic reward is learned to better fit the particular task being learned. The paper claims that the proposed algorithm helps with exploration as well as preservation of known policies. The paper has a strong theoretical background with a section that illustrates the properties of convergence and optimality. The experimental results appear to justify the approach with superior performance with respect to the baselines. The paper deals with an emerging and interesting area of RL and proposes a new mechanism for co-ordinated RL agents.  ""903\tThis paper discusses the use of cooperative multi-agent reinforcement learning (MARL) to solve various tasks, such as ride-sharing and swarm robotics. MARL allows autonomous agents to learn coordinated actions during exploration in a multi-agent system, but the reward signal provided by the environment may not be sufficient to guide the agents towards coordinatedBehaviour. This problem is exacerbated by the fact that MAS can have many stable points, which can lead to arbitrarily bad outcomes. Inefficient exploration can cause the system to miss out on valuable information and lead to poor performance. The paper presents a number of studies that have used MARL to solve complex problems in multi-agent systems, and discusses some of the challenges and limitations of the technique.\tThe paper focuses on learning intrinsic rewards for multi-agent reinforcement learning, which is an important problem. Different from previous works on this topic, the authors propose to train an agent with a learnable gating function that incentives other agents. Theoretical analysis and empirical evaluation are provided to prove the effectiveness of the proposed method.""904\tThis paper discusses the problem of answering complex questions over long structured documents. The document typically contains coherent information on a certain topic, and the contents are grouped by sections or other structures. To answer complex, multi-hop questions over the documents, one requires navigate through different parts of the documents to find different pieces of information relevant to a question. This navigation requires understanding high-level information about the structure of the document, such as the section titles and subsections. The paper provides an example of how this can be done by reading the document with the relevant modules in mind and attending to specific subsections. The paper also discusses potential solutions to the problem, such as using machine learning techniques to identify relevant information from the document structure.\tThe paper proposes a simple attention-based model for conversational and multi-hop QA tasks. The model use BERT-like pre-trained LM ETC separately encodes questions and paragraph (i.e., a collection of sentences). Besides the encodings on sentence-level, the final context encodings also contain extra paragraph embeddings, which are a weighted sum of sentences\u2019 encodings using a simple dot product attention. For the QA interaction, the models use a hard attention mechanism to select an entry representing either a sentence or a paragraph.  The experiments on two extractive QA datasets HYBRIDQA and QASPER show the model performs worse than the MATE model on HYBRIDQA but marginally better than other baselines on QASPER. On multi-hop QA and conversational QA tasks, the model performs marginally better than baselines on *expanded dataset*, but authors do not provide results on original datasets. ""905\tThis paper discusses the problem of answering complex questions over long structured documents. The long document typically contains coherent information on a certain topic, and the contents are grouped by sections or other structures. To answer complex, multi-hop questions over the document, one needs to navigate through different parts of the document to find different pieces of information relevant to a question. This navigation requires understanding high-level information about the structure of the document, such as the section titles and subsections. As in academic papers, the paper provides an example of how one might approach this problem by turning to the section titles and subsections to identify the relevant information and then reading the document in detail with this information in mind.\tThis paper introduces DocHopper, a new model for complex question answering over long documents (e.g., multi-hop QA over multiple paragraphs, conversational QA, reasoning over scientific documents). DocHopper is based on ETC (Ainslie et al., 2020) and DocHopper extends the existing hierarchical attentions from ETC with a new approach to update query representations in latent space. Their model does not jointly encode a question and context and does not require re-encoding of queries as in prior work, which leads to their effectiveness at inference time. They evaluate DocHopper on four different datasets: ShARC, QASPER, HotpotQA, and HybridQA. The proposed method achieves strong performance on those datasets, reducing the computational cost at the inference time. ""906\tThis paper focuses on the problem of answering complex questions over long structured documents. Long documents typically contain coherent information on a certain topic, and the contents are grouped by sections or other structures. To answer complex, multi-hop questions over long documents often requires navigate through different parts of the documents to find different pieces of information relevant to a question. This navigation requires understanding high-level information about the structure of the document, such as the section titles and thesubsections. To address this problem, the paper proposes a new method that uses a combination of document structure information and text analysis to navigate through the documents and find relevant information for complex questions. The paper also presents an example of how this method can be applied to answer a complex question about a specific software application. The paper provides insights into the challenges and potential solutions for this problem, and discusses the potential applications of this method in other areas.\tThe paper proposes an iterative approach for multi-hop question answering. At high-level the proposed model breaks a question into multiple sub-questions and then adds information relevant to each sub-question to the query vector for the next step retrieval. At each iteration an ETC encoder is used to encode the document and a sub-question; the vector corresponding to sub-question is then updated iteratively by contextualizing it over sentences and paragraphs in the document and is used to extract the final answer using a subsequent BERT reader. Evaluation results show improvements on 3 of 4 datasets. ""907\tThis paper discusses the problem of answering complex questions over long structured documents. The documents typically contain coherent information on a certain topic, and the contents are grouped by sections or other structures. To answer these questions, one needs to navigate through different parts of the documents to find different pieces of information relevant to a question. This requires understanding high-level information about the structure of the document, such as the section titles and the subsections (such as the ones titled \u201c Implementation details\u201d) that contain the relevant information. The paper presents an approach to\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c which involves using machine learning techniques to analyze the structure of the document and generate a summary of the information that is relevant to the question. The approach is shown to be effective in a real-world example problem, where a user\u63d0\u95ee\u4e86\u4e00\u4e2a\u5173\u4e8eDOCHOPPER\u7684\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u6587\u6863\u4e2d\u7684\u4e0d\u540c\u90e8\u5206\u6765\u67e5\u627e\u76f8\u5173\u4fe1\u606f\u3002 The paper also discusses the challenges and limitations of this problem, such as the fact that the structure of the document may change over time and the need for additional information to be added to the document to answer the question.\tThis paper provides a novel MRC model (DocHopper) for multi-hop QA over long structured documents. In multi-hop QA, the evidence necessary to answer a user's question is spread across different parts of the long document. Previous approaches find the evidence by iteratively updating the user's query. The problems in these previous approaches are 1) computational efficiency and 2) ineffective modeling strategy for figuring out the relations of the evidence. DocHopper resolves these problems with a hierarchical attention mechanism. Hierarchical attention mechanism provides two types of embedding vectors for a single paragraph: 1) local sentence vectors and 2) global context vector of the paragraph. DocHopper computes the similarities between the query vectors and these sentence/paragraph vectors and selects the proper evidence. Since the sentence/paragraph vectors can be pre-computed, the only inference time required for this method is the time for question embedding, and this brings drastic improvement in the computational efficiency. This paper uses four types of datasets for evaluation: 1) conversational QA (ShARC), 2) TableQA (HybridQA), 3) QA on academic paper (QASPER), and 4) multi-hop factual QA (HotpotQA), and shows the QA performance and computational efficiency of their model.""908\tThe paper discusses the process ofLocalization of a multimedia work, including the options for translating it into a target language. It explains that companies need to translate their work into the target language, and how subtitling and dubbing compare in terms of their costs and practicalities. The paper also touches on the voice selection process for choosing the best voice for the target language. The paper\u6700\u540e offers an overview of the challenges and opportunities involved inLocalization, including the need for cultural accuracy and the need to adapt to different audiences.\tThe paper proposes a label refinement approach. Starting from an initial set of labels, k-means clustering is used to refine the labels. The approach is described in the context of dubbing/voice casting. The method tries to obtain voice characteristics which can be further used for dubbing/voice-casting. Experiments are shown on video game datasets MassEffect and Skyrim and they primarily investigate how different parameters of k-means (# of clusters) and distance measure affect the label refinement. ""909\tThe paper discusses the process ofLocalization of a multimedia work, including the options for translation and the voice selection process for retaining audience immersion. It explains that companies need to translate their works into a target language to market them in a target country, and that various options are available for giving this translation substance. The paper\u8ba8\u8bbas subtitling and dubbing as\u4e24\u79cd\u5e38\u7528\u7684\u7ffb\u8bd1\u65b9\u5f0f\uff0c\u5206\u522b\u4f18\u7f3a\u70b9\uff0c\u5e76\u4ecb\u7ecd\u4e86 voice selection process \uff0c\u5373\u9009\u62e9\u6700\u9002\u5408\u76ee\u6807\u89c2\u4f17\u7684\u58f0\u97f3\u6f14\u5458\u6765\u66ff\u6362\u539f\u59cb\u89d2\u8272\u7684\u914d\u97f3\u3002\tThe paper tackles a voice similarity system task for voice casting problem. The authors trained voice embedding network using voice character label and cluster the embedding features and used these clusters to train final voice embedding network. The introduction is well-written, however, the proposed method is known or marginal improvements from the existing technical skill in machine learning community (pseudo labeling with embedding feature clustering).""910\tThe paper provides an overview of the process ofLocalization of a multimedia work, including the translation into a target language, the options for giving the translation substance, and the voice selection process for retaining audience immersion. The paper\u8ba8\u8bbas the challenges ofLocalization, including the need to consider target audience preferences and the costs and time required to set up the translation. The paper also discusses the benefits ofLocalization, including the ability to market the product to a wider audience and the retention of audience immersion.\tThis paper addresses the task of finding similar-sound voices with application to voice-dubbing (e.g. finding an actor to record dialog in English, translated from original French dialog, such that the English speaker sounds similar to the original French speaker). The paper proposes a method called \"label refining\". This method is based on p-vectors, a prior representation found to model similarity between characters. The method uses 1) non-expert \"initial labels\" to train a p-vector system 2) use k-means to cluster the resulting p-vectors and 3) use the groups learned by k-means to re-group the labels (the \"refined labels\"). To evaluate the method, English-French pairs of voice data from the video games Mass-Effect 3 and Skyrim are used. Performance is measured using clustering measures v-measure and purity-K, as well as accuracy on the test set (which it seems the ground-truth labels are mapping to the correct dubbing speaker). The method achieves a higher accuracy of 0.70 over a state-of-the-art system that achieves 0.69. A second experiment is performed using Skyrim data as a subsidiary corpus used to cluster, with the goal of \"bringing out vocal characteristics\" from the initial labels, which shifts the optimal K towards 2, which the paper claims is because the new representations start to model gender.""911\tThe paper discusses the process ofLocalization of a multimedia work, such as a video game or a motion picture. Localization involves making changes and adaptations to the original work in order to make it available to international distributors in a target country. The companies that produce the work need to translate it into the target language, which can be done using various options, includingsubtitling and dubbing. Subtitling is the simplest and cheapest option, but not the most practical for spectators, as they are more comfortable reading subtitles while hearing speech in another language. Dubbing, which is more expensive and takes longer to set up, better retained the audience's immersion by replacing the original voices with dubbing comedian voices. The voice selection process involved in the selection of the voice for the target language is also discussed.\tThe paper presents a \"label-refining\" technique, which helps users pick voices to provide a better user experience in dubbed video games. The idea is that a voice talent's voice in the target language should match the character's voice characteristics in the source language. The method seems to work by attaching labels to data-driven clusters, and refining these using a second corpus, on which the labels' value for discrimination is measured.""912\tThis paper provides an overview of the state-of-the-art in deep learning for computer vision, particularly convolutional neural networks (CNNs). It highlights the significant performance and inference speed benefits of CNNs in various tasks such as denoising, deraining, deblurring, and deblocking. The paper also discusses the challenges associated with collecting data, such as privacy and regulation issues, and proposes a framework for leverageng multiple sources of data to improve the performance of CNN-based methods. Finally, the paper concludes with a summary of the key findings and future directions.\tThis work is aimed at distributed \"privacy preserving\" training of neural networks for image processing tasks like deblocking, denoising, deraining, and deblurring. \"One of the most important contribution\" [pg 2] is breaking down the neural network model into task-specific convolutional head and tails (trained on \"clients\"), and a common shared (across tasks) Transformer based feature backbone, which is trained on the server. The heads/tails and the transformer backbone are trained in an alternate manner by assuming the other model to be fixed.    The proposed is similar to the method \"Splitfed\" (Thapa et al., 2020) but is extended for different tasks (as described above).  Experimental results demonstrate:  (i) successful training of the neural network models with the proposed method. (ii) better/comparable performance to prior works on distributed/privacy-preserving methods. (iii) better performance using the Vit backbone as compared to CNN backbones, and also with the proposed multi-task vs. single-task setting.""913\tThis paper discusses the use of convolutional neural networks (CNNs) in computer vision tasks, specifically in the area of denoising, deraining, deblurring, and deblocking. The paper highlights the advantages of CNNs in terms of their ability to learn complex image features and their fast inference time. It also discusses the challenges associated with collecting data for these tasks, including privacy and regulation issues and the need for a framework that can leverage multiple sources of data to improve the performance of CNNs. The paper\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848 for these challenges and suggests future directions for the use of CNNs in computer vision.\tIn this work, the authors present a multi-task distributed learning framework called TAViT. The task-specific head CNN and the tail CNN are distributed to clients with their data connected to a standard Transformer body placed in the server. With an alternating training scheme, the heads and tails on client sides are trained by task-specific learning, while the body is trained by task-agnostic learning. Experiments on four different image processing tasks show the success of task-agnostic learning of the Transformer body and its synergistic improvement with the task-specific heads and tails.""914\tThis paper discusses the use of convolutional neural networks (CNNs) for computer vision tasks and their applications in denoising, deraining, deblurring, and deblocking. The paper also presents a deep learning framework that leverages multiple sources of data to improve the performance of CNNs. The performance of CNNs is typically limited by the amount of training data, so this paper proposes a technique for improving the dataset by collecting data from multiple sources. The framework is called Multi-source Deep Learning (MSD) and is designed to support the training of CNNs with large datasets. The paper also discusses the challenges and limitations of MSD, such as privacy concerns and the need for further research in the area.\tThe paper presents an architecture for image processing tasks that splits up a network into three subsequent parts: head, body, and tail. Head and tail parts are CNN-based and can be trained on multiple client devices using federated learning (FedAvg), while the body part of the architecture is transformer-based and is trained on a central server. Head and tail parts are trained for specific tasks, while the body part is trained in a task-agnostic manner by selecting clients from each task for loss optimization. Experimental results show benchmark and convergence results that are comparable or favorable to non-distributed models, as well as comparison results to purely FL and SL approaches with a very small nr of clients.""915\tThis paper discusses the state-of-the-art in deep learning for computer vision, specifically convolutional neural networks (CNNs). It provides an overview of the history of CNNs, their use in computer vision, and their applications. The paper also discusses the performance and limitations of CNNs in terms of training data and scalability. The paper also presents a new deep learning framework called LeNet, which can leverage multiple sources of data to improve its performance. The paper concludes by discussing the future directions and potential applications of LeNet.\tThis paper presents a new distributed learning framework exploiting the vision transformer for various image processing applications. It gives impressive quantitative and qualitative results on multiple image restoration tasks meanwhile keeping privacy. Specifically, it employs a task-agnostic vision transformer to learn universal representation at the server, and several CNN-based task-specific heads and tails to handle different image restoration tasks at the client side. It also gives a training strategy to learn this model.""916\tReward hacking, where RL agents exploit gaps in misspecified reward functions, has been widely observed but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards and investigate how agents capabilities, model capacity, action space resolution, observation space noise, and training time impact reward hacking. More capable agents often exploit reward misspecifications, achieving higher proxy rewards and lower true rewards than less capable agents. We find instances of phase transitions, where the agent's behavior qualitatively shifts and leads to a sharp decrease in the true reward, which pose challenges to monitoring the safety of ML systems. We propose an anomaly detection task for aberrant policies and offer several baseline detectors to address this.\tThis paper targets the very important problem of reward-hacking that occurs when the objectives optimized by intelligent agents are misaligned with respect to the tru objectives of the algorithm designer. The paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking. ""917\tReward hacking, where RL agents exploit gaps in misspecified reward functions, has been widely observed but not yet systematically studied. This paper constructs four RL environments with misspecified rewards and investigates how reward hacking arises. The paper shows that more capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. The paper also finds instances of phase transitions, where the agent's behavior qualitatively Shifts, leading to a sharp decrease in the true reward. To address this, the paper proposes an anomaly detection task for aberrant policies and offers several baseline detectors.\tThis paper studies reward hacking, a common but understudied phenomenon, across a set of environments. Reward hacking emerges in several tasks, meaning that the resulting policy has a high proxy reward but a low true reward. A key finding is that reward hacking increases with agent capabilities so that increasing capability lowers the true reward. This holds across several ways of increasing capabilities (model size, training steps, action space, etc). The authors also find \u2018phase transitions\u2019 where a small increase in capability results in qualitatively new reward hacking behavior, a phenomenon that may require novel monitoring strategies. One such strategy is anomaly detection, for which the authors introduce a benchmark and baselines. ""918\tReward hacking, where RL agents exploit gaps in misspecified reward functions, has been widely observed but not yet systematically studied. This paper investigates reward hacking as a function of various agent capabilities such as model capacity, action space resolution, observation space noise, and training time. The paper also finds instances of phasetransitions where the behavior of the agent qualitative shifts and leads to a sharp decrease in the true reward. To address this, the paper proposes an anomaly detection task for aberrant policies and offers several baseline detectors.\tThis paper provides a systematic study of \u201creward hacking\u201d in the environments with the misspecified rewards. The authors conduct a set of experiments with 4 environments, several types of reward misspecification in each of them and several agents of different expressivity (model capacity). They notice that often the agents that are more capable end up obtaining high proxy reward, but low real reward. Besides, often the transition to the low real reward happens very quickly and authors call this phenomenon \u201cphase transition\u201d. Finally, they propose a baseline for anomaly detection to identify this phase transition.""919\tReward hacking, where RL agents exploit gaps in misspecified reward functions, has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards and investigate how agents capabilities, such as model capacity, action space resolution, observation space noise, and training time, affect their ability to exploit reward misspecifications. More capable agents often achieve higher proxy rewards and lower true rewards than less capable agents, leading to behavior that qualitative changes, and causing a sharp decrease in the true reward. We find instances of phase transitions, where the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward, and propose an anomaly detection task for aberrant policies to address these challenges.\tThis paper investigates the phenomenon of reward hacking as a function of agent capabilities. They introduce four diverse RL environments with nine misspecified rewards and demonstrate that more capable agents are better at exploiting the misspecification. They find instances of phase transitions where a small increase in agent capability produces a large change in behavior that sharply decreases the true reward.  To mitigate the reward hacking problem, they propose to set up an anomaly detection task, given a trusted model with moderate performance on the true reward, where the anomaly detector's task is to identify whether policies from a different model are satisfactory for the true reward. They provide several baseline anomaly detectors and show how they perform on different tasks. ""920\tIn this paper, we propose a f -divergence Thermodynamic Variational Objective ( f -TVO ) as a generalization of the Thermodynamic Variational Objective ( TVO ) and the Kullback-Leibler ( KL ) divergence. f -TVO approximates the dual function of model evidence f\u2217 ( p ( x ) ) rather than the log model evidence log p ( x ) in TVO . The f -TVO is derived from a deformed \u03c7-geometry perspective and can be integrated along the deformed geodesic between the variational posterior distribution and the true posterior distribution. The f -TVO is optimized using a combination of reparameterization trick and Monte Carlo approximation.  experiments on VAE and Bayesian neural network show that the proposed f -TVO performs better than the corresponding baseline f -divergence variational inference techniques.\tThe paper proposes a novel f-divergence Thermodynamic Variational Objective (f-TVO) framework for VI, that extends the TVO towards, a more general, family of f-divergences. The authors propose to use a $\\chi$-deformed exponential distribution, which casts the f-TVO objective as integral along the $\\chi$-path between p(x,z) and q(z|x) (rather than the geometric path in TVOs) under $\\chi$-geometry. The authors propose different variants of f-TVO, that vary between the type of the f-divergance used, as well as how this integral is approximated (K-partitioned ) left-Riemann sum (related to ELBO), right-Riemann sum (related to EUBO) or a 'zig-zag' that alternates between the two. Besides theoretical justifications, results from two sets of experiments show that, in general, the proposed f-TVO perform comparable to or slightly better than the f-VI counterparts, but without clear conclusion wrt the choice of the f-divergence.""921\tThis paper proposes a newf-vergence-basedThermodynamic Variational Objective (f-TVO) for efficient optimization of model-based systems. The f-TVO is a generalization of theThermodynamic Variational Objective (TVO) that replaces the Kullback-Leibler (KL) divergence with a arbitary differentiable f- divergence. The f-TVO approximates the dual function of model evidence f\u2217 (p(x)) rather than the log model evidence log p(x). The f-TVO is derived from a deformed \u03c7-geometry perspective and can be integrated along the deformed geodesic between the variational posterior distribution and the true posterior distribution. The f-TVO is optimized using a combination of reparameterization trick and Monte Carlo approximation. \u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c the proposed f-TVO \u5728 VAE \u548c Bayesian \u795e\u7ecf\u7f51\u7edc\u4e0a\u6bd4\u76f8\u5e94\u57fa\u7ebf\u7684 f -divergence  variational Inference \u66f4\u6709\u6548\u3002 1 \u76ee \u5f55 1. \u5f15\u8a00 1.1 \u7814\u7a76\u80cc\u666f 1.2 \u7814\u7a76\u610f\u4e49 1.3 \u7814\u7a76\u5185\u5bb9 1.4 \u7814\u7a76\u65b9\u6cd5 1.5 \u8bba\u6587\u7ed3\u6784 2 \u76f8\u5173\u5de5\u4f5c 2.1 TVO \u7684\u76f8\u5173\u5de5\u4f5c 2.2 KL div \u7684\u76f8\u5173\u5de5\u4f5c 2.3 f-div \u7684\u76f8\u5173\u5de5\u4f5c 2.4 \u6bd4\u8f83\u7814\u7a76 2.5 \u7ed3\u8bba 3 \u7814\u7a76\u6846\u67b6 3.1 \u03c7-geometry \u6846\u67b6 3.2 \u5b9a\u4e49 f -TVO 3.3 \u4f18\u5316 f -TVO 3.4 \u5b9e\u9a8c\u8bbe\u8ba1 4 \u5b9e\u9a8c\u7ed3\u679c 4.1 \u6570\u636e\u96c6 4.2 \u5b9e\u9a8c\u65b9\u6cd5 4.3 \u5b9e\u9a8c\u7ed3\u679c 4.4 \u5206\u6790 5 \u7ed3\u8bba 5.1 \u603b\u7ed3 5.2 \u8d21\u732e 5.3 \u5c55\u671b 6 \u53c2\u8003\u6587\u732e 6.1 \u76f8\u5173\u6587\u732e\u7efc\u8ff0 6.2 \u76f8\u5173\u6587\u732e\u5217\u8868 7 \u81f4\u8c22 7.1 \u5bf9\u672c\u7814\u7a76\u5de5\u4f5c\u7684\u652f\u6301 7.2 \u5bf9\u672c\u7814\u7a76\u5de5\u4f5c\u7684\u8d21\u732e 8 \u6458\u8981 8.1 \u6458\u8981\u5185\u5bb9 8.2 \u6458\u8981\u7ed3\u6784 9 \u5173\u952e\u8bcd 9.1 \u5173\u952e\u8bcd\u5217\u8868 9.2 \u5173\u952e\u8bcd\u63cf\u8ff0 10 \u7b26\u53f7\u8bf4\u660e 10.1 \u7b26\u53f7\u5217\u8868 10.2 \u7b26\u53f7\u63cf\u8ff0 11 \u81f4\u8bfb\u8005 12 \u53c2\u8003\u6587\u732e 13 \u81f4\u8c22 14 \u9644\u5f55 15 \u81f4\u8c22 16 \u53c2\u8003\u6587\u732e 17\tThe paper presents a new bound as the objective for variational inference. The bound combines the recent progress of thermodynamic variational object which gives a tighter bound than the conventional ELBO, and the f-divergence which induces more possible distribution metrics. Experiments show its better performance on some Bayesian inference tasks.""922\tThis paper proposes a new f -divergence-based Thermodynamic Variational Objective (f -TVO) for efficient inference in models with large model evidence. The f -TVO is a generalization of the Thermodynamic Variational Objective (TVO) that replacing the KL divergence with a different divergence, the f - divergence. The f -TVO is derived from a deformed \u03c7-geometry perspective and can be integrated along the deformed geodesic between the variational posterior distribution and the true posterior distribution to achieve efficient inference. The proposed f -TVO is evaluated on a VAE and a Bayesian neural network and shown to perform better than a corresponding baseline f -divergence variational inference method. 1 Introduction. Variational inference (VI) is a core technique in probabilistic machine learning that is used to infer the parameters of models with large model evidence. The TVO is a commonly used optimization objective for VI, which is based on theKL divergence between the variational distribution and the true distribution. However, the TVO has several limitations, such as its computational complexity and the lack of a natural extension to larger models with more parameters. To address these issues, a new optimization objective, the f -TVO, was proposed in this paper. The f -TVO is a generalization of the TVO that replaces the KL divergence with a different divergence, the f - divergence. The f -TVO is derived from a deformed \u03c7-geometry perspective and can be integrated along the deformed geodesic between the variational posterior distribution and the true posterior distribution to achieve efficient inference. The proposed f -TVO is evaluated on a VAE and a Bayesian neural network and shown to perform better than a corresponding baseline f -divergence variational inference method.\tThis paper proposed an $f$-divergence TVO, which includes some existing works e.g., RVB, CUBO, ELBO into a unified framework. This paper's main idea is to transform $f$-divergence into a generalized $\\chi$-exponenetial family and integral TVO along the $\\chi$-path. The paper provides some theoretical analysis and the optimization methods of the suggested framework and supports the proposed $f$-TVO with numerical results. ""923\tThis paper proposes a new f -divergence-basedThermodynamic Variational Objective (f -TVO) for inferring the model evidence in aVariational Bayes (VB) framework. The f -TVO generalizes the Thermodynamic Variational Objective (TVO) by replacing the KL divergence with a arbitary differentiable f - divergence. The f -TVO is derived from a deformed \u03c7-geometry perspective, and it is able to integral along the \u03c7-path, which is the deformed geodesic between the variational posterior distribution and the true posterior distribution. The f -TVO is optimized using a reparameterization trick and Monte Carlo approximation.\n\nExperiments on a VAE and a Bayesian neural network show that the proposed f -TVO performs better than the corresponding baseline f -divergence variational inference. This is due to the ability of the f -TVO to capture the important features of the model evidence and to better approximate the dual function of the model evidence f\u2217(p(x)) in the TVO.\n\nThe paper also proposes a set of conditions under which the f -TVO is stable, which ensures that the optimization algorithm will not overfit the data. Additionally, the paper provides an analysis of the stability of the f -TVO under certain conditions.\n\nOverall, the proposed f -TVO provides a new framework for inferring the model evidence in aVB framework and performs better than existing methods in certain conditions.\tThis paper proposed new variational inference that combines f-divergence variational inference and the thermodynamic variational objective. The authors introduced several new concepts of exponential families to extend TVO. Finally, the authors provided the estimator of the gradient of the objective function based on the reparametrization trick.""924\tThis paper provides an empirical study of various methods and tools from the RL toolbox that are commonly used for continuous control in the OpenAI Gym and MuJoCo settings. The paper explores the performance of these methods and how they compare to traditional control algorithms in terms of accuracy, stability, and time complexity. The study also includes a detailed analysis of the interactions between individual improvements and the overall performance of the RL algorithm. The paper provides practical insights into the use of RL for continuous control and suggests some potential areas for future research.\tThis paper presents a deep reinforcement algorithm, Ensemble Deep Deterministic Policy Gradients (ED2), for continuous control tasks. The algorithm is empirically derived and is claimed to represent SotA performance on several tasks and while providing more stable results. These claims are justified based primarily on the (reward and stability) results on 4 MuJoCo environments.  ""925\tThis paper discusses the use of deep reinforcement learning (RL) methods in the continuous control setting of the OpenAI Gym and MuJoCo challenges. It presents an empirical study of various tricks and methods that have been shown to improve the performance of RL algorithms in this setting. The paper also discusses the challenges ofAssigning individual improvements to the overall performance of the algorithm andthe uncertainty in the interactions between the individual improvements and the overall performance of the algorithm. The paper provides an overview of the findings and future directions for research in this area.\tThis paper has two main contributions: it introduces an ensemble-based actor-critic method, and it answers some pertinent questions in policy optimization by focusing on its different components. The ensemble is different from multi-actor learners that interact with multiple environments simultaneously, violating the standard RL setup. Instead, the learner of this paper maintains multiple actors and critics but uses only a single actor at a time to interact with the environment. All actors and critics are trained on a common replay buffer. The base method is the streamlined off-policy (SOP) method, which unlike soft actor-critic (SAC) doesn\u2019t use an entropy bonus. Additionally, no exploration noise is added, resulting in their Ensemble Deep Deterministic (ED2) method.  The proposed algorithm ED2 is shown to be superior and more stable in performance according to different measures compared to existing methods. It is also revealed that actor initialization affects performance less than critic initialization. ED2 uses deterministic actors, and its exploration comes from sampling among the actors. Such a form of exploration is also shown to be superior to UCB-style exploration. ""926\tThis paper discusses the use of deep reinforcement learning (RL) methods in continuous control in the OpenAI Gym and MuJoCo settings. It explains the challenges and opportunities associated with this use case, and presents an empirical study of various RL tools and techniques that have been used in the field. The paper also discusses the performance of these methods and the role of individual improvements and credit assignment in improving the overall performance of the RL algorithm.\tThe paper presents an empirical study evaluating the commonly accepted design choice in off-policy Deep RL algorithms in continuous control settings. The use of additive exploration noise, initialization choices, update frequency, and precision for retraining are tested empirically highlighting some interesting results. The paper also introduces ED2 - an ensemble method utilizing the design choices from the study which is demonstrated to achieve SOTA results on Mujoco benchmarks.""927\tThis paper presents a comprehensive empirical study of multiple tools from the RL toolbox applied to continuous control in the OpenAI Gym MuJoCo setting. The study aims to investigate the effects of these tools on the performance of deep reinforcement learning algorithms. The paper also discusses the challenges and opportunities associated with using these tools in the continuous control setting. The study includes a review of the existing literature on the use of RL tools in continuous control, and a detailed analysis of the results of various experiments conducted using various algorithms and tools. The paper concludes by providing recommendations for future research in this field.\tThis paper conducted an experimental study over a range of tricks that are often exploited to facilitate ensemble deep reinforcement learning. The experiment results show several interesting findings. For example, it was found that commonly used additive action noise may not be necessary for effective exploration. Meanwhile, experiments show that the initialization of critics perhaps has a higher impact on learning performance than the initialization methods adopted for actors. These findings can be quite important to guide future design of more effective ensemble reinforce learning algorithms.""928\tThis paper focuses on Equalized Loss (EL) which is a fairness notion that requires the prediction error/loss to be equalized across different demographic groups. It leads to a non-convex optimization problem even if the loss function is convex. The paper proposes algorithms that can leverage off-the-shelf convex programming tools to efficiently find the global optimum of this non-convex problem. The first algorithm is the ELm which is a multi-level model that considers the prediction error across different groups and optimizes the loss function at each level. The other algorithms are the EL-CVF and EL-MP which use the cross-validation and the multi-scale learning method respectively to estimate the global optimum. The paper also provides results on a real-world dataset and discusses the challenges and opportunities in using EL in real-world settings.\t    This paper studies the problem of fair supervised learning under the Equalized Loss (EL) fairness notion, which is formulated as a non-convex constrained optimization problem. The authors introduce two algorithms that find the global (sub-)optimal solution by solving a sequence of convex (constrained) optimizations. Empirically, the algorithms perform well.""929\tThis paper focuses on the problem of finding fairpredictors in supervised learning, which is often faced in various domains such as lending, college admission, natural language processing, and face recognition. The goal is to ensure that the predictions made by the model are fair and do not\u6b67\u89c6protected social groups.Equalized Loss (EL) is a fairness notion that requires the prediction error/loss to be equalized across different demographic groups. This constraint is applied to the learning process, which leads to a non-convex optimization problem. To address this issue, we introduce algorithms that can leverage off-the-shelf convex programming tools and efficiently find the global optimum of the non-convex problem. Our proposed ELm algorithm is particularly effective in finding the global optimum of the EL problem.\tThe authors consider minimization of convex losses constrained by either bounded loss on each group, or bounded difference of losses over two groups. The second formulation is non-convex, whereas the first formulation is convex.   When the losses are strictly convex on both the demographic groups, so that their optima are distinct (I think this is the condition they need, but they use a more restrictive condition in the paper), they can find the \"EL\" fair predictor by solving a sequence of convex constrained optimizations, by exploiting a monotonicity property. They next give a more computationally efficient approximate algorithm for finding the EL fair predictor. ""930\tThis paper discusses the issue of fairness in supervised learning models, specifically in the context of equalized loss (EL) constraints. EL constraints require that the prediction error/loss across different demographic groups be equalized. While existing models may exhibit discrimination against protected social groups, the introduction of EL constraints can address this issue by ensuring that the predictions are fair. The paper presents algorithms that can efficiently solve the non-convex optimization problem associated with EL constraints using off-the-shelf convex programming tools. The algorithms are proposed in the context of ELm, a specific implementation of the EL concept. The paper also discusses the potential challenges and limitations of solving the EL problem using such algorithms.\tThe authors study fair prediction subject to Equalized Loss (EL), and they introduce a variety of approaches for exactly and approximately solving the problem of finding the globally optimal predictor that satisfies EL. First, they show how to solve a sequence of convex constrained optimization problems in order to solve the larger non-convex problem. Next, they show how to approximately solve this problem more efficiently by using unconstrained convex optimization. Lastly, they evaluate both of their approaches on two datasets.""931\tThis paper discusses the issue of fairness in supervised learning models, specifically in the context of equalized loss (EL) fairness notion. EL requires that the prediction error/loss across different demographic groups be equalized. Despite the use of datasets that are inclusive of all demographic groups, supervised learning models may exhibit discrimination against certain groups. The paper focuses on algorithms that can efficiently find the global optimum of a non-convex optimization problem, which is caused by the imposed EL constraint. The paper proposes the ELm algorithm, which leverages off-the-shelf convex programming tools to find the global optimum of the non-convex problem.\tThis paper studies supervised learning models with fairness constraints. They specifically consider equalized loss fairness constraint.  When a traditional (convex) loss minimization problem is cast with additional fairness constraints, the corresponding problem is non-convex. They provide algorithms to efficiently solve this problem up to global optimality. They demonstrate the performance of their algorithms on real-world data. ""932\tThis paper discusses the issue of how systematic generalization is achieved in connectionist networks, and how recent research has shown that modern neural networks do not have the ability to master this challenge. The paper also highlights the importance of strong systematic compositionality in language and thought expression. The paper begins by introducing the crucial characteristic of systematic generalization, which reflects people's ability to learn infinite combinations of finite concepts. It then discusses the previous research on how weak systematic compositionality has been an obstacle to the expression of language and thought in connectionist networks. The paper also examines the recent studies that have shown that modern neural networks do not have the ability to master language-based generalization challenges. The paper ends by highlighting the need for more systematic research in this area to better understand how systematic generalization is achieved in connectionist networks.\tThis paper revisits the problem of neural network's systematic generalization ability from the perspective of meaningful learning, or more specifically, semantic linking. Based on this view, they propose two data augmentation methods from either the inductive learning perspective or deductive learning perspective. They train different model variants with such augmented data. The empirical results on SCAN, GEO, and ADV show that models can behave systematically. They further group some data augmentation methods on the machine translation task and semantic parsing task into the inductive or the deductive category, and show these augmentation methods can bring benefit in real data. ""933\tThis paper explores the issue of how systematic generalization is expressed in connectionist networks and how modern neural networks have struggled to master this challenge. The paper begins by discussing the crucial characteristic of systematic generalization, which is the ability to learn infinite combinations of finite concepts. Weak systematic compositionality has been considered as a primary obstacle to the expression of language and thought in connectionist networks for a long time, and recent studies have shown that modern neural networks have not mastering this challenge in multiple explicitly proposed datasets. The paper ends by discussing the implications of this finding for our understanding of how language and thought are expressed in neural networks and how these networks can be used to improve language processing and other cognitive tasks.\tThe paper introduces an interesting idea of improving the systematic generalization ability via meaningful learning. Through providing augmented data for inductive learning and deductive learning, the sequence-to-sequence model can be more generalizable to compositions of new concepts. It tests on real data to provide evidence of the efficacy. ""934\tThis paper discusses the issue of how modern neural networks are able to generalize systematically, which has been a challenge for connectionist networks. The paper highlights the importance of strong systematic compositionality in language and thought expression, as well as the recent findings that modern neural networks do not have the ability to master this challenge. The paper proposes that more systematic approaches to neural network training could potentially address this issue.\tThis paper considers the problem of learning novel words from a few examples. The authors name their approach as \"meaningful learning,\" which, at a high level, means that we should relate the new word with existing words. The concrete technique they proposed is to use domain-specific rules to generate new data that contains novel words based on the existing examples.""935\tThis paper discusses the issue of how modern neural networks are able to generalize systematically, which has been a goal of research in the field of cognitive science for some time. The paper argues that modern neural networks, while capable of performing complex tasks, have a problem with weak systematic compositionality, which makes it difficult for them to learn infinite combinations of finite concepts and to generalize consistently across different domains. The paper presents several recent studies that have found that modern neural networks are not able to master these language-based generalization challenges, which suggests that there is a need for more systematic approaches to training and developing neural networks.\tThis paperintroduce semantic linking for systematic generalization through the analysis of inductive and deductive learning from a meaningful learning perspective. They show that both prior knowledge and semantic linking play a key role in systematic generalization, which is in line with the so-called 'meaningful learning theory'. Interesting results are attained from SCAN to real data.""936\tThis paper provides an overview of the recent research on point cloud analysis and its applications in computer vision and graphics. The paper explores the importance of geometric representations in the classification task, and presents a series of new and innovative techniques for extracting such representations from 3D point clouds. The paper also discusses the challenges and limitations of current methods and presents research directions for future work. The paper ends with a summary of the main findings and a discussion of the practical implications of these techniques.\tThis paper proposes a novel 3D point cloud representation learning framework. At the core of this method, is a lifting scheme inspired by wavelet decomposition. The proposed method roughly splits the input data in half at each stage, producing a down-sampled approximation C and detail d. Then C is further processed by the next layer, forming a multiscale pyramid. In summary, the contribution of this paper is:  1. Proposed to use the lifting scheme in point cloud processing, using graph convolution networks and transformers as backbone.  2. Evaluated the method against state-of-the-art baselines and showed that the proposed scheme performs well. ""937\tThis paper discusses the use of neural networks to learn implicit geometric representations of 3D point clouds, which are used for classification tasks. The paper provides an overview of the background of point cloud analysis and the importance of geometric representations in computer vision and graphics. The paper also discusses the applications of these geometric representations, including robotics, autonomous driving, visual SLAM, and more. Finally, the paper presents some recent advances in using neural networks for point cloud analysis and discusses future directions and challenges.\tThis paper proposes a new 3D shape representation learning method using multi-scale wavelet decomposition. In particular, the authors introduce a neural network architecture that decomposed 3D shapes into sub-bands components at multiple scales. In particular, starting from a pointcloud the proposed model learns to decompose it into coarse (high frequency) and detail (low frequency) components using an adaptive lifting scheme, similar to the original lifting scheme introduced for defining second-generation wavelets. Subsequently, two transformer models are used to refine the coarse and approximate geometry of the 3D shape. The proposed model achieves state-of-the-art results on the shape classification task on the ModelNet40 and the ScanObjectNN dataset and on the part segmentation task on the ShapeNet Part dataset. The concept of using such an adaptive lifting scheme seems to facilitate learning and to the best of my knowledge is novel for the case of shape representation learning.""938\tThis paper focuses on the analysis of 3D point clouds using neural networks. The paper introduces the importance of geometric representations in the classification task and discusses the use of deep learning to extract expressive semantics from point clouds. The paper also presents an example of a neural network designed for point cloud analysis, and\u8ba8\u8bba\u4e86\u4e00\u4e9b\u672a\u6765\u7814\u7a76\u65b9\u5411.\tThis paper presents a novel framework for 3D shape representation learning, which is based on multi-scale wavelet decomposition. This is very different from existing works. A novel transformer-based neural network, AWT-Net, is also proposed.""939\tThis paper focuses on the analysis of 3D point clouds and the learning of geometric representations implicit in the classification task. It provides an overview of the current state-of-the-art in point cloud analysis and the applications of such techniques in robotics, autonomous driving, visual SLAM, and other fields. The paper also discusses the challenges and future directions in this field, such as the need for more advanced neural networks and the development of more efficient algorithms for point cloud analysis.\tThe paper presents a new deep neural network architecture for 3D point cloud representation learning, based on wavelet decomposition. In particular, the authors propose a data-driven adaptive lifting scheme that introduces non-linearity into wavelet. The original linear operators update(U) and predict(P) in wavelet decomposition are replaced by non-linear graph convolutional networks (GCN). Equipped with wavelet transform and Transformers, the proposed network aims to captures and refines the holistic and complementary geometry of 3D shapes to supplement neighboring local information. Experimental results on standard benchmarks (i.e., shape classification and part segmentation) show that it achieves state-of- the-arts or competitive performance.""940\tThis paper studies the trade-off between finetuning a pretrained language model for natural language generation and achieving strong performance on in- distribution (ID) and out-of- distribution (OOD) tasks. It proposes two families of finetuning approaches: full finetuning and lightweight finetuning. Full finetuning involves training the entire pretrained model and achieving strong ID performance. However, it has poor OOD performance. On the other hand, lightweight finetuning involves freezing most of the pretrained model's parameters during finetuning and achieving strong OOD performance, but worse ID performance. This paper also investigate the effects of various hyperparameters and pretrained language model's architecture on the finetuning performance.\tThis paper proposes a simple yet effective method, cocktail fine-tuning, for the natural language generation tasks. Their results show that cocktail fine-tuning can handle both In-domain data and Out-of-domain data effectively by combing adapter-finetuning and full-finetuning through knowledge distillation and overall has comparable performance compared to their ensembles. It also provides theoretical analysis on multi-class logistic regression to explain why it works.""941\tThis paper discusses the trade-off between achieving strong in- distribution (ID) performance and strong out-of- distribution (OOD) performance when finetuning a pretrained language model for natural language generation tasks like summarization and table-to-text. The paper defines two families of finetuning approaches: full finetuning and lightweight finetuning. It shows that both approaches have their own strengths and weaknesses, and that the choice of approach depends on the specific task and the available resources. The paper also discusses the recent developments in these areas, such as adapter and prefix-tuning methods, and their implications for finetuning pretrained language models for natural language generation.\tThis paper proposes an ensemble model between a full fine-tuning model and a parameter-efficient fine-tuning model to improve the out-of-distribution (OOD) performance of a full fine-tuning model. The proposed method is inspired by the observation that full fine-tuning model achieves good in-distribution (ID) performance while parameter-efficient finetuning model achieves better OOD performance. There are two ensembling methods presented in the paper: linear interpolation between the predictions of the two models; and distill from the predictions of a parameter-efficient model with ID training data. Improved OOD performance is observed with this ensemble method.""942\tThis paper studies the trade-off between in- distribution (ID) and out-of- distribution (OOD) performance when finetuning a pretrained language model for natural language generation tasks such as summarization and table-to-text. It explores two families of finetuning approaches: full finetuning and lightweight finetuning. Full finetuning involves completely finetuning the entire pretrained language model, while lightweight finetuning involves freezing most of the pretrained parameters and achieving stronger OOD performance but worse ID performance. The paper also discusses the limitations of these methods and how they can be overcome.\tThis paper presents interesting an idea of combining lightweight fine-tuning and full fine-tuning to achieve the best of both approaches, i.e. perform best on out-of-domain and in-domain data. The authors proposed two approaches: a simple ensemble method and a so-called cocktail fine-tuning that combines two fine-tuning methods in one single model. They evaluated their tasks in three datasets: WebNLG, XSUM and OpenQA and obtained mixed results. The authors also provided good analyses for more insights. ""943\tThis paper discusses the trade-off between achieving strong in-distribution (ID) performance and strong out-of-distribution (OOD) performance when Finetuning a pretrained language model for natural language generation tasks such as summarization and table-to-text. It discusses two families of finetuning approaches: full finetuning and lightweight finetuning. The paper explains that full finetuning achieves strong ID performance, but poor OOD performance, whereas lightweight finetuning achieves stronger OOD performance, but worse ID performance. The paper also provides examples of applications of these methods and discusses their limitations.\tThe present paper first discusses the trade-off between performance for out-of-domain data and in-domain data with respect to whether the model is fully fine-tuned or lightweight fine-tuned on NLG tasks. Second, it argues that such a trade-off is not necessary if one can make use of both of these two fine-tuning schema in a clever way. To this end, it proposes cocktail fine-tuning, which augments full fine-tuning via distillation from a lightweight model and which achieves equal performance as an ensemble of the two fine-tuning schema. At length, this paper also explains the behavior of the cocktail fine-tuning through a toy model. ""944\tThe paper discusses the use of machine learning (ML) in the clinical context, specifically in the area of health care. It explains the benefits of ML, including its ability to identify patterns and predict future events, and the use of ML algorithms in various medical applications, such as detecting abnormal heart rhythms, predicting neurological recovery, and detecting lung cancer in CT scans. The paper also highlights the challenges that arise when using ML in the clinical context, including the need for careful data annotation and the limitations of manually- Labeled data. Finally, the paper concludes by discussing the potential of ML in the healthcare industry and its place in the future of healthcare.\tIn this work, the authors propose the WARM method to help conduct iterative and interactive weakly-supervised learning. Active learning is used here to refine the labeling functions by focusing on data points that are once labeled. The authors further incorporated gradient propagation to alternatively update the LF parameters and the DP model. Experimental results show that the WARM method can improve the quality of training data.""945\tThis paper discusses the use of machine learning (ML) algorithms in the clinical context. It explains the benefits of ML in the field, including its ability to detect abnormal heart rhythms, predict neurological recovery, and detect lung cancer in CT scans. However, the paper highlights the challenges that arise when using ML in the clinical setting, including the lack of careful annotation of large quantities of clinical data and the need for robust training data. The paper also discusses the use of cheap, potentially noisy sources of data in addition to manually labeled data. The paper concludes by suggesting that ML should be used in a\u8c28\u614e\u3001\u79d1\u5b66\u3001\u5168\u9762(science-driven, careful, and comprehensive) manner, with consideration given to the resources required and the potential impact on patients.\tThis paper proposes WARM, an active learning approach to weakly/programmatically supervised learning.  In the WARM approach, which bases off of the data programming/Snorkel paradigm for weak supervision, users write labeling functions (LFs) to programmatically label training data; these labeling functions are then modeled by the Snorkel framework for weak supervision and used to train downstream models.  In the WARM setup, these LFs are assumed to be, or cast as, differentiable.  The paper then proposes an active learning approach to sampling labeled data points to tune the parameters of these LFs, and validates this approach on several medical datasets.""946\tThis paper discusses the use of machine learning (ML) in the clinical context, particularly in the areas of health, education, and public policy. It highlights the benefits of ML in solving a range of clinical problems and the challenges of collecting and labeling vast amounts of pointillistically labeled training data. The paper also discusses the recent use of cheap and potentially noisy sources of data, such as real-time health information, in place of manually labeled data. The paper ends by highlighting the potential of ML in driving change in the healthcare system.\tThis paper proposes an algorithm for choosing a small set of labels to improve labeling function model performance both directly and for downstream tasks. Additionally, the authors provide a general method to convert standard labeling functions to \"soft\" labeling functions which are differentiable with respect to some parameters (e.g. a threshold). If the labeling functions are differentiable, this paper provides a method to update the labeling function parameters. Finally, experimental results show that the method introduced outperforms other active labeling approaches for weak supervision.""947\tThe paper discusses the use of machine learning (ML) algorithms in the clinical context, specifically in the area of health and health-related quality of life (HRQL). The paper highlights the effectiveness of ML algorithms in solving a wide range of problems, including abnormal heart rhythms in electrocardiogram (ECG), prognosticating neurological recovery, and detecting lung cancer in CT scans. However, the paper also discusses the challenges of using ML in the clinical setting, including the lack of careful annotation of large quantities of clinical data and the use of potentially noisy sources of data. The paper also highlights the need for further research in the area of ML for clinical use and the potential applications of ML in other domains such as education and public policy.\tThe paper gives a method to iteratively and interactively improve the label model in weak supervision. The approach consists of two steps, first is standard weak supervision way of weighted combination of labelling functions to generate labels. Novelty and improvement mainly comes from the second step, where true label for most uncertain data point is queried using which the parameters of the labelling functions are improved, which in turn lead to a more accurate weak supervision model. A key requirement and assumption in this paper's setup is that labelling functions are given by some learnable parameters ( i.e. they can be differentiated w.r.t. their parameters), which allows parameters updates using the true labels acquired. Empirical results on various real world datasets in medical domain show that in some cases this approach can yield a more accurate model in comparison to pure active learning approach and some recent baselines which combine weak supervision with active learning. These results also show that the paper's approach can get accuracy comparable to fully supervised model as well.  ""948\tThis paper discusses the problem of training a classification model with groupannotated training data, where the distribution across different groups may be different. It presents research that has shown that models trained using the standard empirical risk minimize (ERM) objective suffer from poor performance on minority groups, and Group-DRO is a better alternative. The paper proposes a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind the algorithm is to focus on groups with the worst regularized loss, rather than focusing on individual groups, to improve performance on the minority groups. The paper also discusses the limitations of the current approach and suggests possible future directions.\tThis paper proposes a novel ERM-based method for classification task with group annotated training data. The goal is to be group distributionally robust while enhancing the minority performance. The authors make an improvement to an existing method named Group-DRO by modifying the focus on the group with the highest regularized loss to focus on the group that leads to the largest decrease in average training loss. They analyze the convergence and present detailed comparisons with Group-DRO.""949\tThis paper discusses the problem of training a classification model with groupannotated training data. It\u6307\u51fa\u5982\u679c\u4e0d\u540c\u7ec4\u4e4b\u95f4\u5206\u5e03\u6709 shift\uff0c\u4f7f\u7528 ERM  objective \u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5c11\u6570\u65cf\u88d4\u7ec4\u4e0a\u8868\u73b0\u4e0d\u597d\uff0c\u800c group distributionally robust optimization (Group-DRO)  objective\u66f4\u9002\u5408\u8fd9\u79cd\u60c5\u51b5\u3002\u8be5 paper \u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b80\u5355\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5 explicit ly encourage \u5b66\u4e60\u5230\u4e0d\u540c\u7ec4\u4e4b\u95f4\u5171\u4eab\u7684\u7279\u5f81\u3002\u7b97\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0cGroup-DRO \u5173\u6ce8 groups with the worst regularized loss\uff0c\u800c focus \u5219 on groups that share features with others\u3002\u8be5\u7b97\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4 ERM \u8868\u73b0\u66f4\u597d\u3002\tThe paper gives a new algorithm for the setup where the test distribution is different from the train distribution. The setup includes multiple groups whose information is present during the training time but not during test time and the relative proportion of these groups change during test. The most commonly used method group-DRO does distributionally robust optimization or finds a classifier which performs well on the group with worst loss. This paper proposes to focus instead on the group which leads to maximum decrease in the loss while training instead of the group which has the maximum loss. The paper present several synthetic toy cases where their approach could be useful and concludes with experiments on a variety of benchmarks for this setup and shows improved results.""950\tThis paper discusses the problem of training a classification model with group annotated training data. It notes that if there is a distribution shift across different groups, models trained using the standard empirical risk minimize (ERM) objective suffer from poor performance on minority groups. Group-DRO, an objective that focuses on group distributionally robust optimization, performs better on minority groups on some benchmark datasets but performs much worse on other datasets. This paper proposes a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind the algorithm is to focus on groups with the worst regularized loss rather than focusing on individual groups. The proposed algorithm is simple and effective in improving the performance of the model on the majority of the groups.\tThe paper proposes a new method for robust ML under distribution shifts. Past work has looked at formulations that minimize the worst group error. This paper adds a new twist on it and instead argues for focusing on the group that leads to the greatest decrease in average training error for all the groups. This intuition is combined into an algorithm and the paper proves that though their proposed algorithm doesn't minimize a specific loss function, it still finds first-order-stationary points. The results are shown on several synthetic datasets as well as on the WILDS Robust ML benchmark that show the superior performance of the proposed algorithm over several baselines.    Main Contributions:   1). The paper proposes a new approach for robust ML under distribution shifts that performs gradient descent not on the group with worst error but on the group which decreases the average error of all other groups.  2). Results are shown on synthetic and real-world datasets which show the superior performance of the proposed method in achieving group robustness. ""951\tThis paper studies the problem of training a classification model with groupannotated training data. It argues that if there is distribution shift across different groups, models trained using the standard empirical risk minimize (ERM) objective suffer from poor performance on minority groups. Group-DRO is a better alternative, but it performs much worse on some datasets. This paper proposes a new algorithm that explicitly encourages learning of features that are shared across various groups. The key insight behind the algorithm is to focus on groups with the worst regularized loss, rather than just focusing on minority groups. The proposed algorithm is simple and efficient, and it performs well on most datasets.\tThe paper provides an efficient method to generalize to all groups in the presence of sub-population shifts and domain adaptation. The paper conducts extensive simulations to derive insights and also numerical experiments on the benchmark dataset to demonstrate the performance. The proposed method is intuitive, easily implemented, and has good performance.""952\tThis paper presents a comprehensive review of explainability methods for blackbox models. It discusses the importance of interpretability in increasing user trust and the existing literature on this topic. The paper also discusses the limitations of current methods, which only provide univariate explanations and do not take into account feature interactions. The paper proposes a unifying mathematical framework called Removal-based Explanation methods, which can provide both univariate and feature interaction explanations.\tIn previous studies, Shapley value has been widely used to explain model predictions, in which previous studies generally characterized importance for each feature instance separately. While some works have explored the effect of combinatorial features on prediction, no works have explored how features interact to influence each other, such as whether feature A is redundant if feature B is present.   In order to achieve this goal, this paper offers an intuitive solution. The degree of feature interaction is estimated using a matrix called a bivariate Shapley explanation map. The j-th column and i-th row of the matrix represents how feature i influences the prediction if feature j has been included. Item ij can be seen as a revised Shapley value: rather than summing up the marginal contributions of all coalitions, it only considers them when feature j appears. They then propose four different types of interactions based on this definition: least/most influential features, directional/mutual redundancy. The experimental results indicate that the proposed method can efficiently discover these four types of feature interactions. ""953\tThis paper presents a comprehensive review of existing explanations methods for blackbox models. The paper discusses the history of explainability research, the challenges of providing univariate explanations for blackbox models, and the recent proposal of Remove-based Explanation methods, which overcome these challenges. The paper also highlights the importance of taking into account feature interactions in providing explainable explanations. The paper concludes by providing a summary of the key findings and highlighting the future directions for this field of research.\tThis paper attempts to alleviate the shortcomings of existing local-feature-interaction explainers that assume symmetrical feature interactions by proposing a bivariate feature-explanation map that can capture asymmetrical (directional) feature interactions; this analysis provides evidence of mutual and directional redundancy, offering a comprehensive understanding of the features most influential for a given prediction. This proposed approach can also be instantiated using any univariate feature-based explanation method. Empirical results on image, text, and tabular data show the ability of the proposed method to accurately identify mutual and directional redundancies.""954\tThis paper provides an overview of explainability methods for blackbox models, including the latest developments in the field and a discussion of the limitations of current approaches. The paper covers the history of explainability research, the importance of interpretability, the types of explainability methods, and the removal-based framework. The paper also discusses the recent proposal of a unified mathematical framework for removing and providing explanations, called removal-based Explanation methods. The paper concludes by highlighting the need for further research in the field to develop more effective and comprehensive explanations for blackbox models.\tThis work proposed to study the directional feature interactions to explain deep models. The proposed method is a graph-based explainer and the data can be considered as graphs. Then it studies the Bivariate Shapley values to consider the directional feature interactions. Experiments on several datasets show very promising results. ""955\tThis paper discusses the development of explainability methods for blackbox decision-making models. The paper proposes a unified mathematical framework called removal-based Explanation methods, which capture a broad array of existing explainability techniques and allows for the consideration of feature interactions. The paper also highlights the significant drawback of the existing literature, which is limited to providing univariate explanations and does not take into account feature interactions. The paper provides an overview of the importance of considering feature interactions in blackbox decision-making and suggests some potential future directions for the field.\tIn this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method.""956\tThis paper presents an overview of the current state of clinical decision-making support in medicine, highlighting the challenges and opportunities associated with developing systems that can effectively support this process. The paper highlights the importance of considering the design and interpretability of these systems, as well as the need to address the problem of unwarranted variation in medical practice across regions and institutions. The paper also presents an example of a system that has successfully overcome these challenges in the field of oncology, and discusses the ways in which this system has helped to improve patient outcomes and trust in the medical community.\tPOETREE aims to construct an interpretable model for a policy over a time series using decision trees. The healthcare domain is particularly targeted. As opposed to other works, the model directly maps observations of a POMDP to actions. POETREE creates a decision tree from time series data. The decision tree can be conditioned on the history, allowing the tree to be different at different time steps, allowing for example the tree to model that an exam done previously that is no longer informative is no longer likely. Each tree is a soft-probabilistic model first grown incrementally by developing, optimized globally (as it is differentiable), then pruned. Finally, the tree is simplified for interpretability by limiting each condition to a single variable. POETREE is then empirically evaluated and compared to baselines in terms of distribution modeling, interpretability and policy learning.""957\tThis paper discusses the issue of clinical information integration, analysis and action, which leads to wide variation in medical practice across regions and institutions. It\u6307\u51fa\uff0c algorithms designed to support the clinical decision-making process can help overcome this issue, but these systems focus on effectively replacing physicians with autonomous agents, trained tooptimise patient outcomes or diagnosis accuracy. However, these systems lack interpretability, leading to mistrust from the medical community. Instead, the paper proposes to better describe and understand the decision-making process by observing physician behavior. This can lead to more effective and interpretable systems.\t  The authors argued that many methods failed the merits of interpretability in some important areas, e.g. clinical decision-making. Thus, this paper proposed a (soft) tree-based method for synthetic clinical datasets in the matter of interpretability. The authors model the clinical decision process as a partially observable Markov Decision Process (POMDP), which naturally fits the assumption of medical diagnosis.""958\tThis paper discusses the issue of variation in medical practice across regions and institutions due to different approaches to integrating, analyzing and acting upon clinical information. It also highlights the need for algorithms designed to support the clinical decision-making process that can overcome this issue. The paper suggests that algorithms should focus on effectively replacing physicians with autonomous agents while also taking into account the design of end-users. Additionally, the paper argues for greater emphasis on observation and understanding of physician behavior to better describe and understand the decision-making process.\tThis paper proposes a novel approach for learning and representing human decision-making policies from observed behavioral data. The proposed approach emphasizes interpretability as a primary aim, while nevertheless seeking to maintain reasonable modeling accuracy. The decision tree model proposed extends canonical decision tree approaches to the probabilistic setting, allow for optimization of leaf-specific parameters via stochastic gradient descent. The proposed approach is evaluated both in terms of its interpretability (subjective measurements from a panel of licensed physicians) as well as its accuracy in recapitulating actions conditioned patient observations. The utility of the approach is demonstrated on both synthetic and real-world datasets.""959\tThis paper provides an overview of the current challenges in the integration, analysis and action of clinical information, and the use of algorithms to support the clinical decision-making process. It discusses the problems with current approaches to clinical information integration, such as the lack of interpretability of autonomous systems and the limited design concern for end-users. The paper also highlights the importance of physician behavior observation and understanding in order to better describe and understand the decision-making process. Finally, the paper presents a new approach to clinical decision-making that focuses on understanding the physician behavior and using it to improve the quality and interpretability of the decision-making process.\tThis paper proposes a new method to learn (stationary) interpretable policies using soft decision trees in partially observed settings. The soft decision tree structure is extended to allow for recursion over time, and account for policy decisions based on history of collected data. An algorithm is presented to optimize the parameters of the soft decision tree as well as the structure/topology of the tree. The algorithm mainly proceeds by splitting nodes and locally optimizing the parameters of the the associated probability representation of the soft node, and recursively split (if local optimization does not improve validation performance) and fixed as leaf otherwise. A global update step is then used after topology is fixed followed by pruning low probability paths in the trees. Experimental validation on surveys with clinicians demonstrate reasonable interpretability and improved prediction performance on imitating clinician policy. ""960\tThis paper discusses the use of data Augmentation (DA) to alleviate overfitting in training deep neural networks. DA methods such as rotation, Mixup, and Cut Mix improve the performance and robustness of the neural networks. However, these approaches are handcraft and require human prior knowledge, which causes weak transferability of DA across different datasets. To relieve the dependence on manual design and explore more adaptive augmentation, AutoAugment (AA) is proposed, which automates the search of the optimal DA policies (i.e., DA operation, probability, and magnitude) from the training dataset. AA trains a proxy model with the augmentation policy generated by a controller, which is updated through Reinforance. This paper presents the architecture and implementation of AA, and discusses its potential applications in fields such as computer vision and natural language processing.\tThis paper target the task of automatically determining the best augmentation method to obtain improved accuracy. While the previous related studies focus on the image-level augmentation and ignore the semantic information of the augmented images, the proposed algorithm augments the grid-wise patches of the given input with the preserved semantic information. To overcome the enlarged number of combinations to consider all the patches, the algorithm utilizes the MARL algorithm with the unified reward function. By MARL algorithm, the number of parameters can be reduced and the training speed can be much improved, compared to the previous auto-augmentation methods. Through the image classification and fine-grained image recognition tasks, the proposed algorithm was validated, and it shows the state-of-the-art performance among the compared methods.""961\tThe paper introduces the concept of data Augmentation (DA) and presents a new paradigm for implementing it, AutoAugment (AA). DA methods have been widely used to alleviate overfitting in training deep neural networks, but they are typically handcraft and have weak transferability across different datasets. AA automates the search of optimal DA policies from the training dataset using a controller, which is updated through Reinfor. The paper also presents some results from a real-world application of AA in a speech recognition task.\tThis paper proposes an automatic data augmentation approach. Different from existing works, they proposed to augment patches in the image rather than the whole image. The approach is formulated as a multi-agent reinforcement learning problem. They empirically show the effectiveness of their approach across several image classification datasets.""962\tThis paper discusses the use of data Augmentation (DA) to alleviate overfitting in training deep neural networks. DA enriches the diversity of training data and improves the performance and robustness of the neural networks. However, these approaches are typically handcraft and require human prior knowledge, which causes weak transferability of DA across different datasets. To relieve the dependence on manual design and further explore more adaptive augmentation, AutoAugment (AA) is proposed as a new DA paradigm, which automates the search of the optimal DA policies from the training dataset. AA trains a proxy model with the augmentation policy generated by a controller, which is updated through Reinforization. This paper provides an overview of the DA and AA algorithms, and their advantages and limitations.\tThis paper proposed a fine-grained automated data augmentation approach, Patch AutoAugment (PAA), which tries to increase diversity in local regions by divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. The proposed PAA considers the task as a multi-agent reinforcement learning problem,  and adopt a multi-agent reinforcement learning algorithm to automatically search for the optimal augmentation policies by considering the contextual relationship between the patches. They verify the proposed method on many classification and fine-grained recognition dataset(CIFAR10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars and FGVC-Aircraft). The experiments show a good result and visualization results provide some insights that the PAA  help the target network to localize more class-related cues. ""963\tThis paper proposes AutoAugment (AA), a new data Augmentation paradigm that automates the search of optimal augmentation policies from the training dataset. AA training involves training a proxy model with the augmentation policy generated by a controller, which is updated through Reinforaging, a natural learning process that helps the model to adapt to new environments. The proposed AA paradigm is shown to improve the performance and robustness of deep neural networks, and has strong transferability across different datasets. The paper presents the results of extensive experiments on a variety of datasets and demonstrates the effectiveness of AA in alleviate the overfitting risk and improving the performance of deep neural networks.\t  The paper proposes an evolution of the traditional   pipeline of image data augmentation used to reduce ML   model overfitting.    Instead of applying transformations such as shear,   rotate, CutOut, etc. at the image level, the proposed   technique divides the images into a fixed grid and   applies a potentially different transformation to each   cell. The problem of selecting a transform for each cell   is cast as a multi-agent RL (MARL) task, and the agents   learn as the main network trains within a (multi-agent)   Advantage Actor Critic framework.    The agents use a shared reward mechanism, with the reward   defined as the difference between the loss on the   augmented sample and the original loss.    The regular grid is fixed for a dataset and   hyperparameters like the magnitude of the augmentations   follow a fixed schedule; the agents only pick _which_   augmentation to apply on a patch.    Experiments performed on CIFAR-{10,100}, ImageNet,   CUB-200-2011, Stanford Cars, and FGVC-Aircraft show   relatively small but very consistent improvements in   terms of image classification accuracy. Different design   choices (MARL vs. single-agent vs. random, grid size,   etc.) are ablated and discussed in detail. ""964\tThis paper discusses the adversarial vulnerability of deep neural networks and the need to incorporate causality into analyzing the distribution changes caused by adversarial attacks. It presents a causal graph to model the generation process of adversarial examples and defines the adversarial distribution to formalize the intuition of adversarial attacks. The paper also discusses the origin of adversarial vulnerability and the need for models to focus on spurious correlations to avoid these vulnerabilities. It proposes the adversarial distribution alignment method to eliminate the difference between natural and adversarial distributions. The paper provides insights and suggestions for improving the security of deep neural networks against adversarial attacks.\tThe work presents a causal perspective of adversarial attacks on image-based machine learning models by studying a causal graph of the adversarial data creation process and highlighting how such a process makes the learned models vulnerable. It argues that the main reason for adversarial vulnerability is the reliance of models on spurious correlations between labels and style. Accordingly, it proposes a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Empirically, the method is shown to be more robust than two baselines on three datasets.""965\tThis paper discusses the adversarial vulnerability of deep neural networks and the importance of incorporating causality into analyzing the distribution change caused by adversarial attacks. It defines the adversarial distribution to formalize the intuition of adversarial attacks and constructs a causal graph to model the generation process of adversarial examples. The paper then studies the distinction between the natural and adversarial distribution and concludes that the origin of adversarial vulnerability is the focus of models on spurious correlations. It proposes an adversarial distribution alignment method to eliminate the difference between the natural and adversarial distribution and improve the performance of deep neural networks.\tThis paper presents a causal perspective on addressing adversarially vulnerability. It first constructs a causal graph, which then inspires the design of the distribution alignment method for reducing the gap between adversarial and natural data. Extensive experiments on CIFAR10, CIFAR100, and MNIST demonstrate the robustness of the proposed method against various attack methods.""966\tThis paper discusses the adversarial vulnerability of deep neural networks and the need to incorporate causality into analyzing the distribution changes caused by adversarial attacks. It defines the adversarial distribution to formalize the intuition of adversarial attacks and proposes an approach to eliminate the difference between natural and adversarial distributions. The paper also discusses the limitations of current models for adversarial vulnerability and proposes a new causal understanding based approach to solving the problem.\tThe paper shows a causal perspective to the adversarial robustness problem. It creates a graph over content and style variable sets. It identifies the spurious correlation between style and label as the main reason for adversarial examples, and then proposes a method to remove it from the trained model. Experiments on three datasets show that the proposed method is better than two baselines. ""967\tThis paper studies the adversarial vulnerability of deep neural networks and how causality can be incorporated into analyzing the distribution change caused by adversarial attacks. It constructs a causal graph to model the generation process of adversarial examples and defines the adversarial distribution to formalize the intuition of adversarial attacks. The paper studies the distinction between the natural and adversarial distribution and\u5f97\u51fa\u7ed3\u8bba that the origin of adversarial vulnerability is the focus of models on spurious correlations. It proposes the adversarial distribution alignment method to eliminate the difference between the natural and adversarial distribution.\tThis paper proposes a causal graph to model the generation process of adversarial attacks. Based on the proposed causal graph, the authors identify the origin of adversarial vulnerability as the spurious correlation between style variable and class label. Under the adversarial distribution, such spurious correlation can be maliciously used to mislead a victim model. In this light, the authors propose a method to align the adversarial distribution and the natural distribution to prevent a model from learning spurious correlation. The proposed method is empirically validated on prevailing datasets under several attacks.""968\tThis paper discusses the topic of continual learning, a machine learning technique that enables intelligent agents to acquire knowledge over time without forgetting key information in the past. The paper explores recent advances in this field, including the use of external memories, parameter gradients, or subspace modeling to store and process past knowledge in continual learning. The paper also discusses the challenges and limitations of these methods and suggests new directions for future research.\tThis paper introduces a model for continual learning based on the decomposition of linear filters into low-rank components, called atoms. Specifically, the authors decompose convolutional filters shaped (c,c',k,k) into two components: i) alpha, shaped (c,c',m) and D, shaped (m,k,k). The former is learned on the first task and then frozen, whereas for D every task has its own and they are do not conflict during optimization. On top of that, the authors envision two different ensembling schemes that improve performances. i) First, in task-incremental settings, they retrieve atoms from the task of interest and from similar tasks as well (based on SVD decomposition of D matrices and Grassman distance) and ensemble them. ii) Furthermore, in class-incremental settings, they explicitly setup multiple atoms per task, building a task ensemble. During inference, all ensembles are queried and their predictive variance is used to \"discover\" the relevant task, for which a prediction is carried out. Experiments are carried out on 3 datasets in both task-incremental and class-incremental learning settings. ""969\tThis paper discusses the recent advances in continual learning, a field that involves learning from a sequence of tasks while preserving the ability to resolve the old ones to avoid catastrophic forgetting. The paper focuses on several popular methods for maintaining an external memory of previous tasks, such as using original images, synthesized images, or parameter gradients to store information. The paper also discusses the limitations of these methods and suggests new directions for further research.\tThe paper proposes a continual learning algorithm that enforces the convolutional filter in each layer to a low-rank filter subspace defined by a small set of filter atoms. For each task, each convolutional layer is defined by a new filter subspace but subspace coefficients are shared among the tasks. The algorithm is validated on multiple benchmark datasets. ""970\tThis paper discusses continual learning, a field of artificial intelligence that involves learning from a sequence of tasks while preserving the ability to resolve the old ones to avoid catastrophic forgetting. The paper focuses on several recent advances in this field, including the use of external memories, synthesized images, or parameter gradients to maintain past knowledge. The paper also discusses the challenges of implementing these methods and the future directions for continual learning.\tThe paper, motivated by the task subspace modeling literature, enforced a low-rank filter structure to each CNN layer across time in continual learning. It not only ensures that the knowledge of the past tasks is not lost but also saves a lot of computing memory. Meanwhile, the paper proposes novel intra-task ensembles and inter-task ensembles for class-incremental settings and task-incremental settings, respectively. ""971\tThis paper discusses continual learning, a technique for training intelligent agents to learn from a sequence of tasks while avoiding catastrophic forgetting. The paper discusses several popular methods for maintaining an external memory of previous tasks, including using original images, synthesized images, or parameter gradients to store information. It also discusses the problem of heavy memory footprints associated with these methods and proposes several solutions. Finally, the paper concludes by discussing future directions for continual learning and the potential applications of this technique in various fields.\tThis paper tackles the continual learning via enforcing a low-rank filter structure to each CNN layer.  They first perform atom-coefficient filter decomposition and then learn each task with a new filter subspace, so that the method only needs to save the new filters for each task. The contribution of this paper includes the low-rank filter scheme and the designed intra-task and inter-task model ensemble performing on the filters. The proposed method also achieves SOTA performance on several datasets with tiny size of model memory. ""972\tThis paper discusses the efficiency and accuracy of Bayesian learning using Markov Chain Monte Carlo (MCMC) and variational inference. MCMC provides asymptotically accurate samples, but it can be time-consuming to simulate the chain. variational inference approximates the intractable target distribution with parametric families by minimizing the KL divergence, which can be more computationally efficient. One example of a particle-based inference algorithm is Stein Variational Gradient Descent (SVGD), which iterativelyTransports the particles by the functional gradient of KL divergence in the reproducing kernel Hilbert space (RKHS). This algorithm is particularly compelling because it bypasses the inherent distributional assumptions on the variational family. The paper discusses the advantages and limitations of each method and suggests how they can be combined to achieve the best performance.\tIn this paper, the authors analyze the underestimation issue of stein variational gradient descent (SVGD), and propose the maximum mean discrepancy (MMD) descent. From the perspective of the decomposition of the gradient term (driving force and repulsive force), this paper suggested to use another driving force term instead of the original one in SVGD. But the new driving force makes MMD-descent impractical since it depends on an intractable integral of the desired distribution $p$. In addition, the paper identify the log derivative driving force as the problematic term in SVGD, and propose a modified SVGD with particle resampling. They also argue that the proportional asymptotic limit is more relevant to understanding the variance collapse phenomenon. The theoretical dimensional analysis of SVGD on Gaussian also suggested another modified (damped) SVGD.""973\tThe paper provides an overview of the challenges in Bayesian learning and the methods for efficiently and accurately learning complex posterior distributions. The paper highlights the importance of Markov Chain Monte Carlo (MCMC) for simulating chains until convergence, but notes that it can be time-consuming. The paper discusses several methods for approximate learning, including variational inference and particle-based inference algorithms, such as Stein Variational Gradient Descent (SVGD) and the popular variations such as Stein Variational Bayes (SVB) and Variational Autoencoder (VAE). The paper also highlights the importance of considering the complexity of the target distribution and the choice of the variational family, as well as the computational efficiency of the inference algorithm.\tThis paper provides an understanding of the variance collapse phenomenon of SVGD. The paper first (1) introduces the reader to the most important concepts and phenomena, then (2) gives an explanation for why this problem occurs, thanks to a comparison with an accurate (yet computationally intensive) algorithm they call MMD-descent. Finally (3) the paper shows how to fix SVGD with damping. The paper provides experiments and theory, nicely combined.""974\tThis paper discusses the efficiency and accuracy of Bayesian learning using Markov Chain Monte Carlo (MCMC) and variational inference. MCMC provides accurate samples from a complex posterior distribution, but it is time-consuming to simulate the chain. variational inference approximation the intractable target distribution with parametric families by minimizing the KL divergence, which can be more computationally efficient. Stein Variational Gradient Descent (SVGD) is a deterministic particle-based inference algorithm that iteratively transports the particles by the functional gradient of KL divergence in the reproducing kernel Hilbert space (RKHS). This paper provides an overview of the algorithms and their applications.\tThis work studies the variance collapse phenomenon of SVGD. By comparing to MMD-descend, the authors argue that the driving force of SVGD suffers from a bias caused by reusing data, and thus tends to underestimate the variance of the target distribution. Theory are developed in the setting of estimating standard Gaussian with a proposal limit (i.e., $d/n \\to \\gamma$), and explains the understanding in the overparameterized/high-dim setting (i.e., $\\gamma > 1$). Experiments are also conducted to verify the understanding. Finally, motivated by the understanding, new algorithm is proposed to fix the issue of SVGD by damping the driving force term in SVGD.""975\tThis paper discusses the efficient and accurate learning of complex posterior distributions in Bayesian learning. It presents two main challenges: efficiently simulating a Markov Chain Monte Carlo chain until convergence and approximate the target distribution with a variational family using KL divergence. One of the most popular variational approximation methods is particle-based inference algorithms, such as Stein Variational Gradient Descent (SVGD) and its variations. SVGD iteratively transports particles by the functional gradient of KL divergence in the reproducing kernel Hilbert space (RKHS), which can be more efficient than simulating the chain. This paper also discusses other particle-based inference algorithms and their properties, such as the use of importance sampling, and the choice of the RKHS.\tThis paper analyzed the curse-of-dimensionality problem of the vanilla SVGD with Euclidean distance kernel in a qualitative and quantitative way. Specifically, the author first built a connection of SVGD to MMD-descent, where they share identical repulsive forces with different driving forces (if Euclidean distance kernel is adopted). Then, the author argued that the variance collapse problem is rooted in (1) high variance and (2) the deterministic bias of the driving force, which were confirmed by sampling from the isotropic Gaussian. Quantitatively, the author analyzed the stationary variance of MMD-descent and SVGD with isotropic Gaussian under the proportional limit, which confirms the curse-of-dimensionality problem of SVGD. ""976\tThis paper discusses the challenges of handling noisy labels in deep learning and the recent developments in methods for combatting this issue. The paper also highlights the importance of safety-critical areas, where adversarial examples are a significant threat to the performance of deep neural networks. The paper presents several robust label-noise learning methods and adversarial training methods, which have been proposed in the literature. It also discusses the limitations and future directions of these methods.\tThis submission empirically studies the efficacy of adversarial training for mitigating the effect of label noise in training data. Their findings are as follows: 1) \"Smoothing effect\" of adversarial training: \ta) on a 2-dimensional synthetic binary classification dataset where two points are incorrectly labeled, they show that vanilla training yields a classifier that memorizes the bad labels by forming \"clusters\" around the incorrectly labeled points, whereas adversarial training does not yield such clusters \tb) for CIFAR injected with 20/40% random label noise, they ran vanilla and adversarial training on the noisy data and found that if you look at the distribution over labels within the neighborhood of a random incorrectly labeled point, on average the entropy of that distribution is higher for the classifier obtained by adversarial training than by vanilla training 2) For vanilla training on CIFAR (also MNIST) injected with label noise, the gap between accuracy on the correctly labeled training data and the incorrectly labeled data closed over the course of training, whereas this gap does not close or seems to close much more slowly for adversarial training. 3) Over these same noisy datasets, adversarial training seems to mitigate the impact of noisy training data on (clean) test accuracy, unlike vanilla training for which generalization degrades as label noise increases 4) They consider a quantity they call the \"geometry value\" of a data point (x,y), which corresponds to the number of PGD steps needed to find a differently labeled point in the neighborhood of x. This quantity was originally introduced by [Zhang et al. 2021b], and in the present paper they find that: \ta) compared to loss(x,y), it appears to be a more effective way to effective way to distinguish correctly labeled data from incorrectly labeled training data, as well rare data from typical data 5) They propose a \"robust annotator\" for labeling unlabeled data that has possibly been subject to adversarial perturbations. The algorithm repeatedly alternates between 1) identifying training data points with high loss and low geometry value and re-labeling them according to the current classifier and 2) running a step of adversarial training. It appears to do slightly better than a PGD-based annotator baseline when trained over CIFAR injected with label noise. They also note that the geometry value can provide some kind of \"confidence score\" to go along with the label annotations.""977\tThis paper discusses the challenges of learning with noisy labels and the recent developments in methods for combatting these challenges. The paper\u5148\u4ecb\u7ecd\u4e86\u5728\u5b9e\u8df5\u8fc7\u7a0b\u4e2d\u6570\u636e labeling \u901a\u5e38\u662f\u6709\u566a\u58f0\u7684\uff0c\u56e0\u6b64\u5b66\u4e60\u603b\u662f\u4f34\u968f\u7740\u566a\u58f0\uff0c\u8fd9\u662f\u5fc5\u7136\u7684(Natarajan,2013)\u3002\u4e3a\u4e86\u5e94\u5bf9\u566a\u58f0\uff0c\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86 robust label-noise learning \u65b9\u6cd5\uff0c\u5982\u6837\u672c\u9009\u62e9\u548c loss/label \u4fee\u590d( Jiang,2018, Patrini,2017, Nguyen,2019)\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u5b89\u5168\u5173\u952e\u9886\u57df(\u5982\u533b\u7597\u548c\u91d1\u878d),Deep \u795e\u7ecf\u7f51\u7edc\u5fc5\u987b\u5177\u6709\u5bf9 adversarial \u6837\u672c\u7684\u6297 adversarial \u80fd\u529b\uff0c\u8fd9\u8981\u6c42\u795e\u7ecf\u7f51\u7edc\u5728\u8fd0\u884c\u65f6\u9700\u8981\u5bf9 adversarial \u6837\u672c\u8fdb\u884c\u6297 adversarial \u8bad\u7ec3( Szegedy,2014, Nguyen,2015)\u3002\u6700\u8fd1\uff0c\u7814\u7a76\u4eba\u5458\u4e5f\u5728\u63a2\u7d22 label-noise \u5b66\u4e60\u548c adversarial \u8bad\u7ec3\u7684\u72ec\u7acb\u6027\uff0c\u5982 Ding,2020 \u548c Zhang,2021b \u6240\u793a\u3002\tThis paper studies the connection between noisy labels (NL) and adversarial training (AT). The contribution of this paper is two-fold. The first one is to adopt the number of PGD attack steps as a criterion for sample selection to correct noisy labels. The second one is that adversarial training can serve as a way to correct noisy labels. These two contributions indicate that adversarial training can be applied to more general model robustness problems.  ""978\tThis paper discusses the challenges of learning with noisy labels and the recent advances in robust label-noise learning and adversarial training. The paper starts by introducing the concept of label noise and safety-critical deep neural networks. It then discusses the existing methods for combatting label noise, including sample selection and loss/label correction. The paper also presents a new approach, adversarial training, which empirically generates adversarial data to update the model. Finally, the paper discusses the recent advances in label-noise learning and adversarial training, and their potential applications.\tThis paper focuses on understanding adversarial training in the presence of label noise by conducting empirical studies. Based on their observations, the authors propose to use _PGD step number_ of adversarial training as a new measure for sample selection to correct noisy labels. Moreover, they present two use-cases, namely 1. a _robust annotator_ algorithm to label unlabeled instances, and 2. PGD step number as a _confidence score_ for the labeling of unlabeled instances. Empirical observations are primarily made on CIFAR-10 images.""979\tThis paper discusses the challenges of learning with noisy labels and the recent research efforts to address these challenges. The paper starts by introducing the concept of label noise, which is the presence of noisy labels in the data. It also mentions that learning with noisy labels is inevitable in practice.\n\nThe paper then presents several robust label-noise learning methods, such as sample selection and loss/label correction. These methods try to improve the performance of the model on the clean data by removing the noisy labels.\n\nThe paper also discusses the issue of adversarial examples, which are types of malicious data that can attack the performance of deep neural networks. Adversarial training methods try to empirically generate adversarial data on the fly to update the model, but these methods have been shown to be effective in\u9632\u5fa1 adversarial examples.\n\nThe paper also explores the independent research on label-noise learning and adversarial training. Ding et al. (2020) and Zhang et al. (2021b) have shown that non-robust data that are close to the robust data can still have a significant impact on the performance of the model. This suggests that the choice of the robust data should be based on the specific task and the environment.\n\nOverall, the paper provides an overview of the current state of the art in these areas and discusses the future directions for these challenges.\tThis paper studies the adversarial training in the context of label noises. Specifically, it is discovered that adversarial training can prevent the model from overfitting to the label noises, leading to a more smooth landscape. In addition, the authors point out that the number of PGD step sizes can be considered as a useful metric to distinguish instances with correct or incorrect labels.""980\tThis paper discusses a new method for measuring the expected robustness of a neural network model, called Robustness Measurement and Assessment (RoMA). RoMA determines the probability that a random input perturbation might cause misclassification, allowing for formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The method is particularly useful for neural network models that are deployed in critical systems, where small input perturbations can have significant impacts.\tPresent a method to measure the expected robustness of a neural network model, by determining  the probability that a random input perturbation might cause misclassification, providing formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The method can be applied black-box. Applied the approach to compare the robustness of different models, and measure how a model\u2019s robustness is affected by the magnitude of input perturbation. ""981\tThis paper discusses a new method for measuring the expected robustness of a neural network model, called Robustness Measurement and Assessment (RoMA). RoMA determines the probability that a random input perturbation might cause misclassification, allowing you to provide formal guarantees about the expected frequency of errors that a trained model will encounter after deployment. The method is useful for ensuring the reliability of neural network models in critical systems.\tThe paper presents a statistical method - Robustness Measurement and Assessment (RoMA) to measure the expected robustness of a neural network model. The robustness is defined as the probability that a random input perturbation causes an incorrect prediction. The presented approach is a blackbox approach. Different output labels are observed to exhibit different robustness values.   The basic premise of the paper is that the adversarial perturbations are not naturally normal, but a transformation (Box-Cox) can be applied to make them a normal distribution before applying statistical estimation techniques (Anderson-Darling test + z score). ""982\tThis paper discusses a new method for measuring the expected robustness of a neural network model, called Robustness Measurement and Assessment (RoMA). RoMA determines the probability that a random input perturbation might cause misclassification, allowing you to provide formal guarantees about the expected frequency of errors that a trained model will encounter after deployment. The method is useful for ensuring the reliability of neural network models in critical systems.\tThis paper proposes RoMA, a robustness evaluation framework based on local sampling and probability computation.  The main contributions are: 1. Proposal of the ($\\epsilon$,$\\delta$) local robustness score for assessing the probability of random local samples that have different predictions than a given data input with a $\\delta$-confined top-1 confidence.  2. Use of Box-Cox transformation for input data to improve statistical estimation.  3. The method can be implemented in a model-agnostic fashion. ""983\tThis paper discusses a new statistical method called Robustness Measurement and Assessment (RoMA) for measuring the expected robustness of a neural network model. RoMA determines the probability that a random input perturbation might cause misclassification, allowing for formal guarantees about the expected frequency of errors that a trained model will encounter after deployment. The method is used to address the reliability issue faced by neural network models in critical systems.\tSummary:  The paper introduces a statistical method to measure the robustness of deep neural networks. The novel  part of the method is that it's designed to measure the probability of random points near an input being  adversarial, instead of probability of adversarial examples existing in the vicinity of an input. To measure it,  the authors proposes to use Box-cox transformation to transform distribution of confidence scores to normal,  then calculate the probability based on it.  ""984\tThis paper proposes a hierarchical chunking model, which learns representations ofSequential data from the ground up by discovering minimalatomicSequential units as chunks. As learning progresses, themodelchunks previously learned representations into more complex representations guided bySequential dependence. The model learns meaningful and interpretable representations in various domains and language data, and enables flexible transfer between environments. The learning guarantee and the demonstrated ability to learn interpretable representations make the model a potential tool for deep learning applications in various fields.\tThis paper presents HCM, an approach for chunking a sequence of data into a hierarchical representation. More specifically, HCM learns a tree with atomic units (ie the low-level inputs, in this case integers representing things like text characters or quantized pixel values) as the leaves and increasingly complex groupings of them higher up the tree.   HCM learns by iteratively parsing the provided data (ie stream of tokens), in each pass computing marginals for the current set of chunks as well as transition frequencies between them. After updating its marginals and transition frequencies, the two chunks with highest joint probability are combined into one. The process continues until all pairs of chunks pass an independence test.   I believe the main contribution of this paper is in that it presents an idea for interpretable grouping based on the principle of grouping by proximity from cognitive science, and a largely qualitative proof of concept for it.""985\tThe paper proposes a hierarchical chunking model, which learns representations from non-i.i.dSequential data by discovering minimalatomicSequential units as chunks. As learning progresses, the modelchunks previously learned representations into more complex representations guided bySequential dependence. The learned chunks are meaningful and interpretable in various domains such as visual, temporal, visual-temporal and language data. The interpretability enables flexible transfer between environments. The model is shown to learn the representations well with high accuracy on various datasets.\tThe paper proposes a graph-learning model (HCM) for learning hierarchical chunks from sequential data. The paper first proposes an idealised HCM method, for which the paper provides learning guarantees via a proof by induction, and an online approximation to this idealised method, which is more computationally feasible and which is used to perform experiments in temporal, visual, visuotemporal and language sequential data domains. The paper demonstrates that the online method learns interpretable chunks at multiple levels of abstraction and demonstrates positive (and negative) transfer to other hierarchically structured environments with similar (and different) structures.""986\tThis paper proposes a hierarchical chunking model, which learns representations ofSequential data from the ground up by discovering minimalatomicSequential units as chunks. As learning progresses, the model chunkes previouslylearned representations into more complex representations guided bySequential dependence. The model is then trained on a range of domains andlanguage data, and demonstrates that it learns meaningful and interpretable representations. The interpretability of thelearned chunks enables flexible transfer between environments.\tThis paper proposes a method for learning representations of non- i.i.d. data in terms of hierarchical sets of chunks, inspired by cognitive theories of grouping by proximity. These sets are assembled over time from the initial set of primitive data points by finding correlations between temporally/spatially sequential primitives/chunks and appending to the set. The authors show that this learning method is tractable, has convergence w.r.t. hierarchically-decomposable problems, and learns intuitively and practically reasonable chunk sets.""987\tThe paper proposes a hierarchical chunking model, which learns representations from non-i.i.dSequential data by discovering minimalatomicSequential units as chunk. As learning progresses, themodelchunks previously learned representations into more complex representations guided bySequential dependence. Themodellearns meaningful and interpretable representations in visual, temporal, visual-temporal domains and language data and enables flexible transfer between environments. The learning guarantee and the demonstration of the interpretability of the learned chunks provide evidence that the model is able to learn complex skills and adapt to new environments effectively.\tThis paper proposes a non neural system of parsing natural language text by chunking sequences to form hierarchical structures. The algorithm strongly resembles classical parsing algorithms. Decisions about when to chunk a phrase into a constituent are based on chi^2 tests of independence, where a pair of chunks that are considered to be dependent are joined into a single constituent. They test this chunking algorithm on natural language data against an RNN,  concluding that the classical parsing algorithm is more sample efficient in achieving a low KL-divergence from the true sequence data. They also provide some examples of how this algorithm can be applied to temporal image data or video.""988\tThis paper discusses the use of preconditioning in gradient-based optimization for non-linear, non-convex optimization problems. It discusses the impact of preconditioning on the convergence speed of the optimization algorithm and howAdaptive gradient optimizers such as AdaGrad and Adam can improve the convergence speed further. The paper also examines the underlying mathematical models and techniques used in gradient-based optimization and how preconditioning can be used to accelerate the convergence of the algorithm.\tThis work studies the question to what extend a reparametrization of an optimization problem, i.e. representing the original parameters w to optimize for as a function of some other parameters theta, can accelerate the convergences of the gradient flow / gradient descent for nonconvex optimization problems. It studies the dynamics of the flow via eigenvectors of a matrix M formed as the expectation over the outer product of the gradient of the loss with itself to reveal 'slow' and 'fast' modes of the evolution. It subsequently derives sufficient conditions for the reparametrization (which is chosen to be linear but time varying) to balance the decay on all modes. After discussing an efficient approximation of the theoretically derived scheme, numerical results demonstrate the effectiveness of the proposed reparametrization in two exemplary applications.  ""989\tThis paper discusses the use of preconditioning in gradient-based optimization for non-linear, non-convex optimization problems. It discusses the idea of multiplying a symmetric positive-definite preconditioner to the original problem to accelerate convergence. Adaptive gradient optimizers such as AdaGrad Duchi et al. (2011) and Adam Kingma & Ba (2014) use the same idea but update the preconditioner during iterative optimization with adaptive step size. The paper also discusses how to handle errors in the preconditioner and how to compare the performance of different preconditioners.\tThis paper proposes a reparameterization of non-linear non-convex optimization problems. This reparameterization amounts to a linear map (i.e., \"optimization params = linear operation of a different set of parameters). These linear maps are interpreted as a graph convolution network. The experimental results are validated on \"Kuramoto models\" and \"persistent homology models\".""990\tThis paper discusses the issue of slow convergence in non-convex optimization problems for gradient-based algorithms such as the\u68af\u5ea6\u4e0b\u964d(GD)algorithm. It explains how preconditioning, which multiplying a symmetric positive-definite preconditioner matrix to the original problem, can accelerate convergence and lead to faster optimization. Adaptive gradient optimizers such as AdaGrad and Adam use the same idea of updating the preconditioner during iterative optimization with adaptive step size, which also leads to improved convergence speed. The paper also examines the technical challenges involved in improving the convergence of non-convex optimization and proposes some potential solutions.\tThe authors derive a neural reparameterization of non-convex optimization problems in order to accelerate their convergence. They do this by deriving how the slowest components of the optimization variables can have their convergence rate improved by preconditioning with a NTK-based matrix. They make connections between this approach and Group Convolutional Networks. Experimentally, they show this approach improves upon baseline gradient-based optimization on a two datasets. ""991\tThis paper discusses the use of preconditioning in gradient-based optimization for non-linear, non-convex optimization problems. It explains how preconditioning can accelerate convergence of gradient-based optimization algorithms such asadagrad and Adam by re-scales the loss landscape and changing the condition number of the system. It also discusses the latest developments in adaptive gradient optimizers such as AdaGrad Duchi et al. andAdam Kingma & Ba, which useAdaptive gradient steps to improve the convergence speed of non-convex optimization problems. The paper also analyzes the performance of these algorithms on a real-world problem and\u603b\u7ed3\u4e86\u4e00\u4e9b\u7814\u7a76\u7ed3\u8bba.\tThis work proposed a neural reparametrization scheme to accelerate a large class of nonconvex nonlinear optimization problems. The proposed method is grounded on analysis that the dynamics of gradient \ufb02ow are related to the condition number of the system. More specifically, by reparametrizing the optimization problem with a graph convolutional network (GNN), the proposed method can modify the condition number and obtain convergence speed up, the acceleration is demonstrated on optimizing synchronization problems and persistent homology of point-clouds.""992\tThis paper provides an overview of deep convolutional neural networks (DCNNs), which are powerful tools for automatic image analysis. It explains how DCNNs can be applied to various types of image content without the need to develop task-specific algorithms and how they can easily be applied to a broad range of domains with excellent performance. The paper also discusses one of the major weaknesses of DCNNs, which is their dependence on a large set of examples for training. To achieve meaningful performance and avoid underfitting, DCNNs normally rely on relatively large training sets. Additionally, the paper provides an overview of commonly used DCNN models and their performance on various image analysis tasks.\tIn this work auhors proposed an ensemble of multiple SVM classifiers setup in a hierarchical way (somewhat similar to neural networks). Each SVM is trained is a small patch or window from the input imagery and some classifiers can be eliminated from contributing to the predictions. Results show better performance by the model in the small data regime compared with larger convolutional neural networks trained from scratch.""993\tThe paper discusses the success and popularity of deep convolutional neural networks (DCNNs) for automatic image analysis. It also notes one of their weaknesses, which is their dependence on a large set of examples for training. DCNNs have hundreds of layers with thousands of trainable parameters and rely on relatively large training sets to achieve meaningful performance and avoid underfitting. The paper also discusses the challenges of training DCNNs, such as the need for large datasets of labeled ground truth images.\tThe paper describes the use of SVM classifiers trained independently using local receptive fields of images, outputting class probabilities for each pixel that are organized in channels. New independent SVM classifiers are trained over the class probabilities and their results are combined using voting to perform the final prediction. The authors report experiments on two datasets, comparing with versions of ResNet trained from scratch, in which SVMNet shows better results than ResNet when using fewer training examples, For one of the datasets, the ResNet could not converge, while SVMNet obtained up to 80%.""994\tThis paper discusses the use of deep convolutional neural networks (DCNNs) for automatic image analysis. DCNNs are powerful tools that have demonstrated success in a variety of tasks, including object detection, image classification, and scene understanding. The paper discusses one of the major weaknesses of DCNNs, which is their dependence on a large set of examples for training. To achieve meaningful performance and avoid underfitting, DCNNs typically rely on relatively large training sets. Additionally, the paper discusses the challenges involved in training DCNNs, including the need for labeled ground truth images and the need to choose the appropriate architecture for each task.\tThe paper argues that one of the major drawbacks for deep convolutional neural networks (DCNNs) is the need for large annotated training sets. To address this drawback, the paper proposes a new architecture, SVMnet, which is designed to achieve relatively high accuracy  (compared to DCNNs) in settings of small training sets.  The SVMnet architecture is composed of one or more stacked \"SVM layers\". Each SVM layer is composed of a set of independent svm classifiers, where the input to each svm is a patch in the image and the output is a probability estimate for the image class. For example, given a grayscale h X w input image, an SVMnet with 5x5 kernels and stride of 5 would have a 2d \u201carray\u201d of h/5 X w/5 svms, each one trains on a patch of 5x5 pixels whichis flattened to a vector of 25 features. Each svm i is then trained directly to predict the class of the image, based on the 5x5 window, thus resulting in a probability vector for the 5x5 patch. From the validation step, we also obtain the average accuracy of each svm i, denoted by Ai, from which we can compute its weight when aggravating the predictions from all svms. In general each svm layer will have k X l svms, and thh output of the svm layer can be formulated either as a 2d array of k X l predictions or a 3d array of k X l X c of probabilities for each class (where c is the number of classes). When stacking a second SVM layer, the input to that layer is then the k X l X c probability map which is treated as a feature map. Finally, the predictions from the last SVM layer are tallied to produce a majority vote for the image. When training on images with multiple channels, for example RGB images, the channels are flattened to one vector, for example: a 3x5x5 patch would be flattened to a 75 dimensional feature vector.   To demonstrate the applicability of the proposed method, the paper compares SVMnet to ResNet on 3 publicly available datasets and claims to achieve superior performance to resnet in settings of limited training data as well as faster training time. ""995\tThe paper discusses the advantages and limitations of deep convolutional neural networks (DCNNs) for automatic image analysis. It explains that DCNNs have demonstrated success in a wide range of tasks and have gained popularity over the past decade. The paper also explains that DCNNs have a major weakness - their dependence on a large set of examples for training. To achieve meaningful performance and avoid underfitting, DCNNs rely on relatively large training sets. The paper also explains that training DCNNs typically requires large datasets of labeled ground truth images.\tOne of the weakness of traditional DCNNs is it needs large clean-labeled dataset. In this paper, the author proposes SVMNET, a deep learning architecture which includes a layered structure of Support Vector Machine (SVM) ensembles. The result shows that the SVMNET outperforms other deep convolutional neural networks such as ResNet-50 with less training time for cases in which the number of labeled training samples is small.  ""996\tThis paper presents an overview of Graph Neural Networks (GNNs), a type of neural network that has shown impressive results in graph-based applications, such as social networks, recommendation systems, and drug discovery. Despite their success, training GNNs on large-scale real-world graphs, such as Facebook social network, remains challenging due to the unique structure of GNNs and the limited memory capacity/Bandwidth of current servers. This paper presents an overview of distributed training techniques that can be used to address these limitations and scale GNN training efficiently. Additionally, this paper provides an analysis of the challenges and opportunities associated with distributed training and discusses some of the recent research in this area.\tThe paper considers the problem of distributed training for graph learning tasks, under a setting where data privacy is significant for each individual machine and communication to/from a central parameter server is expensive. To preserve privacy each machine has only access to a distinct partition of the overall graph. The central server has access to the full graph. In the LLCG algorithm that the paper proposes, each machine trains on its local graph partition for some time before sending the parameters to the server. The server averages the received parameter, but additionally also does its own training using the full graph available to it. Theoretically the authors show that the proposed method avoids an error gap in the gradient norm that would exist if server correction is not performed. Experimental results show the proposed scheme performing similarly to GGS albeit with much lower communication costs.  ""997\tThis paper presents an overview of Graph Neural Networks (GNNs), a type of neural network designed to learn complex relationships between nodes in a graph, and its application in various graph-based applications. The paper also discusses the challenges faced in training GNNs on large-scale real-world graphs, such as Facebook social network, and proposes a solution using distributed training. Additionally, the paper provides an overview of existing GNN research, including the history of GNNs, its strengths and limitations, and the recent developments in GNN technology.\tTraining GNNs is challenging due to high communication costs or large memory overheads. This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to periodically model averaging on the server using locally trained models.  It also applies global server corrections to refine the locally learned models and solve the irreducible performance degradation caused by ignoring node dependency. This paper provides the convergence analysis and shows the proposed method can address the residual error. The experimental results show significant improvement compared to existing methods.""998\tThis paper discusses the recent advances in Graph Neural Networks (GNNs) and their applications, including social networks, recommendation systems, and drug discovery. It also discusses the challenges that arise in training GNNs on large-scale real-world graphs, such as Facebook social network. One potential solution to these challenges is distributed training, which can improve the efficiency of training GNNs on large graphs. The paper also discusses some recent works in GNNs and their future directions.\tThis paper deals with the problem of distributed training of GNNs. Existing methods are either communication-intensive (sampling) or do not achieve good performance (averaging).  The authors propose a novel method, dubbed \"LLCG: Learn Locally Correct Globally\". Essentially, this method captures the idea of transmitting only local averages but adds a centralized step on the server to account for global structural information lost in the subgraph partition.  The authors further provide theoretical convergence guarantees. They both show that just averaging leads to an insurmountable residual error that explains the poor performance of averaging methods, as well as prove that this residual error disappears when adding the global correction step.""999\tThis paper provides an overview of Graph Neural Networks (GNNs), a powerful machine learning technique for analyzing large-scale graph data. It discusses the challenges and opportunities in training GNNs on large real-world graphs, such as Facebook social network, and suggests several possible solutions. The paper also examines the recent success and limitations of GNNs in various graph-based applications. Finally, it provides an overview of the future directions and challenges of GNNs and other graph-based machine learning techniques.\tThis paper proposes a distributed training technique for GNN. This technique includes local computations done in parallel by several machines and a correction phase done by a centralized server. A theoretical analysis is given for this technique, showing that the server correction phase reduces some irreducible error that happens due to splitting the graph and doing a local computation on each subgraph. Several experiments are made on real datasets which show the merits of this technique over previous techniques in terms of performance, communication steps, and size.""1000\tThis paper discusses the challenges of using deep convolutional neural networks (DNNs) for real-world applications, especially in time-critical or low-resource use cases. The paper highlights the need for special-purpose libraries, network pruning, quantization, and distillation to accelerate network computation and mitigate the time constraints of DNNs. Additionally, the paper discusses the limitations of standard DNNs and how anytime inference can overcome these issues.\tThis work focuses on anytime pixel-level recognition (e.g., semantic segmentation). They propose to add intermediate exists in the architecture for anytime inference. They also consider spatial confidence adaptivity in their network, where they only execute subsequent layers on a small set of non-confidence pixels and obtain the features of other positions via interpolation. They apply the method to semantic segmentation and human pose estimation and demonstrate a reduction in FLOPs and good anytime performance for both tasks.   "